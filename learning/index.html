<!doctype html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />

    <title>skopt.learning API documentation</title>
    <meta name="description" content="Machine learning extensions for model-based optimisation." />

  <link href='http://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300' rel='stylesheet' type='text/css'>
  
  <style type="text/css">
  
* {
  box-sizing: border-box;
}
/*! normalize.css v1.1.1 | MIT License | git.io/normalize */

/* ==========================================================================
   HTML5 display definitions
   ========================================================================== */

/**
 * Correct `block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
nav,
section,
summary {
    display: block;
}

/**
 * Correct `inline-block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

audio,
canvas,
video {
    display: inline-block;
    *display: inline;
    *zoom: 1;
}

/**
 * Prevent modern browsers from displaying `audio` without controls.
 * Remove excess height in iOS 5 devices.
 */

audio:not([controls]) {
    display: none;
    height: 0;
}

/**
 * Address styling not present in IE 7/8/9, Firefox 3, and Safari 4.
 * Known issue: no IE 6 support.
 */

[hidden] {
    display: none;
}

/* ==========================================================================
   Base
   ========================================================================== */

/**
 * 1. Prevent system color scheme's background color being used in Firefox, IE,
 *    and Opera.
 * 2. Prevent system color scheme's text color being used in Firefox, IE, and
 *    Opera.
 * 3. Correct text resizing oddly in IE 6/7 when body `font-size` is set using
 *    `em` units.
 * 4. Prevent iOS text size adjust after orientation change, without disabling
 *    user zoom.
 */

html {
    background: #fff; /* 1 */
    color: #000; /* 2 */
    font-size: 100%; /* 3 */
    -webkit-text-size-adjust: 100%; /* 4 */
    -ms-text-size-adjust: 100%; /* 4 */
}

/**
 * Address `font-family` inconsistency between `textarea` and other form
 * elements.
 */

html,
button,
input,
select,
textarea {
    font-family: sans-serif;
}

/**
 * Address margins handled incorrectly in IE 6/7.
 */

body {
    margin: 0;
}

/* ==========================================================================
   Links
   ========================================================================== */

/**
 * Address `outline` inconsistency between Chrome and other browsers.
 */

a:focus {
    outline: thin dotted;
}

/**
 * Improve readability when focused and also mouse hovered in all browsers.
 */

a:active,
a:hover {
    outline: 0;
}

/* ==========================================================================
   Typography
   ========================================================================== */

/**
 * Address font sizes and margins set differently in IE 6/7.
 * Address font sizes within `section` and `article` in Firefox 4+, Safari 5,
 * and Chrome.
 */

h1 {
    font-size: 2em;
    margin: 0.67em 0;
}

h2 {
    font-size: 1.5em;
    margin: 0.83em 0;
}

h3 {
    font-size: 1.17em;
    margin: 1em 0;
}

h4 {
    font-size: 1em;
    margin: 1.33em 0;
}

h5 {
    font-size: 0.83em;
    margin: 1.67em 0;
}

h6 {
    font-size: 0.67em;
    margin: 2.33em 0;
}

/**
 * Address styling not present in IE 7/8/9, Safari 5, and Chrome.
 */

abbr[title] {
    border-bottom: 1px dotted;
}

/**
 * Address style set to `bolder` in Firefox 3+, Safari 4/5, and Chrome.
 */

b,
strong {
    font-weight: bold;
}

blockquote {
    margin: 1em 40px;
}

/**
 * Address styling not present in Safari 5 and Chrome.
 */

dfn {
    font-style: italic;
}

/**
 * Address differences between Firefox and other browsers.
 * Known issue: no IE 6/7 normalization.
 */

hr {
    -moz-box-sizing: content-box;
    box-sizing: content-box;
    height: 0;
}

/**
 * Address styling not present in IE 6/7/8/9.
 */

mark {
    background: #ff0;
    color: #000;
}

/**
 * Address margins set differently in IE 6/7.
 */

p,
pre {
    margin: 1em 0;
}

/**
 * Correct font family set oddly in IE 6, Safari 4/5, and Chrome.
 */

code,
kbd,
pre,
samp {
    font-family: monospace, serif;
    _font-family: 'courier new', monospace;
    font-size: 1em;
}

/**
 * Improve readability of pre-formatted text in all browsers.
 */

pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
}

/**
 * Address CSS quotes not supported in IE 6/7.
 */

q {
    quotes: none;
}

/**
 * Address `quotes` property not supported in Safari 4.
 */

q:before,
q:after {
    content: '';
    content: none;
}

/**
 * Address inconsistent and variable font size in all browsers.
 */

small {
    font-size: 80%;
}

/**
 * Prevent `sub` and `sup` affecting `line-height` in all browsers.
 */

sub,
sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
}

sup {
    top: -0.5em;
}

sub {
    bottom: -0.25em;
}

/* ==========================================================================
   Lists
   ========================================================================== */

/**
 * Address margins set differently in IE 6/7.
 */

dl,
menu,
ol,
ul {
    margin: 1em 0;
}

dd {
    margin: 0 0 0 40px;
}

/**
 * Address paddings set differently in IE 6/7.
 */

menu,
ol,
ul {
    padding: 0 0 0 40px;
}

/**
 * Correct list images handled incorrectly in IE 7.
 */

nav ul,
nav ol {
    list-style: none;
    list-style-image: none;
}

/* ==========================================================================
   Embedded content
   ========================================================================== */

/**
 * 1. Remove border when inside `a` element in IE 6/7/8/9 and Firefox 3.
 * 2. Improve image quality when scaled in IE 7.
 */

img {
    border: 0; /* 1 */
    -ms-interpolation-mode: bicubic; /* 2 */
}

/**
 * Correct overflow displayed oddly in IE 9.
 */

svg:not(:root) {
    overflow: hidden;
}

/* ==========================================================================
   Figures
   ========================================================================== */

/**
 * Address margin not present in IE 6/7/8/9, Safari 5, and Opera 11.
 */

figure {
    margin: 0;
}

/* ==========================================================================
   Forms
   ========================================================================== */

/**
 * Correct margin displayed oddly in IE 6/7.
 */

form {
    margin: 0;
}

/**
 * Define consistent border, margin, and padding.
 */

fieldset {
    border: 1px solid #c0c0c0;
    margin: 0 2px;
    padding: 0.35em 0.625em 0.75em;
}

/**
 * 1. Correct color not being inherited in IE 6/7/8/9.
 * 2. Correct text not wrapping in Firefox 3.
 * 3. Correct alignment displayed oddly in IE 6/7.
 */

legend {
    border: 0; /* 1 */
    padding: 0;
    white-space: normal; /* 2 */
    *margin-left: -7px; /* 3 */
}

/**
 * 1. Correct font size not being inherited in all browsers.
 * 2. Address margins set differently in IE 6/7, Firefox 3+, Safari 5,
 *    and Chrome.
 * 3. Improve appearance and consistency in all browsers.
 */

button,
input,
select,
textarea {
    font-size: 100%; /* 1 */
    margin: 0; /* 2 */
    vertical-align: baseline; /* 3 */
    *vertical-align: middle; /* 3 */
}

/**
 * Address Firefox 3+ setting `line-height` on `input` using `!important` in
 * the UA stylesheet.
 */

button,
input {
    line-height: normal;
}

/**
 * Address inconsistent `text-transform` inheritance for `button` and `select`.
 * All other form control elements do not inherit `text-transform` values.
 * Correct `button` style inheritance in Chrome, Safari 5+, and IE 6+.
 * Correct `select` style inheritance in Firefox 4+ and Opera.
 */

button,
select {
    text-transform: none;
}

/**
 * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio`
 *    and `video` controls.
 * 2. Correct inability to style clickable `input` types in iOS.
 * 3. Improve usability and consistency of cursor style between image-type
 *    `input` and others.
 * 4. Remove inner spacing in IE 7 without affecting normal text inputs.
 *    Known issue: inner spacing remains in IE 6.
 */

button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
    -webkit-appearance: button; /* 2 */
    cursor: pointer; /* 3 */
    *overflow: visible;  /* 4 */
}

/**
 * Re-set default cursor for disabled elements.
 */

button[disabled],
html input[disabled] {
    cursor: default;
}

/**
 * 1. Address box sizing set to content-box in IE 8/9.
 * 2. Remove excess padding in IE 8/9.
 * 3. Remove excess padding in IE 7.
 *    Known issue: excess padding remains in IE 6.
 */

input[type="checkbox"],
input[type="radio"] {
    box-sizing: border-box; /* 1 */
    padding: 0; /* 2 */
    *height: 13px; /* 3 */
    *width: 13px; /* 3 */
}

/**
 * 1. Address `appearance` set to `searchfield` in Safari 5 and Chrome.
 * 2. Address `box-sizing` set to `border-box` in Safari 5 and Chrome
 *    (include `-moz` to future-proof).
 */

input[type="search"] {
    -webkit-appearance: textfield; /* 1 */
    -moz-box-sizing: content-box;
    -webkit-box-sizing: content-box; /* 2 */
    box-sizing: content-box;
}

/**
 * Remove inner padding and search cancel button in Safari 5 and Chrome
 * on OS X.
 */

input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
    -webkit-appearance: none;
}

/**
 * Remove inner padding and border in Firefox 3+.
 */

button::-moz-focus-inner,
input::-moz-focus-inner {
    border: 0;
    padding: 0;
}

/**
 * 1. Remove default vertical scrollbar in IE 6/7/8/9.
 * 2. Improve readability and alignment in all browsers.
 */

textarea {
    overflow: auto; /* 1 */
    vertical-align: top; /* 2 */
}

/* ==========================================================================
   Tables
   ========================================================================== */

/**
 * Remove most spacing between table cells.
 */

table {
    border-collapse: collapse;
    border-spacing: 0;
}

  </style>

  <style type="text/css">
  
  html, body {
    margin: 0;
    padding: 0;
    min-height: 100%;
  }
  body {
    background: #fff;
    font-family: "Source Sans Pro", "Helvetica Neueue", Helvetica, sans;
    font-weight: 300;
    font-size: 16px;
    line-height: 1.6em;
  }
  #content {
    width: 70%;
    max-width: 850px;
    float: left;
    padding: 30px 60px;
    border-left: 1px solid #ddd;
  }
  #sidebar {
    width: 25%;
    float: left;
    padding: 30px;
    overflow: hidden;
  }
  #nav {
    font-size: 130%;
    margin: 0 0 15px 0;
  }

  #top {
    display: block;
    position: fixed;
    bottom: 5px;
    left: 5px;
    font-size: .85em;
    text-transform: uppercase;
  }

  #footer {
    font-size: .75em;
    padding: 5px 30px;
    border-top: 1px solid #ddd;
    text-align: right;
  }
    #footer p {
      margin: 0 0 0 30px;
      display: inline-block;
    }

  h1, h2, h3, h4, h5 {
    font-weight: 300;
  }
  h1 {
    font-size: 2.5em;
    line-height: 1.1em;
    margin: 0 0 .50em 0;
  }

  h2 {
    font-size: 1.75em;
    margin: 1em 0 .50em 0;
  }

  h3 {
    margin: 25px 0 10px 0;
  }

  h4 {
    margin: 0;
    font-size: 105%;
  }

  a {
    color: #058;
    text-decoration: none;
    transition: color .3s ease-in-out;
  }

  a:hover {
    color: #e08524;
    transition: color .3s ease-in-out;
  }

  pre, code, .mono, .name {
    font-family: "Ubuntu Mono", "Cousine", "DejaVu Sans Mono", monospace;
  }

  .title .name {
    font-weight: bold;
  }
  .section-title {
    margin-top: 2em;
  }
  .ident {
    color: #900;
  }

  code {
    background: #f9f9f9;
  } 

  pre {
    background: #fefefe;
    border: 1px solid #ddd;
    box-shadow: 2px 2px 0 #f3f3f3;
    margin: 0 30px;
    padding: 15px 30px;
  }

  .codehilite {
    margin: 0 30px 10px 30px;
  }

    .codehilite pre {
      margin: 0;
    }
    .codehilite .err { background: #ff3300; color: #fff !important; } 

  table#module-list {
    font-size: 110%;
  }

    table#module-list tr td:first-child {
      padding-right: 10px;
      white-space: nowrap;
    }

    table#module-list td {
      vertical-align: top;
      padding-bottom: 8px;
    }

      table#module-list td p {
        margin: 0 0 7px 0;
      }

  .def {
    display: table;
  }

    .def p {
      display: table-cell;
      vertical-align: top;
      text-align: left;
    }

    .def p:first-child {
      white-space: nowrap;
    }

    .def p:last-child {
      width: 100%;
    }


  #index {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }
    ul#index .class_name {
      /* font-size: 110%; */
      font-weight: bold;
    }
    #index ul {
      margin: 0;
    }

  .item {
    margin: 0 0 15px 0;
  }

    .item .class {
      margin: 0 0 25px 30px;
    }

      .item .class ul.class_list {
        margin: 0 0 20px 0;
      }

    .item .name {
      background: #fafafa;
      margin: 0;
      font-weight: bold;
      padding: 5px 10px;
      border-radius: 3px;
      display: inline-block;
      min-width: 40%;
    }
      .item .name:hover {
        background: #f6f6f6;
      }

    .item .empty_desc {
      margin: 0 0 5px 0;
      padding: 0;
    }

    .item .inheritance {
      margin: 3px 0 0 30px;
    }

    .item .inherited {
      color: #666;
    }

    .item .desc {
      padding: 0 8px;
      margin: 0;
    }

      .item .desc p {
        margin: 0 0 10px 0;
      }

    .source_cont {
      margin: 0;
      padding: 0;
    }

    .source_link a {
      background: #ffc300;
      font-weight: 400;
      font-size: .75em;
      text-transform: uppercase;
      color: #fff;
      text-shadow: 1px 1px 0 #f4b700;
      
      padding: 3px 8px;
      border-radius: 2px;
      transition: background .3s ease-in-out;
    }
      .source_link a:hover {
        background: #FF7200;
        text-shadow: none;
        transition: background .3s ease-in-out;
      }

    .source {
      display: none;
      max-height: 600px;
      overflow-y: scroll;
      margin-bottom: 15px;
    }

      .source .codehilite {
        margin: 0;
      }

  .desc h1, .desc h2, .desc h3 {
    font-size: 100% !important;
  }
  .clear {
    clear: both;
  }

  @media all and (max-width: 950px) {
    #sidebar {
      width: 35%;
    }
    #content {
      width: 65%;
    }
  }
  @media all and (max-width: 650px) {
    #top {
      display: none;
    }
    #sidebar {
      float: none;
      width: auto;
    }
    #content {
      float: none;
      width: auto;
      padding: 30px;
    }

    #index ul {
      padding: 0;
      margin-bottom: 15px;
    }
    #index ul li {
      display: inline-block;
      margin-right: 30px;
    }
    #footer {
      text-align: left;
    }
    #footer p {
      display: block;
      margin: inherit;
    }
  }

  /*****************************/

  </style>

  <style type="text/css">
  .codehilite .hll { background-color: #ffffcc }
.codehilite  { background: #f8f8f8; }
.codehilite .c { color: #408080; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #FF0000 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666666 } /* Operator */
.codehilite .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #BC7A00 } /* Comment.Preproc */
.codehilite .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #408080; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #408080; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .gr { color: #FF0000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #00A000 } /* Generic.Inserted */
.codehilite .go { color: #888888 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #0044DD } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #7D9029 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.codehilite .no { color: #880000 } /* Name.Constant */
.codehilite .nd { color: #AA22FF } /* Name.Decorator */
.codehilite .ni { color: #999999; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #0000FF } /* Name.Function */
.codehilite .nl { color: #A0A000 } /* Name.Label */
.codehilite .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #bbbbbb } /* Text.Whitespace */
.codehilite .mb { color: #666666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666666 } /* Literal.Number.Float */
.codehilite .mh { color: #666666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666666 } /* Literal.Number.Oct */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #BB6688 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .il { color: #666666 } /* Literal.Number.Integer.Long */
  </style>

  <style type="text/css">
  
/* ==========================================================================
   EXAMPLE Media Queries for Responsive Design.
   These examples override the primary ('mobile first') styles.
   Modify as content requires.
   ========================================================================== */

@media only screen and (min-width: 35em) {
    /* Style adjustments for viewports that meet the condition */
}

@media print,
       (-o-min-device-pixel-ratio: 5/4),
       (-webkit-min-device-pixel-ratio: 1.25),
       (min-resolution: 120dpi) {
    /* Style adjustments for high resolution devices */
}

/* ==========================================================================
   Print styles.
   Inlined to avoid required HTTP connection: h5bp.com/r
   ========================================================================== */

@media print {
    * {
        background: transparent !important;
        color: #000 !important; /* Black prints faster: h5bp.com/s */
        box-shadow: none !important;
        text-shadow: none !important;
    }

    a,
    a:visited {
        text-decoration: underline;
    }

    a[href]:after {
        content: " (" attr(href) ")";
    }

    abbr[title]:after {
        content: " (" attr(title) ")";
    }

    /*
     * Don't show links for images, or javascript/internal links
     */

    .ir a:after,
    a[href^="javascript:"]:after,
    a[href^="#"]:after {
        content: "";
    }

    pre,
    blockquote {
        border: 1px solid #999;
        page-break-inside: avoid;
    }

    thead {
        display: table-header-group; /* h5bp.com/t */
    }

    tr,
    img {
        page-break-inside: avoid;
    }

    img {
        max-width: 100% !important;
    }

    @page {
        margin: 0.5cm;
    }

    p,
    h2,
    h3 {
        orphans: 3;
        widows: 3;
    }

    h2,
    h3 {
        page-break-after: avoid;
    }
}

  </style>

  <script type="text/javascript">
  function toggle(id, $link) {
    $node = document.getElementById(id);
    if (!$node)
    return;
    if (!$node.style.display || $node.style.display == 'none') {
    $node.style.display = 'block';
    $link.innerHTML = 'Hide source &nequiv;';
    } else {
    $node.style.display = 'none';
    $link.innerHTML = 'Show source &equiv;';
    }
  }
  </script>
</head>
<body>
<a href="https://github.com/scikit-optimize/scikit-optimize"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/652c5b9acfaddf3a9c326fa6bde407b87f7be0f4/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6f72616e67655f6666373630302e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_orange_ff7600.png"></a>
<a href="#" id="top">Top</a>

<div id="container">
    
  
  <div id="sidebar">
    <h1>API</h1>
    <ul id="index">


    <li class="set"><h3><a href="#header-classes">Classes</a></h3>
      <ul>
        <li class="mono">
        <span class="class_name"><a href="#skopt.learning.DecisionTreeRegressor">DecisionTreeRegressor</a></span>
        </li>
        <li class="mono">
        <span class="class_name"><a href="#skopt.learning.ExtraTreesRegressor">ExtraTreesRegressor</a></span>
        </li>
        <li class="mono">
        <span class="class_name"><a href="#skopt.learning.GradientBoostingQuantileRegressor">GradientBoostingQuantileRegressor</a></span>
        </li>
        <li class="mono">
        <span class="class_name"><a href="#skopt.learning.RandomForestRegressor">RandomForestRegressor</a></span>
        </li>
      </ul>
    </li>


    <li class="set"><h3><a href="http://scikit-optimize.github.io">Top module</a></h3>
    </li>
    </ul>
  </div>

    <article id="content">
      
  

  


  <header id="section-intro">
  <h1 class="title"><span class="name">skopt.learning</span> module</h1>
  <p>Machine learning extensions for model-based optimisation.</p>
  
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning" class="source">
    <div class="codehilite"><pre><span></span><span class="sd">&quot;&quot;&quot;Machine learning extensions for model-based optimisation.&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">.tree</span> <span class="kn">import</span> <span class="n">DecisionTreeRegressor</span>
<span class="kn">from</span> <span class="nn">.forest</span> <span class="kn">import</span> <span class="n">RandomForestRegressor</span>
<span class="kn">from</span> <span class="nn">.forest</span> <span class="kn">import</span> <span class="n">ExtraTreesRegressor</span>
<span class="kn">from</span> <span class="nn">.gbrt</span> <span class="kn">import</span> <span class="n">GradientBoostingQuantileRegressor</span>

<span class="n">__all__</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;DecisionTreeRegressor&quot;</span><span class="p">,</span>
           <span class="s2">&quot;RandomForestRegressor&quot;</span><span class="p">,</span>
           <span class="s2">&quot;ExtraTreesRegressor&quot;</span><span class="p">,</span>
           <span class="s2">&quot;GradientBoostingQuantileRegressor&quot;</span><span class="p">)</span>
</pre></div>

  </div>

  </header>

  <section id="section-items">


    <h2 class="section-title" id="header-classes">Classes</h2>
      
      <div class="item">
      <p id="skopt.learning.DecisionTreeRegressor" class="name">class <span class="ident">DecisionTreeRegressor</span></p>
      
  
    <div class="desc"><p>DecisionTreeRegressor that supports <code>return_std</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.DecisionTreeRegressor', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.DecisionTreeRegressor" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">DecisionTreeRegressor</span><span class="p">(</span><span class="n">sk_DecisionTreeRegressor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    DecisionTreeRegressor that supports `return_std`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict continuous output for `X`.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `X` [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">            Input data.</span>

<span class="sd">        * `return_std` [bool, default=False]:</span>
<span class="sd">            Whether or not to return the standard deviation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        * `predictions` [array-like, shape=(n_samples,)]:</span>
<span class="sd">            Predicted values for X. If criterion is set to &quot;mse&quot;,</span>
<span class="sd">            then `predictions[i] ~= mean(y | X[i])`.</span>

<span class="sd">        * `std` [array-like, shape=(n_samples,)]:</span>
<span class="sd">            Standard deviation of `y` at `X`. If criterion</span>
<span class="sd">            is set to &quot;mse&quot;, then `std[i] ~= std(y | X[i])`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">DecisionTreeRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_std</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">!=</span> <span class="s2">&quot;mse&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Expected impurity to be &#39;mse&#39;, got </span><span class="si">%s</span><span class="s2"> instead&quot;</span>
                    <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">)</span>

            <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">impurity</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">)]</span> <span class="o">**</span> <span class="mf">0.5</span>

        <span class="k">return</span> <span class="n">mean</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#skopt.learning.DecisionTreeRegressor">DecisionTreeRegressor</a></li>
          <li>sklearn.tree.tree.DecisionTreeRegressor</li>
          <li>sklearn.tree.tree.BaseDecisionTree</li>
          <li>abc.NewBase</li>
          <li>sklearn.base.BaseEstimator</li>
          <li>sklearn.feature_selection.from_model._LearntSelectorMixin</li>
          <li>sklearn.base.TransformerMixin</li>
          <li>sklearn.base.RegressorMixin</li>
          <li>builtins.object</li>
          </ul>
          <h3>Static methods</h3>
            
  <div class="item">
    <div class="name def" id="skopt.learning.DecisionTreeRegressor.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, criterion=&#39;mse&#39;, splitter=&#39;best&#39;, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=None, random_state=None, max_leaf_nodes=None, presort=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Initialize self.  See help(type(self)) for accurate signature.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.DecisionTreeRegressor.__init__', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.DecisionTreeRegressor.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span>
             <span class="n">splitter</span><span class="o">=</span><span class="s2">&quot;best&quot;</span><span class="p">,</span>
             <span class="n">max_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
             <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
             <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
             <span class="n">max_features</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
             <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
             <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
             <span class="n">presort</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">DecisionTreeRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span>
        <span class="n">criterion</span><span class="o">=</span><span class="n">criterion</span><span class="p">,</span>
        <span class="n">splitter</span><span class="o">=</span><span class="n">splitter</span><span class="p">,</span>
        <span class="n">max_depth</span><span class="o">=</span><span class="n">max_depth</span><span class="p">,</span>
        <span class="n">min_samples_split</span><span class="o">=</span><span class="n">min_samples_split</span><span class="p">,</span>
        <span class="n">min_samples_leaf</span><span class="o">=</span><span class="n">min_samples_leaf</span><span class="p">,</span>
        <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="n">min_weight_fraction_leaf</span><span class="p">,</span>
        <span class="n">max_features</span><span class="o">=</span><span class="n">max_features</span><span class="p">,</span>
        <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="n">max_leaf_nodes</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="n">presort</span><span class="o">=</span><span class="n">presort</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.DecisionTreeRegressor.apply">
    <p>def <span class="ident">apply</span>(</p><p>self, X, check_input=True)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the index of the leaf that each sample is predicted as.</p>
<p>.. versionadded:: 0.17</p>
<h2>Parameters</h2>
<p>X : array_like or sparse matrix, shape = [n_samples, n_features]
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<p>check_input : boolean, (default=True)
    Allow to bypass several input checking.
    Don't use this parameter unless you know what you do.</p>
<h2>Returns</h2>
<p>X_leaves : array_like, shape = [n_samples,]
    For each datapoint x in X, return the index of the leaf x
    ends up in. Leaves are numbered within
    <code>[0; self.tree_.node_count)</code>, possibly with gaps in the
    numbering.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.DecisionTreeRegressor.apply', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.DecisionTreeRegressor.apply" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Returns the index of the leaf that each sample is predicted as.</span>
<span class="sd">    .. versionadded:: 0.17</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array_like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>
<span class="sd">    check_input : boolean, (default=True)</span>
<span class="sd">        Allow to bypass several input checking.</span>
<span class="sd">        Don&#39;t use this parameter unless you know what you do.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X_leaves : array_like, shape = [n_samples,]</span>
<span class="sd">        For each datapoint x in X, return the index of the leaf x</span>
<span class="sd">        ends up in. Leaves are numbered within</span>
<span class="sd">        ``[0; self.tree_.node_count)``, possibly with gaps in the</span>
<span class="sd">        numbering.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.DecisionTreeRegressor.decision_path">
    <p>def <span class="ident">decision_path</span>(</p><p>self, X, check_input=True)</p>
    </div>
    

    
  
    <div class="desc"><p>Return the decision path in the tree</p>
<h2>Parameters</h2>
<p>X : array_like or sparse matrix, shape = [n_samples, n_features]
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<p>check_input : boolean, (default=True)
    Allow to bypass several input checking.
    Don't use this parameter unless you know what you do.</p>
<h2>Returns</h2>
<p>indicator : sparse csr array, shape = [n_samples, n_nodes]
    Return a node indicator matrix where non zero elements
    indicates that the samples goes through the nodes.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.DecisionTreeRegressor.decision_path', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.DecisionTreeRegressor.decision_path" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">decision_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return the decision path in the tree</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array_like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>
<span class="sd">    check_input : boolean, (default=True)</span>
<span class="sd">        Allow to bypass several input checking.</span>
<span class="sd">        Don&#39;t use this parameter unless you know what you do.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    indicator : sparse csr array, shape = [n_samples, n_nodes]</span>
<span class="sd">        Return a node indicator matrix where non zero elements</span>
<span class="sd">        indicates that the samples goes through the nodes.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">decision_path</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.DecisionTreeRegressor.fit">
    <p>def <span class="ident">fit</span>(</p><p>self, X, y, sample_weight=None, check_input=True, X_idx_sorted=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Build a decision tree from the training set (X, y).</p>
<h2>Parameters</h2>
<p>X : array-like or sparse matrix, shape = [n_samples, n_features]
    The training input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csc_matrix</code>.</p>
<p>y : array-like, shape = [n_samples] or [n_samples, n_outputs]
    The target values (class labels in classification, real numbers in
    regression). In the regression case, use <code>dtype=np.float64</code> and
    <code>order='C'</code> for maximum efficiency.</p>
<p>sample_weight : array-like, shape = [n_samples] or None
    Sample weights. If None, then samples are equally weighted. Splits
    that would create child nodes with net zero or negative weight are
    ignored while searching for a split in each node. In the case of
    classification, splits are also ignored if they would result in any
    single class carrying a negative weight in either child node.</p>
<p>check_input : boolean, (default=True)
    Allow to bypass several input checking.
    Don't use this parameter unless you know what you do.</p>
<p>X_idx_sorted : array-like, shape = [n_samples, n_features], optional
    The indexes of the sorted training input samples. If many tree
    are grown on the same dataset, this allows the ordering to be
    cached between trees. If None, the data will be sorted here.
    Don't use this parameter unless you know what to do.</p>
<h2>Returns</h2>
<p>self : object
    Returns self.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.DecisionTreeRegressor.fit', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.DecisionTreeRegressor.fit" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
        <span class="n">X_idx_sorted</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build a decision tree from the training set (X, y).</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">        The training input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csc_matrix``.</span>
<span class="sd">    y : array-like, shape = [n_samples] or [n_samples, n_outputs]</span>
<span class="sd">        The target values (class labels in classification, real numbers in</span>
<span class="sd">        regression). In the regression case, use ``dtype=np.float64`` and</span>
<span class="sd">        ``order=&#39;C&#39;`` for maximum efficiency.</span>
<span class="sd">    sample_weight : array-like, shape = [n_samples] or None</span>
<span class="sd">        Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">        that would create child nodes with net zero or negative weight are</span>
<span class="sd">        ignored while searching for a split in each node. In the case of</span>
<span class="sd">        classification, splits are also ignored if they would result in any</span>
<span class="sd">        single class carrying a negative weight in either child node.</span>
<span class="sd">    check_input : boolean, (default=True)</span>
<span class="sd">        Allow to bypass several input checking.</span>
<span class="sd">        Don&#39;t use this parameter unless you know what you do.</span>
<span class="sd">    X_idx_sorted : array-like, shape = [n_samples, n_features], optional</span>
<span class="sd">        The indexes of the sorted training input samples. If many tree</span>
<span class="sd">        are grown on the same dataset, this allows the ordering to be</span>
<span class="sd">        cached between trees. If None, the data will be sorted here.</span>
<span class="sd">        Don&#39;t use this parameter unless you know what to do.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    self : object</span>
<span class="sd">        Returns self.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">check_input</span><span class="p">:</span>
        <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csc&quot;</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
            <span class="n">X</span><span class="o">.</span><span class="n">sort_indices</span><span class="p">()</span>
            <span class="k">if</span> <span class="n">X</span><span class="o">.</span><span class="n">indices</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">intc</span> <span class="ow">or</span> <span class="n">X</span><span class="o">.</span><span class="n">indptr</span><span class="o">.</span><span class="n">dtype</span> <span class="o">!=</span> <span class="n">np</span><span class="o">.</span><span class="n">intc</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;No support for np.int64 index based &quot;</span>
                                 <span class="s2">&quot;sparse matrices&quot;</span><span class="p">)</span>
    <span class="c1"># Determine output settings</span>
    <span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">is_classification</span> <span class="o">=</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ClassifierMixin</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">expanded_class_weight</span> <span class="o">=</span> <span class="bp">None</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># reshape is necessary to preserve the data contiguity against vs</span>
        <span class="c1"># [:, np.newaxis] that does not.</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="k">if</span> <span class="n">is_classification</span><span class="p">:</span>
        <span class="n">check_classification_targets</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">y_original</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
        <span class="n">y_encoded</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">):</span>
            <span class="n">classes_k</span><span class="p">,</span> <span class="n">y_encoded</span><span class="p">[:,</span> <span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">[:,</span> <span class="n">k</span><span class="p">],</span>
                                                   <span class="n">return_inverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">classes_k</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">classes_k</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">y_encoded</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">expanded_class_weight</span> <span class="o">=</span> <span class="n">compute_sample_weight</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">,</span> <span class="n">y_original</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="p">[</span><span class="bp">None</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">intp</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span> <span class="o">!=</span> <span class="n">DOUBLE</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">y</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">contiguous</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DOUBLE</span><span class="p">)</span>
    <span class="c1"># Check parameters</span>
    <span class="n">max_depth</span> <span class="o">=</span> <span class="p">((</span><span class="mi">2</span> <span class="o">**</span> <span class="mi">31</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="ow">is</span> <span class="bp">None</span>
                 <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span><span class="p">)</span>
    <span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="ow">is</span> <span class="bp">None</span>
                      <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span><span class="p">,</span> <span class="p">(</span><span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)):</span>
        <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># float</span>
        <span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span><span class="p">,</span> <span class="p">(</span><span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)):</span>
        <span class="n">min_samples_split</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># float</span>
        <span class="n">min_samples_split</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">ceil</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">*</span> <span class="n">n_samples</span><span class="p">))</span>
        <span class="n">min_samples_split</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="p">)</span>
    <span class="n">min_samples_split</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">min_samples_split</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">min_samples_leaf</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_features</span><span class="p">,</span> <span class="n">six</span><span class="o">.</span><span class="n">string_types</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">==</span> <span class="s2">&quot;auto&quot;</span><span class="p">:</span>
            <span class="k">if</span> <span class="n">is_classification</span><span class="p">:</span>
                <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">)))</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">max_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">==</span> <span class="s2">&quot;sqrt&quot;</span><span class="p">:</span>
            <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">)))</span>
        <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">==</span> <span class="s2">&quot;log2&quot;</span><span class="p">:</span>
            <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">)))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s1">&#39;Invalid value for max_features. Allowed string &#39;</span>
                <span class="s1">&#39;values are &quot;auto&quot;, &quot;sqrt&quot; or &quot;log2&quot;.&#39;</span><span class="p">)</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">max_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span>
    <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_features</span><span class="p">,</span> <span class="p">(</span><span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)):</span>
        <span class="n">max_features</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span>
    <span class="k">else</span><span class="p">:</span>  <span class="c1"># float</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">&gt;</span> <span class="mf">0.0</span><span class="p">:</span>
            <span class="n">max_features</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span>
                               <span class="nb">int</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">max_features</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_features_</span> <span class="o">=</span> <span class="n">max_features</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)</span> <span class="o">!=</span> <span class="n">n_samples</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Number of labels=</span><span class="si">%d</span><span class="s2"> does not match &quot;</span>
                         <span class="s2">&quot;number of samples=</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">),</span> <span class="n">n_samples</span><span class="p">))</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mf">0.</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">&lt;=</span> <span class="mf">1.</span> <span class="ow">or</span>
            <span class="mi">2</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;min_samples_split must be in at least 2&quot;</span>
                         <span class="s2">&quot; or in (0, 1], got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">min_samples_split</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mf">0.</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">&lt;=</span> <span class="mf">0.5</span> <span class="ow">or</span>
            <span class="mi">1</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;min_samples_leaf must be at least than 1 &quot;</span>
                         <span class="s2">&quot;or in (0, 0.5], got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">min_samples_leaf</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="mi">0</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">&lt;=</span> <span class="mf">0.5</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;min_weight_fraction_leaf must in [0, 0.5]&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">max_depth</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;max_depth must be greater than zero. &quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="mi">0</span> <span class="o">&lt;</span> <span class="n">max_features</span> <span class="o">&lt;=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;max_features must be in (0, n_features]&quot;</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">max_leaf_nodes</span><span class="p">,</span> <span class="p">(</span><span class="n">numbers</span><span class="o">.</span><span class="n">Integral</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">integer</span><span class="p">)):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;max_leaf_nodes must be integral number but was &quot;</span>
                         <span class="s2">&quot;</span><span class="si">%r</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="n">max_leaf_nodes</span><span class="p">)</span>
    <span class="k">if</span> <span class="o">-</span><span class="mi">1</span> <span class="o">&lt;</span> <span class="n">max_leaf_nodes</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">((</span><span class="s2">&quot;max_leaf_nodes {0} must be either smaller than &quot;</span>
                          <span class="s2">&quot;0 or larger than 1&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">max_leaf_nodes</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="p">(</span><span class="nb">getattr</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span> <span class="o">!=</span> <span class="n">DOUBLE</span> <span class="ow">or</span>
                <span class="ow">not</span> <span class="n">sample_weight</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">contiguous</span><span class="p">):</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span>
                <span class="n">sample_weight</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DOUBLE</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Sample weights array has more &quot;</span>
                             <span class="s2">&quot;than one dimension: </span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
                             <span class="nb">len</span><span class="p">(</span><span class="n">sample_weight</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">)</span> <span class="o">!=</span> <span class="n">n_samples</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Number of weights=</span><span class="si">%d</span><span class="s2"> does not match &quot;</span>
                             <span class="s2">&quot;number of samples=</span><span class="si">%d</span><span class="s2">&quot;</span> <span class="o">%</span>
                             <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">),</span> <span class="n">n_samples</span><span class="p">))</span>
    <span class="k">if</span> <span class="n">expanded_class_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">expanded_class_weight</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">expanded_class_weight</span>
    <span class="c1"># Set min_weight_leaf from min_weight_fraction_leaf</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">!=</span> <span class="mf">0.</span> <span class="ow">and</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">min_weight_leaf</span> <span class="o">=</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">*</span>
                           <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">sample_weight</span><span class="p">))</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">min_weight_leaf</span> <span class="o">=</span> <span class="mf">0.</span>
    <span class="n">presort</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">presort</span>
    <span class="c1"># Allow presort to be &#39;auto&#39;, which means True if the dataset is dense,</span>
    <span class="c1"># otherwise it will be False.</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">presort</span> <span class="o">==</span> <span class="s1">&#39;auto&#39;</span> <span class="ow">and</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="n">presort</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">elif</span> <span class="bp">self</span><span class="o">.</span><span class="n">presort</span> <span class="o">==</span> <span class="s1">&#39;auto&#39;</span><span class="p">:</span>
        <span class="n">presort</span> <span class="o">=</span> <span class="bp">True</span>
    <span class="k">if</span> <span class="n">presort</span> <span class="ow">is</span> <span class="bp">True</span> <span class="ow">and</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Presorting is not supported for sparse &quot;</span>
                         <span class="s2">&quot;matrices.&quot;</span><span class="p">)</span>
    <span class="c1"># If multiple trees are built on the same dataset, we only want to</span>
    <span class="c1"># presort once. Splitters now can accept presorted indices if desired,</span>
    <span class="c1"># but do not handle any presorting themselves. Ensemble algorithms</span>
    <span class="c1"># which desire presorting must do presorting themselves and pass that</span>
    <span class="c1"># matrix into each tree.</span>
    <span class="k">if</span> <span class="n">X_idx_sorted</span> <span class="ow">is</span> <span class="bp">None</span> <span class="ow">and</span> <span class="n">presort</span><span class="p">:</span>
        <span class="n">X_idx_sorted</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asfortranarray</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">),</span>
                                         <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">int32</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">presort</span> <span class="ow">and</span> <span class="n">X_idx_sorted</span><span class="o">.</span><span class="n">shape</span> <span class="o">!=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;The shape of X (X.shape = {}) doesn&#39;t match &quot;</span>
                         <span class="s2">&quot;the shape of X_idx_sorted (X_idx_sorted&quot;</span>
                         <span class="s2">&quot;.shape = {})&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span>
                                               <span class="n">X_idx_sorted</span><span class="o">.</span><span class="n">shape</span><span class="p">))</span>
    <span class="c1"># Build tree</span>
    <span class="n">criterion</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">criterion</span><span class="p">,</span> <span class="n">Criterion</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">is_classification</span><span class="p">:</span>
            <span class="n">criterion</span> <span class="o">=</span> <span class="n">CRITERIA_CLF</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">,</span>
                                                     <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">criterion</span> <span class="o">=</span> <span class="n">CRITERIA_REG</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">](</span><span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">)</span>
    <span class="n">SPLITTERS</span> <span class="o">=</span> <span class="n">SPARSE_SPLITTERS</span> <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">else</span> <span class="n">DENSE_SPLITTERS</span>
    <span class="n">splitter</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">splitter</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">splitter</span><span class="p">,</span> <span class="n">Splitter</span><span class="p">):</span>
        <span class="n">splitter</span> <span class="o">=</span> <span class="n">SPLITTERS</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">splitter</span><span class="p">](</span><span class="n">criterion</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">max_features_</span><span class="p">,</span>
                                            <span class="n">min_samples_leaf</span><span class="p">,</span>
                                            <span class="n">min_weight_leaf</span><span class="p">,</span>
                                            <span class="n">random_state</span><span class="p">,</span>
                                            <span class="bp">self</span><span class="o">.</span><span class="n">presort</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tree_</span> <span class="o">=</span> <span class="n">Tree</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span><span class="p">)</span>
    <span class="c1"># Use BestFirst if max_leaf_nodes given; use DepthFirst otherwise</span>
    <span class="k">if</span> <span class="n">max_leaf_nodes</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">builder</span> <span class="o">=</span> <span class="n">DepthFirstTreeBuilder</span><span class="p">(</span><span class="n">splitter</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="p">,</span>
                                        <span class="n">min_samples_leaf</span><span class="p">,</span>
                                        <span class="n">min_weight_leaf</span><span class="p">,</span>
                                        <span class="n">max_depth</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">builder</span> <span class="o">=</span> <span class="n">BestFirstTreeBuilder</span><span class="p">(</span><span class="n">splitter</span><span class="p">,</span> <span class="n">min_samples_split</span><span class="p">,</span>
                                       <span class="n">min_samples_leaf</span><span class="p">,</span>
                                       <span class="n">min_weight_leaf</span><span class="p">,</span>
                                       <span class="n">max_depth</span><span class="p">,</span>
                                       <span class="n">max_leaf_nodes</span><span class="p">)</span>
    <span class="n">builder</span><span class="o">.</span><span class="n">build</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tree_</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">X_idx_sorted</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.DecisionTreeRegressor.fit_transform">
    <p>def <span class="ident">fit_transform</span>(</p><p>self, X, y=None, **fit_params)</p>
    </div>
    

    
  
    <div class="desc"><p>Fit to data, then transform it.</p>
<p>Fits transformer to X and y with optional parameters fit_params
and returns a transformed version of X.</p>
<h2>Parameters</h2>
<p>X : numpy array of shape [n_samples, n_features]
    Training set.</p>
<p>y : numpy array of shape [n_samples]
    Target values.</p>
<h2>Returns</h2>
<p>X_new : numpy array of shape [n_samples, n_features_new]
    Transformed array.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.DecisionTreeRegressor.fit_transform', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.DecisionTreeRegressor.fit_transform" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fit to data, then transform it.</span>
<span class="sd">    Fits transformer to X and y with optional parameters fit_params</span>
<span class="sd">    and returns a transformed version of X.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : numpy array of shape [n_samples, n_features]</span>
<span class="sd">        Training set.</span>
<span class="sd">    y : numpy array of shape [n_samples]</span>
<span class="sd">        Target values.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X_new : numpy array of shape [n_samples, n_features_new]</span>
<span class="sd">        Transformed array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># non-optimized default implementation; override when a better</span>
    <span class="c1"># method is possible for a given clustering algorithm</span>
    <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># fit method of arity 1 (unsupervised transformation)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># fit method of arity 2 (supervised transformation)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.DecisionTreeRegressor.get_params">
    <p>def <span class="ident">get_params</span>(</p><p>self, deep=True)</p>
    </div>
    

    
  
    <div class="desc"><p>Get parameters for this estimator.</p>
<h2>Parameters</h2>
<p>deep: boolean, optional
    If True, will return the parameters for this estimator and
    contained subobjects that are estimators.</p>
<h2>Returns</h2>
<p>params : mapping of string to any
    Parameter names mapped to their values.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.DecisionTreeRegressor.get_params', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.DecisionTreeRegressor.get_params" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deep</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get parameters for this estimator.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    deep: boolean, optional</span>
<span class="sd">        If True, will return the parameters for this estimator and</span>
<span class="sd">        contained subobjects that are estimators.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    params : mapping of string to any</span>
<span class="sd">        Parameter names mapped to their values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_param_names</span><span class="p">():</span>
        <span class="c1"># We need deprecation warnings to always be on in order to</span>
        <span class="c1"># catch deprecated param values.</span>
        <span class="c1"># This is set in utils/__init__.py but it gets overwritten</span>
        <span class="c1"># when running under python3 somehow.</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;always&quot;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">(</span><span class="n">record</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">w</span><span class="p">:</span>
                <span class="n">value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="ow">and</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">category</span> <span class="o">==</span> <span class="ne">DeprecationWarning</span><span class="p">:</span>
                <span class="c1"># if the parameter is deprecated, don&#39;t show it</span>
                <span class="k">continue</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">filters</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># XXX: should we rather test if instance of estimator?</span>
        <span class="k">if</span> <span class="n">deep</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s1">&#39;get_params&#39;</span><span class="p">):</span>
            <span class="n">deep_items</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="n">out</span><span class="o">.</span><span class="n">update</span><span class="p">((</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;__&#39;</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">deep_items</span><span class="p">)</span>
        <span class="n">out</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.DecisionTreeRegressor.predict">
    <p>def <span class="ident">predict</span>(</p><p>self, X, return_std=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Predict continuous output for <code>X</code>.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>X</code> [array-like, shape=(n_samples, n_features)]:
    Input data.</p>
</li>
<li>
<p><code>return_std</code> [bool, default=False]:
    Whether or not to return the standard deviation.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li>
<p><code>predictions</code> [array-like, shape=(n_samples,)]:
    Predicted values for X. If criterion is set to "mse",
    then <code>predictions[i] ~= mean(y | X[i])</code>.</p>
</li>
<li>
<p><code>std</code> [array-like, shape=(n_samples,)]:
    Standard deviation of <code>y</code> at <code>X</code>. If criterion
    is set to "mse", then <code>std[i] ~= std(y | X[i])</code>.</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.DecisionTreeRegressor.predict', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.DecisionTreeRegressor.predict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Predict continuous output for `X`.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X` [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">        Input data.</span>
<span class="sd">    * `return_std` [bool, default=False]:</span>
<span class="sd">        Whether or not to return the standard deviation.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `predictions` [array-like, shape=(n_samples,)]:</span>
<span class="sd">        Predicted values for X. If criterion is set to &quot;mse&quot;,</span>
<span class="sd">        then `predictions[i] ~= mean(y | X[i])`.</span>
<span class="sd">    * `std` [array-like, shape=(n_samples,)]:</span>
<span class="sd">        Standard deviation of `y` at `X`. If criterion</span>
<span class="sd">        is set to &quot;mse&quot;, then `std[i] ~= std(y | X[i])`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">DecisionTreeRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_std</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">!=</span> <span class="s2">&quot;mse&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected impurity to be &#39;mse&#39;, got </span><span class="si">%s</span><span class="s2"> instead&quot;</span>
                <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">impurity</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">)]</span> <span class="o">**</span> <span class="mf">0.5</span>
    <span class="k">return</span> <span class="n">mean</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.DecisionTreeRegressor.score">
    <p>def <span class="ident">score</span>(</p><p>self, X, y, sample_weight=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the coefficient of determination R^2 of the prediction.</p>
<p>The coefficient R^2 is defined as (1 - u/v), where u is the regression
sum of squares ((y_true - y_pred) <strong> 2).sum() and v is the residual
sum of squares ((y_true - y_true.mean()) </strong> 2).sum().
Best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features,
would get a R^2 score of 0.0.</p>
<h2>Parameters</h2>
<p>X : array-like, shape = (n_samples, n_features)
    Test samples.</p>
<p>y : array-like, shape = (n_samples) or (n_samples, n_outputs)
    True values for X.</p>
<p>sample_weight : array-like, shape = [n_samples], optional
    Sample weights.</p>
<h2>Returns</h2>
<p>score : float
    R^2 of self.predict(X) wrt. y.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.DecisionTreeRegressor.score', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.DecisionTreeRegressor.score" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the coefficient of determination R^2 of the prediction.</span>
<span class="sd">    The coefficient R^2 is defined as (1 - u/v), where u is the regression</span>
<span class="sd">    sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual</span>
<span class="sd">    sum of squares ((y_true - y_true.mean()) ** 2).sum().</span>
<span class="sd">    Best possible score is 1.0 and it can be negative (because the</span>
<span class="sd">    model can be arbitrarily worse). A constant model that always</span>
<span class="sd">    predicts the expected value of y, disregarding the input features,</span>
<span class="sd">    would get a R^2 score of 0.0.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like, shape = (n_samples, n_features)</span>
<span class="sd">        Test samples.</span>
<span class="sd">    y : array-like, shape = (n_samples) or (n_samples, n_outputs)</span>
<span class="sd">        True values for X.</span>
<span class="sd">    sample_weight : array-like, shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    score : float</span>
<span class="sd">        R^2 of self.predict(X) wrt. y.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
    <span class="k">return</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                    <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;variance_weighted&#39;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.DecisionTreeRegressor.set_params">
    <p>def <span class="ident">set_params</span>(</p><p>self, **params)</p>
    </div>
    

    
  
    <div class="desc"><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<h2>Returns</h2>
<p>self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.DecisionTreeRegressor.set_params', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.DecisionTreeRegressor.set_params" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set the parameters of this estimator.</span>
<span class="sd">    The method works on simple estimators as well as on nested objects</span>
<span class="sd">    (such as pipelines). The latter have parameters of the form</span>
<span class="sd">    ``&lt;component&gt;__&lt;parameter&gt;`` so that it&#39;s possible to update each</span>
<span class="sd">    component of a nested object.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">params</span><span class="p">:</span>
        <span class="c1"># Simple optimisation to gain speed (inspect is slow)</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="n">valid_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">(</span><span class="n">deep</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="n">split</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;__&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">split</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># nested objects case</span>
            <span class="n">name</span><span class="p">,</span> <span class="n">sub_name</span> <span class="o">=</span> <span class="n">split</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_params</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid parameter </span><span class="si">%s</span><span class="s1"> for estimator </span><span class="si">%s</span><span class="s1">. &#39;</span>
                                 <span class="s1">&#39;Check the list of available parameters &#39;</span>
                                 <span class="s1">&#39;with `estimator.get_params().keys()`.&#39;</span> <span class="o">%</span>
                                 <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">))</span>
            <span class="n">sub_object</span> <span class="o">=</span> <span class="n">valid_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
            <span class="n">sub_object</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">sub_name</span><span class="p">:</span> <span class="n">value</span><span class="p">})</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># simple objects case</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_params</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid parameter </span><span class="si">%s</span><span class="s1"> for estimator </span><span class="si">%s</span><span class="s1">. &#39;</span>
                                 <span class="s1">&#39;Check the list of available parameters &#39;</span>
                                 <span class="s1">&#39;with `estimator.get_params().keys()`.&#39;</span> <span class="o">%</span>
                                 <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span><span class="p">))</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.DecisionTreeRegressor.transform">
    <p>def <span class="ident">transform</span>(</p><p>*args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.</p>
<p>Reduce X to its most important features.</p>
<div class="codehilite"><pre><span></span>    Uses ``coef_`` or ``feature_importances_`` to determine the most
    important features.  For models with a ``coef_`` for each class, the
    absolute sum over the classes is used.

    Parameters
    ----------
    X : array or scipy sparse matrix of shape [n_samples, n_features]
        The input samples.

    threshold : string, float or None, optional (default=None)
        The threshold value to use for feature selection. Features whose
        importance is greater or equal are kept while the others are
        discarded. If &quot;median&quot; (resp. &quot;mean&quot;), then the threshold value is
        the median (resp. the mean) of the feature importances. A scaling
        factor (e.g., &quot;1.25*mean&quot;) may also be used. If None and if
        available, the object attribute ``threshold`` is used. Otherwise,
        &quot;mean&quot; is used by default.

    Returns
    -------
    X_r : array of shape [n_samples, n_selected_features]
        The input samples with only the selected features.
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.DecisionTreeRegressor.transform', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.DecisionTreeRegressor.transform" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">DeprecationWarning</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fun</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
          <h3>Instance variables</h3>
            <div class="item">
            <p id="skopt.learning.DecisionTreeRegressor.feature_importances_" class="name">var <span class="ident">feature_importances_</span></p>
            

            
  
    <div class="desc"><p>Return the feature importances.</p>
<p>The importance of a feature is computed as the (normalized) total
reduction of the criterion brought by that feature.
It is also known as the Gini importance.</p>
<h2>Returns</h2>
<p>feature_importances_ : array, shape = [n_features]</p></div>
  <div class="source_cont">
</div>

            </div>
      </div>
      </div>
      
      <div class="item">
      <p id="skopt.learning.ExtraTreesRegressor" class="name">class <span class="ident">ExtraTreesRegressor</span></p>
      
  
    <div class="desc"><p>ExtraTreesRegressor that supports <code>return_std</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.ExtraTreesRegressor', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.ExtraTreesRegressor" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">ExtraTreesRegressor</span><span class="p">(</span><span class="n">sk_ExtraTreesRegressor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    ExtraTreesRegressor that supports `return_std`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict continuous output for X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `X` [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">            Input data.</span>

<span class="sd">        * `return_std` [bool, default False]:</span>
<span class="sd">            Whether or not to return the standard deviation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        * `predictions` [array-like, shape=(n_samples,)]:</span>
<span class="sd">            Predicted values for X. If criterion is set to &quot;mse&quot;,</span>
<span class="sd">            then `predictions[i] ~= mean(y | X[i])`.</span>

<span class="sd">        * `std` [array-like, shape=(n_samples,)]:</span>
<span class="sd">            Standard deviation of `y` at `X`. If criterion</span>
<span class="sd">            is set to &quot;mse&quot;, then `std[i] ~= std(y | X[i])`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">ExtraTreesRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_std</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">!=</span> <span class="s2">&quot;mse&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Expected impurity to be &#39;mse&#39;, got </span><span class="si">%s</span><span class="s2"> instead&quot;</span>
                    <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">)</span>

            <span class="c1"># This derives std(y | x) as described in 4.3.2 of arXiv:1211.0906</span>
            <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

            <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
                <span class="n">var_tree</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">impurity</span><span class="p">[</span><span class="n">tree</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">)]</span>
                <span class="n">mean_tree</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                <span class="n">std</span> <span class="o">+=</span> <span class="n">var_tree</span> <span class="o">+</span> <span class="n">mean_tree</span> <span class="o">**</span> <span class="mi">2</span>

            <span class="n">std</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
            <span class="n">std</span> <span class="o">-=</span> <span class="n">mean</span> <span class="o">**</span> <span class="mf">2.0</span>
            <span class="n">std</span><span class="p">[</span><span class="n">std</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="n">std</span> <span class="o">=</span> <span class="n">std</span> <span class="o">**</span> <span class="mf">0.5</span>

            <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span>

        <span class="k">return</span> <span class="n">mean</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#skopt.learning.ExtraTreesRegressor">ExtraTreesRegressor</a></li>
          <li>sklearn.ensemble.forest.ExtraTreesRegressor</li>
          <li>sklearn.ensemble.forest.ForestRegressor</li>
          <li>abc.NewBase</li>
          <li>sklearn.ensemble.forest.BaseForest</li>
          <li>abc.NewBase</li>
          <li>sklearn.ensemble.base.BaseEnsemble</li>
          <li>sklearn.base.BaseEstimator</li>
          <li>sklearn.base.MetaEstimatorMixin</li>
          <li>sklearn.feature_selection.from_model._LearntSelectorMixin</li>
          <li>sklearn.base.TransformerMixin</li>
          <li>sklearn.base.RegressorMixin</li>
          <li>builtins.object</li>
          </ul>
          <h3>Static methods</h3>
            
  <div class="item">
    <div class="name def" id="skopt.learning.ExtraTreesRegressor.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, n_estimators=10, criterion=&#39;mse&#39;, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=&#39;auto&#39;, max_leaf_nodes=None, bootstrap=False, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Initialize self.  See help(type(self)) for accurate signature.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.ExtraTreesRegressor.__init__', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.ExtraTreesRegressor.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
             <span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span>
             <span class="n">max_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
             <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
             <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
             <span class="n">max_features</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
             <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
             <span class="n">bootstrap</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
             <span class="n">oob_score</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
             <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
             <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
             <span class="n">warm_start</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ExtraTreesRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span>
        <span class="n">base_estimator</span><span class="o">=</span><span class="n">ExtraTreeRegressor</span><span class="p">(),</span>
        <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
        <span class="n">estimator_params</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;criterion&quot;</span><span class="p">,</span> <span class="s2">&quot;max_depth&quot;</span><span class="p">,</span> <span class="s2">&quot;min_samples_split&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;min_samples_leaf&quot;</span><span class="p">,</span> <span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;max_features&quot;</span><span class="p">,</span> <span class="s2">&quot;max_leaf_nodes&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;random_state&quot;</span><span class="p">),</span>
        <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
        <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
        <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.ExtraTreesRegressor.apply">
    <p>def <span class="ident">apply</span>(</p><p>self, X)</p>
    </div>
    

    
  
    <div class="desc"><p>Apply trees in the forest to X, return leaf indices.</p>
<h2>Parameters</h2>
<p>X : array-like or sparse matrix, shape = [n_samples, n_features]
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<h2>Returns</h2>
<p>X_leaves : array_like, shape = [n_samples, n_estimators]
    For each datapoint x in X and for each tree in the forest,
    return the index of the leaf x ends up in.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.ExtraTreesRegressor.apply', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.ExtraTreesRegressor.apply" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply trees in the forest to X, return leaf indices.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X_leaves : array_like, shape = [n_samples, n_estimators]</span>
<span class="sd">        For each datapoint x in X and for each tree in the forest,</span>
<span class="sd">        return the index of the leaf x ends up in.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                       <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;threading&quot;</span><span class="p">)(</span>
        <span class="n">delayed</span><span class="p">(</span><span class="n">parallel_helper</span><span class="p">)(</span><span class="n">tree</span><span class="p">,</span> <span class="s1">&#39;apply&#39;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.ExtraTreesRegressor.decision_path">
    <p>def <span class="ident">decision_path</span>(</p><p>self, X)</p>
    </div>
    

    
  
    <div class="desc"><p>Return the decision path in the forest</p>
<h2>Parameters</h2>
<p>X : array-like or sparse matrix, shape = [n_samples, n_features]
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<h2>Returns</h2>
<p>indicator : sparse csr array, shape = [n_samples, n_nodes]
    Return a node indicator matrix where non zero elements
    indicates that the samples goes through the nodes.</p>
<p>n_nodes_ptr : array of size (n_estimators + 1, )
    The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]
    gives the indicator value for the i-th estimator.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.ExtraTreesRegressor.decision_path', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.ExtraTreesRegressor.decision_path" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">decision_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return the decision path in the forest</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    indicator : sparse csr array, shape = [n_samples, n_nodes]</span>
<span class="sd">        Return a node indicator matrix where non zero elements</span>
<span class="sd">        indicates that the samples goes through the nodes.</span>
<span class="sd">    n_nodes_ptr : array of size (n_estimators + 1, )</span>
<span class="sd">        The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]</span>
<span class="sd">        gives the indicator value for the i-th estimator.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">indicators</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                          <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;threading&quot;</span><span class="p">)(</span>
        <span class="n">delayed</span><span class="p">(</span><span class="n">parallel_helper</span><span class="p">)(</span><span class="n">tree</span><span class="p">,</span> <span class="s1">&#39;decision_path&#39;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span>
                                  <span class="n">check_input</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
    <span class="n">n_nodes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">n_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indicators</span><span class="p">])</span>
    <span class="n">n_nodes_ptr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">)</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">sparse_hstack</span><span class="p">(</span><span class="n">indicators</span><span class="p">)</span><span class="o">.</span><span class="n">tocsr</span><span class="p">(),</span> <span class="n">n_nodes_ptr</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.ExtraTreesRegressor.fit">
    <p>def <span class="ident">fit</span>(</p><p>self, X, y, sample_weight=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Build a forest of trees from the training set (X, y).</p>
<h2>Parameters</h2>
<p>X : array-like or sparse matrix of shape = [n_samples, n_features]
    The training input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csc_matrix</code>.</p>
<p>y : array-like, shape = [n_samples] or [n_samples, n_outputs]
    The target values (class labels in classification, real numbers in
    regression).</p>
<p>sample_weight : array-like, shape = [n_samples] or None
    Sample weights. If None, then samples are equally weighted. Splits
    that would create child nodes with net zero or negative weight are
    ignored while searching for a split in each node. In the case of
    classification, splits are also ignored if they would result in any
    single class carrying a negative weight in either child node.</p>
<h2>Returns</h2>
<p>self : object
    Returns self.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.ExtraTreesRegressor.fit', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.ExtraTreesRegressor.fit" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build a forest of trees from the training set (X, y).</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or sparse matrix of shape = [n_samples, n_features]</span>
<span class="sd">        The training input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csc_matrix``.</span>
<span class="sd">    y : array-like, shape = [n_samples] or [n_samples, n_outputs]</span>
<span class="sd">        The target values (class labels in classification, real numbers in</span>
<span class="sd">        regression).</span>
<span class="sd">    sample_weight : array-like, shape = [n_samples] or None</span>
<span class="sd">        Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">        that would create child nodes with net zero or negative weight are</span>
<span class="sd">        ignored while searching for a split in each node. In the case of</span>
<span class="sd">        classification, splits are also ignored if they would result in any</span>
<span class="sd">        single class carrying a negative weight in either child node.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    self : object</span>
<span class="sd">        Returns self.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Validate or convert input data</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csc&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csc&#39;</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="c1"># Pre-sort indices to avoid that each individual tree of the</span>
        <span class="c1"># ensemble sorts the indices.</span>
        <span class="n">X</span><span class="o">.</span><span class="n">sort_indices</span><span class="p">()</span>
    <span class="c1"># Remap output</span>
    <span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;A column-vector y was passed when a 1d array was&quot;</span>
             <span class="s2">&quot; expected. Please change the shape of y to &quot;</span>
             <span class="s2">&quot;(n_samples,), for example using ravel().&quot;</span><span class="p">,</span>
             <span class="n">DataConversionWarning</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># reshape is necessary to preserve the data contiguity against vs</span>
        <span class="c1"># [:, np.newaxis] that does not.</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">expanded_class_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_y_class_weight</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span> <span class="o">!=</span> <span class="n">DOUBLE</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">y</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">contiguous</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DOUBLE</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">expanded_class_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">expanded_class_weight</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">expanded_class_weight</span>
    <span class="c1"># Check parameters</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_validate_estimator</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">bootstrap</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_score</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Out of bag estimation only available&quot;</span>
                         <span class="s2">&quot; if bootstrap=True&quot;</span><span class="p">)</span>
    <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span><span class="p">:</span>
        <span class="c1"># Free allocated memory, if any</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n_more_estimators</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">n_more_estimators</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;n_estimators=</span><span class="si">%d</span><span class="s1"> must be larger or equal to &#39;</span>
                         <span class="s1">&#39;len(estimators_)=</span><span class="si">%d</span><span class="s1"> when warm_start==True&#39;</span>
                         <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="n">n_more_estimators</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Warm-start fitting without increasing n_estimators does not &quot;</span>
             <span class="s2">&quot;fit new trees.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># We draw from the random state to get the random state we</span>
            <span class="c1"># would have got if we hadn&#39;t used a warm_start.</span>
            <span class="n">random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">MAX_INT</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">))</span>
        <span class="n">trees</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_more_estimators</span><span class="p">):</span>
            <span class="n">tree</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_estimator</span><span class="p">(</span><span class="n">append</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">MAX_INT</span><span class="p">))</span>
            <span class="n">trees</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span>
        <span class="c1"># Parallel loop: we use the threading backend as the Cython code</span>
        <span class="c1"># for fitting the trees is internally releasing the Python GIL</span>
        <span class="c1"># making threading always more efficient than multiprocessing in</span>
        <span class="c1"># that case.</span>
        <span class="n">trees</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                         <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;threading&quot;</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">_parallel_build_trees</span><span class="p">)(</span>
                <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">trees</span><span class="p">),</span>
                <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trees</span><span class="p">))</span>
        <span class="c1"># Collect newly grown trees</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">trees</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_score</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_oob_score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># Decapsulate classes_ attributes</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;classes_&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.ExtraTreesRegressor.fit_transform">
    <p>def <span class="ident">fit_transform</span>(</p><p>self, X, y=None, **fit_params)</p>
    </div>
    

    
  
    <div class="desc"><p>Fit to data, then transform it.</p>
<p>Fits transformer to X and y with optional parameters fit_params
and returns a transformed version of X.</p>
<h2>Parameters</h2>
<p>X : numpy array of shape [n_samples, n_features]
    Training set.</p>
<p>y : numpy array of shape [n_samples]
    Target values.</p>
<h2>Returns</h2>
<p>X_new : numpy array of shape [n_samples, n_features_new]
    Transformed array.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.ExtraTreesRegressor.fit_transform', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.ExtraTreesRegressor.fit_transform" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fit to data, then transform it.</span>
<span class="sd">    Fits transformer to X and y with optional parameters fit_params</span>
<span class="sd">    and returns a transformed version of X.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : numpy array of shape [n_samples, n_features]</span>
<span class="sd">        Training set.</span>
<span class="sd">    y : numpy array of shape [n_samples]</span>
<span class="sd">        Target values.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X_new : numpy array of shape [n_samples, n_features_new]</span>
<span class="sd">        Transformed array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># non-optimized default implementation; override when a better</span>
    <span class="c1"># method is possible for a given clustering algorithm</span>
    <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># fit method of arity 1 (unsupervised transformation)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># fit method of arity 2 (supervised transformation)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.ExtraTreesRegressor.get_params">
    <p>def <span class="ident">get_params</span>(</p><p>self, deep=True)</p>
    </div>
    

    
  
    <div class="desc"><p>Get parameters for this estimator.</p>
<h2>Parameters</h2>
<p>deep: boolean, optional
    If True, will return the parameters for this estimator and
    contained subobjects that are estimators.</p>
<h2>Returns</h2>
<p>params : mapping of string to any
    Parameter names mapped to their values.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.ExtraTreesRegressor.get_params', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.ExtraTreesRegressor.get_params" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deep</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get parameters for this estimator.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    deep: boolean, optional</span>
<span class="sd">        If True, will return the parameters for this estimator and</span>
<span class="sd">        contained subobjects that are estimators.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    params : mapping of string to any</span>
<span class="sd">        Parameter names mapped to their values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_param_names</span><span class="p">():</span>
        <span class="c1"># We need deprecation warnings to always be on in order to</span>
        <span class="c1"># catch deprecated param values.</span>
        <span class="c1"># This is set in utils/__init__.py but it gets overwritten</span>
        <span class="c1"># when running under python3 somehow.</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;always&quot;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">(</span><span class="n">record</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">w</span><span class="p">:</span>
                <span class="n">value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="ow">and</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">category</span> <span class="o">==</span> <span class="ne">DeprecationWarning</span><span class="p">:</span>
                <span class="c1"># if the parameter is deprecated, don&#39;t show it</span>
                <span class="k">continue</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">filters</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># XXX: should we rather test if instance of estimator?</span>
        <span class="k">if</span> <span class="n">deep</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s1">&#39;get_params&#39;</span><span class="p">):</span>
            <span class="n">deep_items</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="n">out</span><span class="o">.</span><span class="n">update</span><span class="p">((</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;__&#39;</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">deep_items</span><span class="p">)</span>
        <span class="n">out</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.ExtraTreesRegressor.predict">
    <p>def <span class="ident">predict</span>(</p><p>self, X, return_std=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Predict continuous output for X.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>X</code> [array-like, shape=(n_samples, n_features)]:
    Input data.</p>
</li>
<li>
<p><code>return_std</code> [bool, default False]:
    Whether or not to return the standard deviation.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li>
<p><code>predictions</code> [array-like, shape=(n_samples,)]:
    Predicted values for X. If criterion is set to "mse",
    then <code>predictions[i] ~= mean(y | X[i])</code>.</p>
</li>
<li>
<p><code>std</code> [array-like, shape=(n_samples,)]:
    Standard deviation of <code>y</code> at <code>X</code>. If criterion
    is set to "mse", then <code>std[i] ~= std(y | X[i])</code>.</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.ExtraTreesRegressor.predict', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.ExtraTreesRegressor.predict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Predict continuous output for X.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X` [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">        Input data.</span>
<span class="sd">    * `return_std` [bool, default False]:</span>
<span class="sd">        Whether or not to return the standard deviation.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `predictions` [array-like, shape=(n_samples,)]:</span>
<span class="sd">        Predicted values for X. If criterion is set to &quot;mse&quot;,</span>
<span class="sd">        then `predictions[i] ~= mean(y | X[i])`.</span>
<span class="sd">    * `std` [array-like, shape=(n_samples,)]:</span>
<span class="sd">        Standard deviation of `y` at `X`. If criterion</span>
<span class="sd">        is set to &quot;mse&quot;, then `std[i] ~= std(y | X[i])`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">ExtraTreesRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_std</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">!=</span> <span class="s2">&quot;mse&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected impurity to be &#39;mse&#39;, got </span><span class="si">%s</span><span class="s2"> instead&quot;</span>
                <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">)</span>
        <span class="c1"># This derives std(y | x) as described in 4.3.2 of arXiv:1211.0906</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
            <span class="n">var_tree</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">impurity</span><span class="p">[</span><span class="n">tree</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">)]</span>
            <span class="n">mean_tree</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">std</span> <span class="o">+=</span> <span class="n">var_tree</span> <span class="o">+</span> <span class="n">mean_tree</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">std</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">-=</span> <span class="n">mean</span> <span class="o">**</span> <span class="mf">2.0</span>
        <span class="n">std</span><span class="p">[</span><span class="n">std</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">std</span> <span class="o">**</span> <span class="mf">0.5</span>
        <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span>
    <span class="k">return</span> <span class="n">mean</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.ExtraTreesRegressor.score">
    <p>def <span class="ident">score</span>(</p><p>self, X, y, sample_weight=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the coefficient of determination R^2 of the prediction.</p>
<p>The coefficient R^2 is defined as (1 - u/v), where u is the regression
sum of squares ((y_true - y_pred) <strong> 2).sum() and v is the residual
sum of squares ((y_true - y_true.mean()) </strong> 2).sum().
Best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features,
would get a R^2 score of 0.0.</p>
<h2>Parameters</h2>
<p>X : array-like, shape = (n_samples, n_features)
    Test samples.</p>
<p>y : array-like, shape = (n_samples) or (n_samples, n_outputs)
    True values for X.</p>
<p>sample_weight : array-like, shape = [n_samples], optional
    Sample weights.</p>
<h2>Returns</h2>
<p>score : float
    R^2 of self.predict(X) wrt. y.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.ExtraTreesRegressor.score', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.ExtraTreesRegressor.score" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the coefficient of determination R^2 of the prediction.</span>
<span class="sd">    The coefficient R^2 is defined as (1 - u/v), where u is the regression</span>
<span class="sd">    sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual</span>
<span class="sd">    sum of squares ((y_true - y_true.mean()) ** 2).sum().</span>
<span class="sd">    Best possible score is 1.0 and it can be negative (because the</span>
<span class="sd">    model can be arbitrarily worse). A constant model that always</span>
<span class="sd">    predicts the expected value of y, disregarding the input features,</span>
<span class="sd">    would get a R^2 score of 0.0.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like, shape = (n_samples, n_features)</span>
<span class="sd">        Test samples.</span>
<span class="sd">    y : array-like, shape = (n_samples) or (n_samples, n_outputs)</span>
<span class="sd">        True values for X.</span>
<span class="sd">    sample_weight : array-like, shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    score : float</span>
<span class="sd">        R^2 of self.predict(X) wrt. y.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
    <span class="k">return</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                    <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;variance_weighted&#39;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.ExtraTreesRegressor.set_params">
    <p>def <span class="ident">set_params</span>(</p><p>self, **params)</p>
    </div>
    

    
  
    <div class="desc"><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<h2>Returns</h2>
<p>self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.ExtraTreesRegressor.set_params', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.ExtraTreesRegressor.set_params" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set the parameters of this estimator.</span>
<span class="sd">    The method works on simple estimators as well as on nested objects</span>
<span class="sd">    (such as pipelines). The latter have parameters of the form</span>
<span class="sd">    ``&lt;component&gt;__&lt;parameter&gt;`` so that it&#39;s possible to update each</span>
<span class="sd">    component of a nested object.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">params</span><span class="p">:</span>
        <span class="c1"># Simple optimisation to gain speed (inspect is slow)</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="n">valid_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">(</span><span class="n">deep</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="n">split</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;__&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">split</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># nested objects case</span>
            <span class="n">name</span><span class="p">,</span> <span class="n">sub_name</span> <span class="o">=</span> <span class="n">split</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_params</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid parameter </span><span class="si">%s</span><span class="s1"> for estimator </span><span class="si">%s</span><span class="s1">. &#39;</span>
                                 <span class="s1">&#39;Check the list of available parameters &#39;</span>
                                 <span class="s1">&#39;with `estimator.get_params().keys()`.&#39;</span> <span class="o">%</span>
                                 <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">))</span>
            <span class="n">sub_object</span> <span class="o">=</span> <span class="n">valid_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
            <span class="n">sub_object</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">sub_name</span><span class="p">:</span> <span class="n">value</span><span class="p">})</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># simple objects case</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_params</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid parameter </span><span class="si">%s</span><span class="s1"> for estimator </span><span class="si">%s</span><span class="s1">. &#39;</span>
                                 <span class="s1">&#39;Check the list of available parameters &#39;</span>
                                 <span class="s1">&#39;with `estimator.get_params().keys()`.&#39;</span> <span class="o">%</span>
                                 <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span><span class="p">))</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.ExtraTreesRegressor.transform">
    <p>def <span class="ident">transform</span>(</p><p>*args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.</p>
<p>Reduce X to its most important features.</p>
<div class="codehilite"><pre><span></span>    Uses ``coef_`` or ``feature_importances_`` to determine the most
    important features.  For models with a ``coef_`` for each class, the
    absolute sum over the classes is used.

    Parameters
    ----------
    X : array or scipy sparse matrix of shape [n_samples, n_features]
        The input samples.

    threshold : string, float or None, optional (default=None)
        The threshold value to use for feature selection. Features whose
        importance is greater or equal are kept while the others are
        discarded. If &quot;median&quot; (resp. &quot;mean&quot;), then the threshold value is
        the median (resp. the mean) of the feature importances. A scaling
        factor (e.g., &quot;1.25*mean&quot;) may also be used. If None and if
        available, the object attribute ``threshold`` is used. Otherwise,
        &quot;mean&quot; is used by default.

    Returns
    -------
    X_r : array of shape [n_samples, n_selected_features]
        The input samples with only the selected features.
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.ExtraTreesRegressor.transform', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.ExtraTreesRegressor.transform" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">DeprecationWarning</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fun</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
          <h3>Instance variables</h3>
            <div class="item">
            <p id="skopt.learning.ExtraTreesRegressor.feature_importances_" class="name">var <span class="ident">feature_importances_</span></p>
            

            
  
    <div class="desc"><p>Return the feature importances (the higher, the more important the
   feature).</p>
<h2>Returns</h2>
<p>feature_importances_ : array, shape = [n_features]</p></div>
  <div class="source_cont">
</div>

            </div>
      </div>
      </div>
      
      <div class="item">
      <p id="skopt.learning.GradientBoostingQuantileRegressor" class="name">class <span class="ident">GradientBoostingQuantileRegressor</span></p>
      
  
    <div class="desc"><p>Predict several quantiles with one estimator.</p>
<p>This is a wrapper around <code>GradientBoostingRegressor</code>'s quantile
regression that allows you to predict several <code>quantiles</code> in
one go.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>quantiles</code> [array-like]:
    Quantiles to predict. By default the 16, 50 and 84%
    quantiles are predicted.</p>
</li>
<li>
<p><code>base_estimator</code> [GradientBoostingRegressor instance or None (default)]:
    Quantile regressor used to make predictions. Only instances
    of <code>GradientBoostingRegressor</code> are supported. Use this to change
    the hyper-parameters of the estimator.</p>
</li>
<li>
<p><code>random-state</code> [int, RandomState instance, or None (default)]:
    Set random state to something other than None for reproducible
    results.</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.GradientBoostingQuantileRegressor', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.GradientBoostingQuantileRegressor" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">GradientBoostingQuantileRegressor</span><span class="p">(</span><span class="n">BaseEstimator</span><span class="p">,</span> <span class="n">RegressorMixin</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict several quantiles with one estimator.</span>

<span class="sd">    This is a wrapper around `GradientBoostingRegressor`&#39;s quantile</span>
<span class="sd">    regression that allows you to predict several `quantiles` in</span>
<span class="sd">    one go.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `quantiles` [array-like]:</span>
<span class="sd">        Quantiles to predict. By default the 16, 50 and 84%</span>
<span class="sd">        quantiles are predicted.</span>

<span class="sd">    * `base_estimator` [GradientBoostingRegressor instance or None (default)]:</span>
<span class="sd">        Quantile regressor used to make predictions. Only instances</span>
<span class="sd">        of `GradientBoostingRegressor` are supported. Use this to change</span>
<span class="sd">        the hyper-parameters of the estimator.</span>

<span class="sd">    * `random-state` [int, RandomState instance, or None (default)]:</span>
<span class="sd">        Set random state to something other than None for reproducible</span>
<span class="sd">        results.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">quantiles</span><span class="o">=</span><span class="p">[</span><span class="mf">0.16</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.84</span><span class="p">],</span> <span class="n">base_estimator</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                 <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span> <span class="o">=</span> <span class="n">quantiles</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span> <span class="o">=</span> <span class="n">base_estimator</span>

    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Fit one regressor for each quantile.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `X` [array-like, shape=(n_samples, n_features):</span>
<span class="sd">            Training vectors, where `n_samples` is the number of samples</span>
<span class="sd">            and `n_features` is the number of features.</span>

<span class="sd">        * `y` [array-like, shape=(n_samples,)]:</span>
<span class="sd">            Target values (real numbers in regression)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;quantile&#39;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">base_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">,</span> <span class="n">GradientBoostingRegressor</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;base_estimator has to be of type&#39;</span>
                                 <span class="s1">&#39; GradientBoostingRegressor.&#39;</span><span class="p">)</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="n">base_estimator</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;quantile&#39;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;base_estimator has to use quantile&#39;</span>
                                 <span class="s1">&#39; loss not </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">base_estimator</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="p">:</span>
            <span class="n">regressor</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">)</span>
            <span class="n">regressor</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">q</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
            <span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

            <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">regressor</span><span class="p">)</span>

        <span class="k">return</span> <span class="bp">self</span>

    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Predict.</span>

<span class="sd">        Predict `X` at every quantile if `return_std` is set to False.</span>
<span class="sd">        If `return_std` is set to True, then return the mean</span>
<span class="sd">        and the predicted standard deviation, which is approximated as</span>
<span class="sd">        the (0.84th quantile - 0.16th quantile) divided by 2.0</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `X` [array-like, shape=(n_samples, n_features):</span>
<span class="sd">            where `n_samples` is the number of samples</span>
<span class="sd">            and `n_features` is the number of features.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">predicted_quantiles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
            <span class="p">[</span><span class="n">rgr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">rgr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">])</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">return_std</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">predicted_quantiles</span><span class="o">.</span><span class="n">T</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">std_quantiles</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.16</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.84</span><span class="p">]</span>
            <span class="n">is_present_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">in1d</span><span class="p">(</span><span class="n">std_quantiles</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="p">)</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">is_present_mask</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;return_std works only if the quantiles during &quot;</span>
                    <span class="s2">&quot;instantiation include 0.16, 0.5 and 0.84&quot;</span><span class="p">)</span>
            <span class="n">low</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mf">0.16</span><span class="p">)]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">high</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mf">0.84</span><span class="p">)]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="p">((</span><span class="n">high</span> <span class="o">-</span> <span class="n">low</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">)</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#skopt.learning.GradientBoostingQuantileRegressor">GradientBoostingQuantileRegressor</a></li>
          <li>sklearn.base.BaseEstimator</li>
          <li>sklearn.base.RegressorMixin</li>
          <li>builtins.object</li>
          </ul>
          <h3>Static methods</h3>
            
  <div class="item">
    <div class="name def" id="skopt.learning.GradientBoostingQuantileRegressor.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, quantiles=[0.16, 0.5, 0.84], base_estimator=None, random_state=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Initialize self.  See help(type(self)) for accurate signature.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.GradientBoostingQuantileRegressor.__init__', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.GradientBoostingQuantileRegressor.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">quantiles</span><span class="o">=</span><span class="p">[</span><span class="mf">0.16</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.84</span><span class="p">],</span> <span class="n">base_estimator</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
             <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span> <span class="o">=</span> <span class="n">quantiles</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">random_state</span> <span class="o">=</span> <span class="n">random_state</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span> <span class="o">=</span> <span class="n">base_estimator</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.GradientBoostingQuantileRegressor.fit">
    <p>def <span class="ident">fit</span>(</p><p>self, X, y)</p>
    </div>
    

    
  
    <div class="desc"><p>Fit one regressor for each quantile.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>X</code> [array-like, shape=(n_samples, n_features):
    Training vectors, where <code>n_samples</code> is the number of samples
    and <code>n_features</code> is the number of features.</p>
</li>
<li>
<p><code>y</code> [array-like, shape=(n_samples,)]:
    Target values (real numbers in regression)</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.GradientBoostingQuantileRegressor.fit', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.GradientBoostingQuantileRegressor.fit" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fit one regressor for each quantile.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X` [array-like, shape=(n_samples, n_features):</span>
<span class="sd">        Training vectors, where `n_samples` is the number of samples</span>
<span class="sd">        and `n_features` is the number of features.</span>
<span class="sd">    * `y` [array-like, shape=(n_samples,)]:</span>
<span class="sd">        Target values (real numbers in regression)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">loss</span><span class="o">=</span><span class="s1">&#39;quantile&#39;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">base_estimator</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">base_estimator</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">,</span> <span class="n">GradientBoostingRegressor</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;base_estimator has to be of type&#39;</span>
                             <span class="s1">&#39; GradientBoostingRegressor.&#39;</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">base_estimator</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;quantile&#39;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;base_estimator has to use quantile&#39;</span>
                             <span class="s1">&#39; loss not </span><span class="si">%s</span><span class="s1">&#39;</span> <span class="o">%</span> <span class="n">base_estimator</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">q</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="p">:</span>
        <span class="n">regressor</span> <span class="o">=</span> <span class="n">clone</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">)</span>
        <span class="n">regressor</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">q</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
        <span class="n">regressor</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">regressor</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.GradientBoostingQuantileRegressor.get_params">
    <p>def <span class="ident">get_params</span>(</p><p>self, deep=True)</p>
    </div>
    

    
  
    <div class="desc"><p>Get parameters for this estimator.</p>
<h2>Parameters</h2>
<p>deep: boolean, optional
    If True, will return the parameters for this estimator and
    contained subobjects that are estimators.</p>
<h2>Returns</h2>
<p>params : mapping of string to any
    Parameter names mapped to their values.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.GradientBoostingQuantileRegressor.get_params', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.GradientBoostingQuantileRegressor.get_params" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deep</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get parameters for this estimator.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    deep: boolean, optional</span>
<span class="sd">        If True, will return the parameters for this estimator and</span>
<span class="sd">        contained subobjects that are estimators.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    params : mapping of string to any</span>
<span class="sd">        Parameter names mapped to their values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_param_names</span><span class="p">():</span>
        <span class="c1"># We need deprecation warnings to always be on in order to</span>
        <span class="c1"># catch deprecated param values.</span>
        <span class="c1"># This is set in utils/__init__.py but it gets overwritten</span>
        <span class="c1"># when running under python3 somehow.</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;always&quot;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">(</span><span class="n">record</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">w</span><span class="p">:</span>
                <span class="n">value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="ow">and</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">category</span> <span class="o">==</span> <span class="ne">DeprecationWarning</span><span class="p">:</span>
                <span class="c1"># if the parameter is deprecated, don&#39;t show it</span>
                <span class="k">continue</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">filters</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># XXX: should we rather test if instance of estimator?</span>
        <span class="k">if</span> <span class="n">deep</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s1">&#39;get_params&#39;</span><span class="p">):</span>
            <span class="n">deep_items</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="n">out</span><span class="o">.</span><span class="n">update</span><span class="p">((</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;__&#39;</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">deep_items</span><span class="p">)</span>
        <span class="n">out</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.GradientBoostingQuantileRegressor.predict">
    <p>def <span class="ident">predict</span>(</p><p>self, X, return_std=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Predict.</p>
<p>Predict <code>X</code> at every quantile if <code>return_std</code> is set to False.
If <code>return_std</code> is set to True, then return the mean
and the predicted standard deviation, which is approximated as
the (0.84th quantile - 0.16th quantile) divided by 2.0</p>
<h2>Parameters</h2>
<ul>
<li><code>X</code> [array-like, shape=(n_samples, n_features):
    where <code>n_samples</code> is the number of samples
    and <code>n_features</code> is the number of features.</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.GradientBoostingQuantileRegressor.predict', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.GradientBoostingQuantileRegressor.predict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Predict.</span>
<span class="sd">    Predict `X` at every quantile if `return_std` is set to False.</span>
<span class="sd">    If `return_std` is set to True, then return the mean</span>
<span class="sd">    and the predicted standard deviation, which is approximated as</span>
<span class="sd">    the (0.84th quantile - 0.16th quantile) divided by 2.0</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X` [array-like, shape=(n_samples, n_features):</span>
<span class="sd">        where `n_samples` is the number of samples</span>
<span class="sd">        and `n_features` is the number of features.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">predicted_quantiles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span>
        <span class="p">[</span><span class="n">rgr</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">for</span> <span class="n">rgr</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">])</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">return_std</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">predicted_quantiles</span><span class="o">.</span><span class="n">T</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">std_quantiles</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.16</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.84</span><span class="p">]</span>
        <span class="n">is_present_mask</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">in1d</span><span class="p">(</span><span class="n">std_quantiles</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="p">)</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">all</span><span class="p">(</span><span class="n">is_present_mask</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;return_std works only if the quantiles during &quot;</span>
                <span class="s2">&quot;instantiation include 0.16, 0.5 and 0.84&quot;</span><span class="p">)</span>
        <span class="n">low</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mf">0.16</span><span class="p">)]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">high</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mf">0.84</span><span class="p">)]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">regressors_</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">quantiles</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)]</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="p">((</span><span class="n">high</span> <span class="o">-</span> <span class="n">low</span><span class="p">)</span> <span class="o">/</span> <span class="mf">2.0</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.GradientBoostingQuantileRegressor.score">
    <p>def <span class="ident">score</span>(</p><p>self, X, y, sample_weight=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the coefficient of determination R^2 of the prediction.</p>
<p>The coefficient R^2 is defined as (1 - u/v), where u is the regression
sum of squares ((y_true - y_pred) <strong> 2).sum() and v is the residual
sum of squares ((y_true - y_true.mean()) </strong> 2).sum().
Best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features,
would get a R^2 score of 0.0.</p>
<h2>Parameters</h2>
<p>X : array-like, shape = (n_samples, n_features)
    Test samples.</p>
<p>y : array-like, shape = (n_samples) or (n_samples, n_outputs)
    True values for X.</p>
<p>sample_weight : array-like, shape = [n_samples], optional
    Sample weights.</p>
<h2>Returns</h2>
<p>score : float
    R^2 of self.predict(X) wrt. y.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.GradientBoostingQuantileRegressor.score', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.GradientBoostingQuantileRegressor.score" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the coefficient of determination R^2 of the prediction.</span>
<span class="sd">    The coefficient R^2 is defined as (1 - u/v), where u is the regression</span>
<span class="sd">    sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual</span>
<span class="sd">    sum of squares ((y_true - y_true.mean()) ** 2).sum().</span>
<span class="sd">    Best possible score is 1.0 and it can be negative (because the</span>
<span class="sd">    model can be arbitrarily worse). A constant model that always</span>
<span class="sd">    predicts the expected value of y, disregarding the input features,</span>
<span class="sd">    would get a R^2 score of 0.0.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like, shape = (n_samples, n_features)</span>
<span class="sd">        Test samples.</span>
<span class="sd">    y : array-like, shape = (n_samples) or (n_samples, n_outputs)</span>
<span class="sd">        True values for X.</span>
<span class="sd">    sample_weight : array-like, shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    score : float</span>
<span class="sd">        R^2 of self.predict(X) wrt. y.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
    <span class="k">return</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                    <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;variance_weighted&#39;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.GradientBoostingQuantileRegressor.set_params">
    <p>def <span class="ident">set_params</span>(</p><p>self, **params)</p>
    </div>
    

    
  
    <div class="desc"><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<h2>Returns</h2>
<p>self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.GradientBoostingQuantileRegressor.set_params', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.GradientBoostingQuantileRegressor.set_params" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set the parameters of this estimator.</span>
<span class="sd">    The method works on simple estimators as well as on nested objects</span>
<span class="sd">    (such as pipelines). The latter have parameters of the form</span>
<span class="sd">    ``&lt;component&gt;__&lt;parameter&gt;`` so that it&#39;s possible to update each</span>
<span class="sd">    component of a nested object.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">params</span><span class="p">:</span>
        <span class="c1"># Simple optimisation to gain speed (inspect is slow)</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="n">valid_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">(</span><span class="n">deep</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="n">split</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;__&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">split</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># nested objects case</span>
            <span class="n">name</span><span class="p">,</span> <span class="n">sub_name</span> <span class="o">=</span> <span class="n">split</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_params</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid parameter </span><span class="si">%s</span><span class="s1"> for estimator </span><span class="si">%s</span><span class="s1">. &#39;</span>
                                 <span class="s1">&#39;Check the list of available parameters &#39;</span>
                                 <span class="s1">&#39;with `estimator.get_params().keys()`.&#39;</span> <span class="o">%</span>
                                 <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">))</span>
            <span class="n">sub_object</span> <span class="o">=</span> <span class="n">valid_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
            <span class="n">sub_object</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">sub_name</span><span class="p">:</span> <span class="n">value</span><span class="p">})</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># simple objects case</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_params</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid parameter </span><span class="si">%s</span><span class="s1"> for estimator </span><span class="si">%s</span><span class="s1">. &#39;</span>
                                 <span class="s1">&#39;Check the list of available parameters &#39;</span>
                                 <span class="s1">&#39;with `estimator.get_params().keys()`.&#39;</span> <span class="o">%</span>
                                 <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span><span class="p">))</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
          <h3>Instance variables</h3>
            <div class="item">
            <p id="skopt.learning.GradientBoostingQuantileRegressor.base_estimator" class="name">var <span class="ident">base_estimator</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.learning.GradientBoostingQuantileRegressor.quantiles" class="name">var <span class="ident">quantiles</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
            <div class="item">
            <p id="skopt.learning.GradientBoostingQuantileRegressor.random_state" class="name">var <span class="ident">random_state</span></p>
            

            
  
  <div class="source_cont">
</div>

            </div>
      </div>
      </div>
      
      <div class="item">
      <p id="skopt.learning.RandomForestRegressor" class="name">class <span class="ident">RandomForestRegressor</span></p>
      
  
    <div class="desc"><p>RandomForestRegressor that supports <code>return_std</code>.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.RandomForestRegressor', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.RandomForestRegressor" class="source">
    <div class="codehilite"><pre><span></span><span class="k">class</span> <span class="nc">RandomForestRegressor</span><span class="p">(</span><span class="n">sk_RandomForestRegressor</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    RandomForestRegressor that supports `return_std`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Predict continuous output for X.</span>

<span class="sd">        Parameters</span>
<span class="sd">        ----------</span>
<span class="sd">        * `X` [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">            Input data.</span>

<span class="sd">        * `return_std` [bool, default False]:</span>
<span class="sd">            Whether or not to return the standard deviation.</span>

<span class="sd">        Returns</span>
<span class="sd">        -------</span>
<span class="sd">        * `predictions` [array-like, shape=(n_samples,)]:</span>
<span class="sd">            Predicted values for X. If criterion is set to &quot;mse&quot;,</span>
<span class="sd">            then `predictions[i] ~= mean(y | X[i])`.</span>

<span class="sd">        * `std` [array-like, shape=(n_samples,)]:</span>
<span class="sd">            Standard deviation of `y` at `X`. If criterion</span>
<span class="sd">            is set to &quot;mse&quot;, then `std[i] ~= std(y | X[i])`.</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mean</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">return_std</span><span class="p">:</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">!=</span> <span class="s2">&quot;mse&quot;</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                    <span class="s2">&quot;Expected impurity to be &#39;mse&#39;, got </span><span class="si">%s</span><span class="s2"> instead&quot;</span>
                    <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">)</span>

            <span class="c1"># This derives std(y | x) as described in 4.3.2 of arXiv:1211.0906</span>
            <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>

            <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
                <span class="n">var_tree</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">impurity</span><span class="p">[</span><span class="n">tree</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">)]</span>
                <span class="n">mean_tree</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
                <span class="n">std</span> <span class="o">+=</span> <span class="n">var_tree</span> <span class="o">+</span> <span class="n">mean_tree</span> <span class="o">**</span> <span class="mi">2</span>

            <span class="n">std</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
            <span class="n">std</span> <span class="o">-=</span> <span class="n">mean</span> <span class="o">**</span> <span class="mf">2.0</span>
            <span class="n">std</span><span class="p">[</span><span class="n">std</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
            <span class="n">std</span> <span class="o">=</span> <span class="n">std</span> <span class="o">**</span> <span class="mf">0.5</span>

            <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span>

        <span class="k">return</span> <span class="n">mean</span>
</pre></div>

  </div>
</div>


      <div class="class">
          <h3>Ancestors (in MRO)</h3>
          <ul class="class_list">
          <li><a href="#skopt.learning.RandomForestRegressor">RandomForestRegressor</a></li>
          <li>sklearn.ensemble.forest.RandomForestRegressor</li>
          <li>sklearn.ensemble.forest.ForestRegressor</li>
          <li>abc.NewBase</li>
          <li>sklearn.ensemble.forest.BaseForest</li>
          <li>abc.NewBase</li>
          <li>sklearn.ensemble.base.BaseEnsemble</li>
          <li>sklearn.base.BaseEstimator</li>
          <li>sklearn.base.MetaEstimatorMixin</li>
          <li>sklearn.feature_selection.from_model._LearntSelectorMixin</li>
          <li>sklearn.base.TransformerMixin</li>
          <li>sklearn.base.RegressorMixin</li>
          <li>builtins.object</li>
          </ul>
          <h3>Static methods</h3>
            
  <div class="item">
    <div class="name def" id="skopt.learning.RandomForestRegressor.__init__">
    <p>def <span class="ident">__init__</span>(</p><p>self, n_estimators=10, criterion=&#39;mse&#39;, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=&#39;auto&#39;, max_leaf_nodes=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Initialize self.  See help(type(self)) for accurate signature.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.RandomForestRegressor.__init__', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.RandomForestRegressor.__init__" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>
             <span class="n">n_estimators</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
             <span class="n">criterion</span><span class="o">=</span><span class="s2">&quot;mse&quot;</span><span class="p">,</span>
             <span class="n">max_depth</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
             <span class="n">min_samples_split</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
             <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">min_weight_fraction_leaf</span><span class="o">=</span><span class="mf">0.</span><span class="p">,</span>
             <span class="n">max_features</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
             <span class="n">max_leaf_nodes</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
             <span class="n">bootstrap</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span>
             <span class="n">oob_score</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
             <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
             <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
             <span class="n">verbose</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
             <span class="n">warm_start</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">__init__</span><span class="p">(</span>
        <span class="n">base_estimator</span><span class="o">=</span><span class="n">DecisionTreeRegressor</span><span class="p">(),</span>
        <span class="n">n_estimators</span><span class="o">=</span><span class="n">n_estimators</span><span class="p">,</span>
        <span class="n">estimator_params</span><span class="o">=</span><span class="p">(</span><span class="s2">&quot;criterion&quot;</span><span class="p">,</span> <span class="s2">&quot;max_depth&quot;</span><span class="p">,</span> <span class="s2">&quot;min_samples_split&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;min_samples_leaf&quot;</span><span class="p">,</span> <span class="s2">&quot;min_weight_fraction_leaf&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;max_features&quot;</span><span class="p">,</span> <span class="s2">&quot;max_leaf_nodes&quot;</span><span class="p">,</span>
                          <span class="s2">&quot;random_state&quot;</span><span class="p">),</span>
        <span class="n">bootstrap</span><span class="o">=</span><span class="n">bootstrap</span><span class="p">,</span>
        <span class="n">oob_score</span><span class="o">=</span><span class="n">oob_score</span><span class="p">,</span>
        <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
        <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
        <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
        <span class="n">warm_start</span><span class="o">=</span><span class="n">warm_start</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_depth</span> <span class="o">=</span> <span class="n">max_depth</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_split</span> <span class="o">=</span> <span class="n">min_samples_split</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">min_samples_leaf</span> <span class="o">=</span> <span class="n">min_samples_leaf</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">min_weight_fraction_leaf</span> <span class="o">=</span> <span class="n">min_weight_fraction_leaf</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_features</span> <span class="o">=</span> <span class="n">max_features</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_leaf_nodes</span> <span class="o">=</span> <span class="n">max_leaf_nodes</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.RandomForestRegressor.apply">
    <p>def <span class="ident">apply</span>(</p><p>self, X)</p>
    </div>
    

    
  
    <div class="desc"><p>Apply trees in the forest to X, return leaf indices.</p>
<h2>Parameters</h2>
<p>X : array-like or sparse matrix, shape = [n_samples, n_features]
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<h2>Returns</h2>
<p>X_leaves : array_like, shape = [n_samples, n_estimators]
    For each datapoint x in X and for each tree in the forest,
    return the index of the leaf x ends up in.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.RandomForestRegressor.apply', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.RandomForestRegressor.apply" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">apply</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply trees in the forest to X, return leaf indices.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X_leaves : array_like, shape = [n_samples, n_estimators]</span>
<span class="sd">        For each datapoint x in X and for each tree in the forest,</span>
<span class="sd">        return the index of the leaf x ends up in.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">results</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                       <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;threading&quot;</span><span class="p">)(</span>
        <span class="n">delayed</span><span class="p">(</span><span class="n">parallel_helper</span><span class="p">)(</span><span class="n">tree</span><span class="p">,</span> <span class="s1">&#39;apply&#39;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">check_input</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">results</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.RandomForestRegressor.decision_path">
    <p>def <span class="ident">decision_path</span>(</p><p>self, X)</p>
    </div>
    

    
  
    <div class="desc"><p>Return the decision path in the forest</p>
<h2>Parameters</h2>
<p>X : array-like or sparse matrix, shape = [n_samples, n_features]
    The input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csr_matrix</code>.</p>
<h2>Returns</h2>
<p>indicator : sparse csr array, shape = [n_samples, n_nodes]
    Return a node indicator matrix where non zero elements
    indicates that the samples goes through the nodes.</p>
<p>n_nodes_ptr : array of size (n_estimators + 1, )
    The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]
    gives the indicator value for the i-th estimator.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.RandomForestRegressor.decision_path', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.RandomForestRegressor.decision_path" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">decision_path</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return the decision path in the forest</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or sparse matrix, shape = [n_samples, n_features]</span>
<span class="sd">        The input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csr_matrix``.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    indicator : sparse csr array, shape = [n_samples, n_nodes]</span>
<span class="sd">        Return a node indicator matrix where non zero elements</span>
<span class="sd">        indicates that the samples goes through the nodes.</span>
<span class="sd">    n_nodes_ptr : array of size (n_estimators + 1, )</span>
<span class="sd">        The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]</span>
<span class="sd">        gives the indicator value for the i-th estimator.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">X</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_X_predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="n">indicators</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                          <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;threading&quot;</span><span class="p">)(</span>
        <span class="n">delayed</span><span class="p">(</span><span class="n">parallel_helper</span><span class="p">)(</span><span class="n">tree</span><span class="p">,</span> <span class="s1">&#39;decision_path&#39;</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span>
                                  <span class="n">check_input</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
    <span class="n">n_nodes</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">n_nodes</span><span class="o">.</span><span class="n">extend</span><span class="p">([</span><span class="n">i</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">indicators</span><span class="p">])</span>
    <span class="n">n_nodes_ptr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">n_nodes</span><span class="p">)</span><span class="o">.</span><span class="n">cumsum</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">sparse_hstack</span><span class="p">(</span><span class="n">indicators</span><span class="p">)</span><span class="o">.</span><span class="n">tocsr</span><span class="p">(),</span> <span class="n">n_nodes_ptr</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.RandomForestRegressor.fit">
    <p>def <span class="ident">fit</span>(</p><p>self, X, y, sample_weight=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Build a forest of trees from the training set (X, y).</p>
<h2>Parameters</h2>
<p>X : array-like or sparse matrix of shape = [n_samples, n_features]
    The training input samples. Internally, it will be converted to
    <code>dtype=np.float32</code> and if a sparse matrix is provided
    to a sparse <code>csc_matrix</code>.</p>
<p>y : array-like, shape = [n_samples] or [n_samples, n_outputs]
    The target values (class labels in classification, real numbers in
    regression).</p>
<p>sample_weight : array-like, shape = [n_samples] or None
    Sample weights. If None, then samples are equally weighted. Splits
    that would create child nodes with net zero or negative weight are
    ignored while searching for a split in each node. In the case of
    classification, splits are also ignored if they would result in any
    single class carrying a negative weight in either child node.</p>
<h2>Returns</h2>
<p>self : object
    Returns self.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.RandomForestRegressor.fit', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.RandomForestRegressor.fit" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Build a forest of trees from the training set (X, y).</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like or sparse matrix of shape = [n_samples, n_features]</span>
<span class="sd">        The training input samples. Internally, it will be converted to</span>
<span class="sd">        ``dtype=np.float32`` and if a sparse matrix is provided</span>
<span class="sd">        to a sparse ``csc_matrix``.</span>
<span class="sd">    y : array-like, shape = [n_samples] or [n_samples, n_outputs]</span>
<span class="sd">        The target values (class labels in classification, real numbers in</span>
<span class="sd">        regression).</span>
<span class="sd">    sample_weight : array-like, shape = [n_samples] or None</span>
<span class="sd">        Sample weights. If None, then samples are equally weighted. Splits</span>
<span class="sd">        that would create child nodes with net zero or negative weight are</span>
<span class="sd">        ignored while searching for a split in each node. In the case of</span>
<span class="sd">        classification, splits are also ignored if they would result in any</span>
<span class="sd">        single class carrying a negative weight in either child node.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    self : object</span>
<span class="sd">        Returns self.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Validate or convert input data</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s2">&quot;csc&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DTYPE</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">check_array</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">accept_sparse</span><span class="o">=</span><span class="s1">&#39;csc&#39;</span><span class="p">,</span> <span class="n">ensure_2d</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">issparse</span><span class="p">(</span><span class="n">X</span><span class="p">):</span>
        <span class="c1"># Pre-sort indices to avoid that each individual tree of the</span>
        <span class="c1"># ensemble sorts the indices.</span>
        <span class="n">X</span><span class="o">.</span><span class="n">sort_indices</span><span class="p">()</span>
    <span class="c1"># Remap output</span>
    <span class="n">n_samples</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_features_</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_1d</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span> <span class="ow">and</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;A column-vector y was passed when a 1d array was&quot;</span>
             <span class="s2">&quot; expected. Please change the shape of y to &quot;</span>
             <span class="s2">&quot;(n_samples,), for example using ravel().&quot;</span><span class="p">,</span>
             <span class="n">DataConversionWarning</span><span class="p">,</span> <span class="n">stacklevel</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">y</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="c1"># reshape is necessary to preserve the data contiguity against vs</span>
        <span class="c1"># [:, np.newaxis] that does not.</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">=</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">y</span><span class="p">,</span> <span class="n">expanded_class_weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_validate_y_class_weight</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">if</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="s2">&quot;dtype&quot;</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span> <span class="o">!=</span> <span class="n">DOUBLE</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">y</span><span class="o">.</span><span class="n">flags</span><span class="o">.</span><span class="n">contiguous</span><span class="p">:</span>
        <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ascontiguousarray</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">DOUBLE</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">expanded_class_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">sample_weight</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">sample_weight</span> <span class="o">*</span> <span class="n">expanded_class_weight</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">sample_weight</span> <span class="o">=</span> <span class="n">expanded_class_weight</span>
    <span class="c1"># Check parameters</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_validate_estimator</span><span class="p">()</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">bootstrap</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_score</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Out of bag estimation only available&quot;</span>
                         <span class="s2">&quot; if bootstrap=True&quot;</span><span class="p">)</span>
    <span class="n">random_state</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">random_state</span><span class="p">)</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span><span class="p">:</span>
        <span class="c1"># Free allocated memory, if any</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">n_more_estimators</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">n_more_estimators</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;n_estimators=</span><span class="si">%d</span><span class="s1"> must be larger or equal to &#39;</span>
                         <span class="s1">&#39;len(estimators_)=</span><span class="si">%d</span><span class="s1"> when warm_start==True&#39;</span>
                         <span class="o">%</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_estimators</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)))</span>
    <span class="k">elif</span> <span class="n">n_more_estimators</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">warn</span><span class="p">(</span><span class="s2">&quot;Warm-start fitting without increasing n_estimators does not &quot;</span>
             <span class="s2">&quot;fit new trees.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">warm_start</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># We draw from the random state to get the random state we</span>
            <span class="c1"># would have got if we hadn&#39;t used a warm_start.</span>
            <span class="n">random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">MAX_INT</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">))</span>
        <span class="n">trees</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_more_estimators</span><span class="p">):</span>
            <span class="n">tree</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_estimator</span><span class="p">(</span><span class="n">append</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
            <span class="n">tree</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="n">MAX_INT</span><span class="p">))</span>
            <span class="n">trees</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">tree</span><span class="p">)</span>
        <span class="c1"># Parallel loop: we use the threading backend as the Cython code</span>
        <span class="c1"># for fitting the trees is internally releasing the Python GIL</span>
        <span class="c1"># making threading always more efficient than multiprocessing in</span>
        <span class="c1"># that case.</span>
        <span class="n">trees</span> <span class="o">=</span> <span class="n">Parallel</span><span class="p">(</span><span class="n">n_jobs</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">n_jobs</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span>
                         <span class="n">backend</span><span class="o">=</span><span class="s2">&quot;threading&quot;</span><span class="p">)(</span>
            <span class="n">delayed</span><span class="p">(</span><span class="n">_parallel_build_trees</span><span class="p">)(</span>
                <span class="n">t</span><span class="p">,</span> <span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">trees</span><span class="p">),</span>
                <span class="n">verbose</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">verbose</span><span class="p">,</span> <span class="n">class_weight</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">class_weight</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">trees</span><span class="p">))</span>
        <span class="c1"># Collect newly grown trees</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="o">.</span><span class="n">extend</span><span class="p">(</span><span class="n">trees</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">oob_score</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_set_oob_score</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c1"># Decapsulate classes_ attributes</span>
    <span class="k">if</span> <span class="nb">hasattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="s2">&quot;classes_&quot;</span><span class="p">)</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_outputs_</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">classes_</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.RandomForestRegressor.fit_transform">
    <p>def <span class="ident">fit_transform</span>(</p><p>self, X, y=None, **fit_params)</p>
    </div>
    

    
  
    <div class="desc"><p>Fit to data, then transform it.</p>
<p>Fits transformer to X and y with optional parameters fit_params
and returns a transformed version of X.</p>
<h2>Parameters</h2>
<p>X : numpy array of shape [n_samples, n_features]
    Training set.</p>
<p>y : numpy array of shape [n_samples]
    Target values.</p>
<h2>Returns</h2>
<p>X_new : numpy array of shape [n_samples, n_features_new]
    Transformed array.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.RandomForestRegressor.fit_transform', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.RandomForestRegressor.fit_transform" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">fit_transform</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fit to data, then transform it.</span>
<span class="sd">    Fits transformer to X and y with optional parameters fit_params</span>
<span class="sd">    and returns a transformed version of X.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : numpy array of shape [n_samples, n_features]</span>
<span class="sd">        Training set.</span>
<span class="sd">    y : numpy array of shape [n_samples]</span>
<span class="sd">        Target values.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    X_new : numpy array of shape [n_samples, n_features_new]</span>
<span class="sd">        Transformed array.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># non-optimized default implementation; override when a better</span>
    <span class="c1"># method is possible for a given clustering algorithm</span>
    <span class="k">if</span> <span class="n">y</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="c1"># fit method of arity 1 (unsupervised transformation)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># fit method of arity 2 (supervised transformation)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="o">**</span><span class="n">fit_params</span><span class="p">)</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.RandomForestRegressor.get_params">
    <p>def <span class="ident">get_params</span>(</p><p>self, deep=True)</p>
    </div>
    

    
  
    <div class="desc"><p>Get parameters for this estimator.</p>
<h2>Parameters</h2>
<p>deep: boolean, optional
    If True, will return the parameters for this estimator and
    contained subobjects that are estimators.</p>
<h2>Returns</h2>
<p>params : mapping of string to any
    Parameter names mapped to their values.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.RandomForestRegressor.get_params', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.RandomForestRegressor.get_params" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">get_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">deep</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get parameters for this estimator.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    deep: boolean, optional</span>
<span class="sd">        If True, will return the parameters for this estimator and</span>
<span class="sd">        contained subobjects that are estimators.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    params : mapping of string to any</span>
<span class="sd">        Parameter names mapped to their values.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">key</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_param_names</span><span class="p">():</span>
        <span class="c1"># We need deprecation warnings to always be on in order to</span>
        <span class="c1"># catch deprecated param values.</span>
        <span class="c1"># This is set in utils/__init__.py but it gets overwritten</span>
        <span class="c1"># when running under python3 somehow.</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s2">&quot;always&quot;</span><span class="p">,</span> <span class="ne">DeprecationWarning</span><span class="p">)</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">(</span><span class="n">record</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="k">as</span> <span class="n">w</span><span class="p">:</span>
                <span class="n">value</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="bp">None</span><span class="p">)</span>
            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">w</span><span class="p">)</span> <span class="ow">and</span> <span class="n">w</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">category</span> <span class="o">==</span> <span class="ne">DeprecationWarning</span><span class="p">:</span>
                <span class="c1"># if the parameter is deprecated, don&#39;t show it</span>
                <span class="k">continue</span>
        <span class="k">finally</span><span class="p">:</span>
            <span class="n">warnings</span><span class="o">.</span><span class="n">filters</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># XXX: should we rather test if instance of estimator?</span>
        <span class="k">if</span> <span class="n">deep</span> <span class="ow">and</span> <span class="nb">hasattr</span><span class="p">(</span><span class="n">value</span><span class="p">,</span> <span class="s1">&#39;get_params&#39;</span><span class="p">):</span>
            <span class="n">deep_items</span> <span class="o">=</span> <span class="n">value</span><span class="o">.</span><span class="n">get_params</span><span class="p">()</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
            <span class="n">out</span><span class="o">.</span><span class="n">update</span><span class="p">((</span><span class="n">key</span> <span class="o">+</span> <span class="s1">&#39;__&#39;</span> <span class="o">+</span> <span class="n">k</span><span class="p">,</span> <span class="n">val</span><span class="p">)</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">deep_items</span><span class="p">)</span>
        <span class="n">out</span><span class="p">[</span><span class="n">key</span><span class="p">]</span> <span class="o">=</span> <span class="n">value</span>
    <span class="k">return</span> <span class="n">out</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.RandomForestRegressor.predict">
    <p>def <span class="ident">predict</span>(</p><p>self, X, return_std=False)</p>
    </div>
    

    
  
    <div class="desc"><p>Predict continuous output for X.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>X</code> [array-like, shape=(n_samples, n_features)]:
    Input data.</p>
</li>
<li>
<p><code>return_std</code> [bool, default False]:
    Whether or not to return the standard deviation.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li>
<p><code>predictions</code> [array-like, shape=(n_samples,)]:
    Predicted values for X. If criterion is set to "mse",
    then <code>predictions[i] ~= mean(y | X[i])</code>.</p>
</li>
<li>
<p><code>std</code> [array-like, shape=(n_samples,)]:
    Standard deviation of <code>y</code> at <code>X</code>. If criterion
    is set to "mse", then <code>std[i] ~= std(y | X[i])</code>.</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.RandomForestRegressor.predict', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.RandomForestRegressor.predict" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">return_std</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Predict continuous output for X.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `X` [array-like, shape=(n_samples, n_features)]:</span>
<span class="sd">        Input data.</span>
<span class="sd">    * `return_std` [bool, default False]:</span>
<span class="sd">        Whether or not to return the standard deviation.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `predictions` [array-like, shape=(n_samples,)]:</span>
<span class="sd">        Predicted values for X. If criterion is set to &quot;mse&quot;,</span>
<span class="sd">        then `predictions[i] ~= mean(y | X[i])`.</span>
<span class="sd">    * `std` [array-like, shape=(n_samples,)]:</span>
<span class="sd">        Standard deviation of `y` at `X`. If criterion</span>
<span class="sd">        is set to &quot;mse&quot;, then `std[i] ~= std(y | X[i])`.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">RandomForestRegressor</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">return_std</span><span class="p">:</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">!=</span> <span class="s2">&quot;mse&quot;</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Expected impurity to be &#39;mse&#39;, got </span><span class="si">%s</span><span class="s2"> instead&quot;</span>
                <span class="o">%</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">)</span>
        <span class="c1"># This derives std(y | x) as described in 4.3.2 of arXiv:1211.0906</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">))</span>
        <span class="k">for</span> <span class="n">tree</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">:</span>
            <span class="n">var_tree</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">tree_</span><span class="o">.</span><span class="n">impurity</span><span class="p">[</span><span class="n">tree</span><span class="o">.</span><span class="n">apply</span><span class="p">(</span><span class="n">X</span><span class="p">)]</span>
            <span class="n">mean_tree</span> <span class="o">=</span> <span class="n">tree</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
            <span class="n">std</span> <span class="o">+=</span> <span class="n">var_tree</span> <span class="o">+</span> <span class="n">mean_tree</span> <span class="o">**</span> <span class="mi">2</span>
        <span class="n">std</span> <span class="o">/=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">estimators_</span><span class="p">)</span>
        <span class="n">std</span> <span class="o">-=</span> <span class="n">mean</span> <span class="o">**</span> <span class="mf">2.0</span>
        <span class="n">std</span><span class="p">[</span><span class="n">std</span> <span class="o">&lt;</span> <span class="mf">0.0</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
        <span class="n">std</span> <span class="o">=</span> <span class="n">std</span> <span class="o">**</span> <span class="mf">0.5</span>
        <span class="k">return</span> <span class="n">mean</span><span class="p">,</span> <span class="n">std</span>
    <span class="k">return</span> <span class="n">mean</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.RandomForestRegressor.score">
    <p>def <span class="ident">score</span>(</p><p>self, X, y, sample_weight=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Returns the coefficient of determination R^2 of the prediction.</p>
<p>The coefficient R^2 is defined as (1 - u/v), where u is the regression
sum of squares ((y_true - y_pred) <strong> 2).sum() and v is the residual
sum of squares ((y_true - y_true.mean()) </strong> 2).sum().
Best possible score is 1.0 and it can be negative (because the
model can be arbitrarily worse). A constant model that always
predicts the expected value of y, disregarding the input features,
would get a R^2 score of 0.0.</p>
<h2>Parameters</h2>
<p>X : array-like, shape = (n_samples, n_features)
    Test samples.</p>
<p>y : array-like, shape = (n_samples) or (n_samples, n_outputs)
    True values for X.</p>
<p>sample_weight : array-like, shape = [n_samples], optional
    Sample weights.</p>
<h2>Returns</h2>
<p>score : float
    R^2 of self.predict(X) wrt. y.</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.RandomForestRegressor.score', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.RandomForestRegressor.score" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">sample_weight</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Returns the coefficient of determination R^2 of the prediction.</span>
<span class="sd">    The coefficient R^2 is defined as (1 - u/v), where u is the regression</span>
<span class="sd">    sum of squares ((y_true - y_pred) ** 2).sum() and v is the residual</span>
<span class="sd">    sum of squares ((y_true - y_true.mean()) ** 2).sum().</span>
<span class="sd">    Best possible score is 1.0 and it can be negative (because the</span>
<span class="sd">    model can be arbitrarily worse). A constant model that always</span>
<span class="sd">    predicts the expected value of y, disregarding the input features,</span>
<span class="sd">    would get a R^2 score of 0.0.</span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    X : array-like, shape = (n_samples, n_features)</span>
<span class="sd">        Test samples.</span>
<span class="sd">    y : array-like, shape = (n_samples) or (n_samples, n_outputs)</span>
<span class="sd">        True values for X.</span>
<span class="sd">    sample_weight : array-like, shape = [n_samples], optional</span>
<span class="sd">        Sample weights.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    score : float</span>
<span class="sd">        R^2 of self.predict(X) wrt. y.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="kn">from</span> <span class="nn">.metrics</span> <span class="kn">import</span> <span class="n">r2_score</span>
    <span class="k">return</span> <span class="n">r2_score</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">),</span> <span class="n">sample_weight</span><span class="o">=</span><span class="n">sample_weight</span><span class="p">,</span>
                    <span class="n">multioutput</span><span class="o">=</span><span class="s1">&#39;variance_weighted&#39;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.RandomForestRegressor.set_params">
    <p>def <span class="ident">set_params</span>(</p><p>self, **params)</p>
    </div>
    

    
  
    <div class="desc"><p>Set the parameters of this estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The latter have parameters of the form
<code>&lt;component&gt;__&lt;parameter&gt;</code> so that it's possible to update each
component of a nested object.</p>
<h2>Returns</h2>
<p>self</p></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.RandomForestRegressor.set_params', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.RandomForestRegressor.set_params" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">set_params</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">**</span><span class="n">params</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Set the parameters of this estimator.</span>
<span class="sd">    The method works on simple estimators as well as on nested objects</span>
<span class="sd">    (such as pipelines). The latter have parameters of the form</span>
<span class="sd">    ``&lt;component&gt;__&lt;parameter&gt;`` so that it&#39;s possible to update each</span>
<span class="sd">    component of a nested object.</span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">params</span><span class="p">:</span>
        <span class="c1"># Simple optimisation to gain speed (inspect is slow)</span>
        <span class="k">return</span> <span class="bp">self</span>
    <span class="n">valid_params</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_params</span><span class="p">(</span><span class="n">deep</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">six</span><span class="o">.</span><span class="n">iteritems</span><span class="p">(</span><span class="n">params</span><span class="p">):</span>
        <span class="n">split</span> <span class="o">=</span> <span class="n">key</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;__&#39;</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">split</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="c1"># nested objects case</span>
            <span class="n">name</span><span class="p">,</span> <span class="n">sub_name</span> <span class="o">=</span> <span class="n">split</span>
            <span class="k">if</span> <span class="n">name</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_params</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid parameter </span><span class="si">%s</span><span class="s1"> for estimator </span><span class="si">%s</span><span class="s1">. &#39;</span>
                                 <span class="s1">&#39;Check the list of available parameters &#39;</span>
                                 <span class="s1">&#39;with `estimator.get_params().keys()`.&#39;</span> <span class="o">%</span>
                                 <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="bp">self</span><span class="p">))</span>
            <span class="n">sub_object</span> <span class="o">=</span> <span class="n">valid_params</span><span class="p">[</span><span class="n">name</span><span class="p">]</span>
            <span class="n">sub_object</span><span class="o">.</span><span class="n">set_params</span><span class="p">(</span><span class="o">**</span><span class="p">{</span><span class="n">sub_name</span><span class="p">:</span> <span class="n">value</span><span class="p">})</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># simple objects case</span>
            <span class="k">if</span> <span class="n">key</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">valid_params</span><span class="p">:</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s1">&#39;Invalid parameter </span><span class="si">%s</span><span class="s1"> for estimator </span><span class="si">%s</span><span class="s1">. &#39;</span>
                                 <span class="s1">&#39;Check the list of available parameters &#39;</span>
                                 <span class="s1">&#39;with `estimator.get_params().keys()`.&#39;</span> <span class="o">%</span>
                                 <span class="p">(</span><span class="n">key</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">__class__</span><span class="o">.</span><span class="n">__name__</span><span class="p">))</span>
            <span class="nb">setattr</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</pre></div>

  </div>
</div>

  </div>
  
            
  <div class="item">
    <div class="name def" id="skopt.learning.RandomForestRegressor.transform">
    <p>def <span class="ident">transform</span>(</p><p>*args, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>DEPRECATED: Support to use estimators as feature selectors will be removed in version 0.19. Use SelectFromModel instead.</p>
<p>Reduce X to its most important features.</p>
<div class="codehilite"><pre><span></span>    Uses ``coef_`` or ``feature_importances_`` to determine the most
    important features.  For models with a ``coef_`` for each class, the
    absolute sum over the classes is used.

    Parameters
    ----------
    X : array or scipy sparse matrix of shape [n_samples, n_features]
        The input samples.

    threshold : string, float or None, optional (default=None)
        The threshold value to use for feature selection. Features whose
        importance is greater or equal are kept while the others are
        discarded. If &quot;median&quot; (resp. &quot;mean&quot;), then the threshold value is
        the median (resp. the mean) of the feature importances. A scaling
        factor (e.g., &quot;1.25*mean&quot;) may also be used. If None and if
        available, the object attribute ``threshold`` is used. Otherwise,
        &quot;mean&quot; is used by default.

    Returns
    -------
    X_r : array of shape [n_samples, n_selected_features]
        The input samples with only the selected features.
</pre></div></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.learning.RandomForestRegressor.transform', this);">Show source &equiv;</a></p>
  <div id="source-skopt.learning.RandomForestRegressor.transform" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">wrapped</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">warnings</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">category</span><span class="o">=</span><span class="ne">DeprecationWarning</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">fun</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
          <h3>Instance variables</h3>
            <div class="item">
            <p id="skopt.learning.RandomForestRegressor.feature_importances_" class="name">var <span class="ident">feature_importances_</span></p>
            

            
  
    <div class="desc"><p>Return the feature importances (the higher, the more important the
   feature).</p>
<h2>Returns</h2>
<p>feature_importances_ : array, shape = [n_features]</p></div>
  <div class="source_cont">
</div>

            </div>
      </div>
      </div>
  </section>

    </article>
  <div class="clear"> </div>
  <footer id="footer">
    <p>
      Documentation generated by
      <a href="https://github.com/BurntSushi/pdoc">pdoc 0.3.2</a>
    </p>

    <p>Design by <a href="http://nadh.in">Kailash Nadh</a></p>
  </footer>
</div>
</body>
</html>
