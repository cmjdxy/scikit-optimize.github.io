<!doctype html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />

    <title>skopt API documentation</title>
    <meta name="description" content="Scikit-Optimize, or `skopt`, is a simple and efficient library
for sequential model-based optimizati..." />

  <link href='https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,300' rel='stylesheet' type='text/css'>
  
  <style type="text/css">
  
* {
  box-sizing: border-box;
}
/*! normalize.css v1.1.1 | MIT License | git.io/normalize */

/* ==========================================================================
   HTML5 display definitions
   ========================================================================== */

/**
 * Correct `block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
nav,
section,
summary {
    display: block;
}

/**
 * Correct `inline-block` display not defined in IE 6/7/8/9 and Firefox 3.
 */

audio,
canvas,
video {
    display: inline-block;
    *display: inline;
    *zoom: 1;
}

/**
 * Prevent modern browsers from displaying `audio` without controls.
 * Remove excess height in iOS 5 devices.
 */

audio:not([controls]) {
    display: none;
    height: 0;
}

/**
 * Address styling not present in IE 7/8/9, Firefox 3, and Safari 4.
 * Known issue: no IE 6 support.
 */

[hidden] {
    display: none;
}

/* ==========================================================================
   Base
   ========================================================================== */

/**
 * 1. Prevent system color scheme's background color being used in Firefox, IE,
 *    and Opera.
 * 2. Prevent system color scheme's text color being used in Firefox, IE, and
 *    Opera.
 * 3. Correct text resizing oddly in IE 6/7 when body `font-size` is set using
 *    `em` units.
 * 4. Prevent iOS text size adjust after orientation change, without disabling
 *    user zoom.
 */

html {
    background: #fff; /* 1 */
    color: #000; /* 2 */
    font-size: 100%; /* 3 */
    -webkit-text-size-adjust: 100%; /* 4 */
    -ms-text-size-adjust: 100%; /* 4 */
}

/**
 * Address `font-family` inconsistency between `textarea` and other form
 * elements.
 */

html,
button,
input,
select,
textarea {
    font-family: sans-serif;
}

/**
 * Address margins handled incorrectly in IE 6/7.
 */

body {
    margin: 0;
}

/* ==========================================================================
   Links
   ========================================================================== */

/**
 * Address `outline` inconsistency between Chrome and other browsers.
 */

a:focus {
    outline: thin dotted;
}

/**
 * Improve readability when focused and also mouse hovered in all browsers.
 */

a:active,
a:hover {
    outline: 0;
}

/* ==========================================================================
   Typography
   ========================================================================== */

/**
 * Address font sizes and margins set differently in IE 6/7.
 * Address font sizes within `section` and `article` in Firefox 4+, Safari 5,
 * and Chrome.
 */

h1 {
    font-size: 2em;
    margin: 0.67em 0;
}

h2 {
    font-size: 1.5em;
    margin: 0.83em 0;
}

h3 {
    font-size: 1.17em;
    margin: 1em 0;
}

h4 {
    font-size: 1em;
    margin: 1.33em 0;
}

h5 {
    font-size: 0.83em;
    margin: 1.67em 0;
}

h6 {
    font-size: 0.67em;
    margin: 2.33em 0;
}

/**
 * Address styling not present in IE 7/8/9, Safari 5, and Chrome.
 */

abbr[title] {
    border-bottom: 1px dotted;
}

/**
 * Address style set to `bolder` in Firefox 3+, Safari 4/5, and Chrome.
 */

b,
strong {
    font-weight: bold;
}

blockquote {
    margin: 1em 40px;
}

/**
 * Address styling not present in Safari 5 and Chrome.
 */

dfn {
    font-style: italic;
}

/**
 * Address differences between Firefox and other browsers.
 * Known issue: no IE 6/7 normalization.
 */

hr {
    -moz-box-sizing: content-box;
    box-sizing: content-box;
    height: 0;
}

/**
 * Address styling not present in IE 6/7/8/9.
 */

mark {
    background: #ff0;
    color: #000;
}

/**
 * Address margins set differently in IE 6/7.
 */

p,
pre {
    margin: 1em 0;
}

/**
 * Correct font family set oddly in IE 6, Safari 4/5, and Chrome.
 */

code,
kbd,
pre,
samp {
    font-family: monospace, serif;
    _font-family: 'courier new', monospace;
    font-size: 0.9em;
}

/**
 * Improve readability of pre-formatted text in all browsers.
 */

pre {
    white-space: pre;
    white-space: pre-wrap;
    word-wrap: break-word;
}

/**
 * Address CSS quotes not supported in IE 6/7.
 */

q {
    quotes: none;
}

/**
 * Address `quotes` property not supported in Safari 4.
 */

q:before,
q:after {
    content: '';
    content: none;
}

/**
 * Address inconsistent and variable font size in all browsers.
 */

small {
    font-size: 80%;
}

/**
 * Prevent `sub` and `sup` affecting `line-height` in all browsers.
 */

sub,
sup {
    font-size: 75%;
    line-height: 0;
    position: relative;
    vertical-align: baseline;
}

sup {
    top: -0.5em;
}

sub {
    bottom: -0.25em;
}

/* ==========================================================================
   Lists
   ========================================================================== */

/**
 * Address margins set differently in IE 6/7.
 */

dl,
menu,
ol,
ul {
    margin: 1em 0;
}

dd {
    margin: 0 0 0 40px;
}

/**
 * Address paddings set differently in IE 6/7.
 */

menu,
ol,
ul {
    padding: 0 0 0 40px;
}

/**
 * Correct list images handled incorrectly in IE 7.
 */

nav ul,
nav ol {
    list-style: none;
    list-style-image: none;
}

/* ==========================================================================
   Embedded content
   ========================================================================== */

/**
 * 1. Remove border when inside `a` element in IE 6/7/8/9 and Firefox 3.
 * 2. Improve image quality when scaled in IE 7.
 */

img {
    border: 0; /* 1 */
    -ms-interpolation-mode: bicubic; /* 2 */
}

/**
 * Correct overflow displayed oddly in IE 9.
 */

svg:not(:root) {
    overflow: hidden;
}

/* ==========================================================================
   Figures
   ========================================================================== */

/**
 * Address margin not present in IE 6/7/8/9, Safari 5, and Opera 11.
 */

figure {
    margin: 0;
}

/* ==========================================================================
   Forms
   ========================================================================== */

/**
 * Correct margin displayed oddly in IE 6/7.
 */

form {
    margin: 0;
}

/**
 * Define consistent border, margin, and padding.
 */

fieldset {
    border: 1px solid #c0c0c0;
    margin: 0 2px;
    padding: 0.35em 0.625em 0.75em;
}

/**
 * 1. Correct color not being inherited in IE 6/7/8/9.
 * 2. Correct text not wrapping in Firefox 3.
 * 3. Correct alignment displayed oddly in IE 6/7.
 */

legend {
    border: 0; /* 1 */
    padding: 0;
    white-space: normal; /* 2 */
    *margin-left: -7px; /* 3 */
}

/**
 * 1. Correct font size not being inherited in all browsers.
 * 2. Address margins set differently in IE 6/7, Firefox 3+, Safari 5,
 *    and Chrome.
 * 3. Improve appearance and consistency in all browsers.
 */

button,
input,
select,
textarea {
    font-size: 100%; /* 1 */
    margin: 0; /* 2 */
    vertical-align: baseline; /* 3 */
    *vertical-align: middle; /* 3 */
}

/**
 * Address Firefox 3+ setting `line-height` on `input` using `!important` in
 * the UA stylesheet.
 */

button,
input {
    line-height: normal;
}

/**
 * Address inconsistent `text-transform` inheritance for `button` and `select`.
 * All other form control elements do not inherit `text-transform` values.
 * Correct `button` style inheritance in Chrome, Safari 5+, and IE 6+.
 * Correct `select` style inheritance in Firefox 4+ and Opera.
 */

button,
select {
    text-transform: none;
}

/**
 * 1. Avoid the WebKit bug in Android 4.0.* where (2) destroys native `audio`
 *    and `video` controls.
 * 2. Correct inability to style clickable `input` types in iOS.
 * 3. Improve usability and consistency of cursor style between image-type
 *    `input` and others.
 * 4. Remove inner spacing in IE 7 without affecting normal text inputs.
 *    Known issue: inner spacing remains in IE 6.
 */

button,
html input[type="button"], /* 1 */
input[type="reset"],
input[type="submit"] {
    -webkit-appearance: button; /* 2 */
    cursor: pointer; /* 3 */
    *overflow: visible;  /* 4 */
}

/**
 * Re-set default cursor for disabled elements.
 */

button[disabled],
html input[disabled] {
    cursor: default;
}

/**
 * 1. Address box sizing set to content-box in IE 8/9.
 * 2. Remove excess padding in IE 8/9.
 * 3. Remove excess padding in IE 7.
 *    Known issue: excess padding remains in IE 6.
 */

input[type="checkbox"],
input[type="radio"] {
    box-sizing: border-box; /* 1 */
    padding: 0; /* 2 */
    *height: 13px; /* 3 */
    *width: 13px; /* 3 */
}

/**
 * 1. Address `appearance` set to `searchfield` in Safari 5 and Chrome.
 * 2. Address `box-sizing` set to `border-box` in Safari 5 and Chrome
 *    (include `-moz` to future-proof).
 */

input[type="search"] {
    -webkit-appearance: textfield; /* 1 */
    -moz-box-sizing: content-box;
    -webkit-box-sizing: content-box; /* 2 */
    box-sizing: content-box;
}

/**
 * Remove inner padding and search cancel button in Safari 5 and Chrome
 * on OS X.
 */

input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
    -webkit-appearance: none;
}

/**
 * Remove inner padding and border in Firefox 3+.
 */

button::-moz-focus-inner,
input::-moz-focus-inner {
    border: 0;
    padding: 0;
}

/**
 * 1. Remove default vertical scrollbar in IE 6/7/8/9.
 * 2. Improve readability and alignment in all browsers.
 */

textarea {
    overflow: auto; /* 1 */
    vertical-align: top; /* 2 */
}

/* ==========================================================================
   Tables
   ========================================================================== */

/**
 * Remove most spacing between table cells.
 */

table {
    border-collapse: collapse;
    border-spacing: 0;
}

  </style>

  <style type="text/css">
  
  html, body {
    margin: 0;
    padding: 0;
    min-height: 100%;
  }
  body {
    background: #fff;
    font-family: "Source Sans Pro", "Helvetica Neueue", Helvetica, sans;
    font-weight: 300;
    font-size: 16px;
    line-height: 1.6em;
  }
  #content {
    width: 70%;
    max-width: 850px;
    float: left;
    padding: 30px 60px;
    border-left: 1px solid #ddd;
  }
  #sidebar {
    width: 25%;
    float: left;
    padding: 30px;
    padding-top: 0px;
    overflow: hidden;
  }
  #nav {
    font-size: 130%;
    margin: 0 0 15px 0;
  }

  #top {
    display: block;
    position: fixed;
    bottom: 5px;
    left: 5px;
    font-size: .85em;
    text-transform: uppercase;
  }

  #footer {
    font-size: .75em;
    padding: 5px 30px;
    border-top: 1px solid #ddd;
    text-align: right;
  }
    #footer p {
      margin: 0 0 0 30px;
      display: inline-block;
    }

  h1, h2, h3, h4, h5 {
    font-weight: 300;
  }
  h1 {
    font-size: 2.5em;
    line-height: 1.1em;
    margin: 0 0 .50em 0;
  }

  h2 {
    font-size: 1.75em;
    margin: 1em 0 .50em 0;
  }

  h3 {
    margin: 25px 0 10px 0;
  }

  h4 {
    margin: 0;
    font-size: 105%;
  }

  a {
    color: #058;
    text-decoration: none;
    transition: color .3s ease-in-out;
  }

  a:hover {
    color: #e08524;
    transition: color .3s ease-in-out;
  }

  pre, code, .mono, .name {
    font-family: "Ubuntu Mono", "Cousine", "DejaVu Sans Mono", monospace;
  }

  .title .name {
    font-weight: bold;
  }
  .section-title {
    margin-top: 2em;
  }
  .ident {
    color: #900;
  }

  code {
    background: #f9f9f9;
  }

  pre {
    background: #fefefe;
    border: 1px solid #ddd;
    box-shadow: 2px 2px 0 #f3f3f3;
    margin: 0 30px;
    padding: 15px 30px;
  }

  .codehilite {
    margin: 0 30px 10px 30px;
  }

    .codehilite pre {
      margin: 0;
    }
    .codehilite .err { background: #ff3300; color: #fff !important; }

  table#module-list {
    font-size: 110%;
  }

    table#module-list tr td:first-child {
      padding-right: 10px;
      white-space: nowrap;
    }

    table#module-list td {
      vertical-align: top;
      padding-bottom: 8px;
    }

      table#module-list td p {
        margin: 0 0 7px 0;
      }

  .def {
    display: table;
  }

    .def p {
      display: table-cell;
      vertical-align: top;
      text-align: left;
    }

    .def p:first-child {
      white-space: nowrap;
    }

    .def p:last-child {
      width: 100%;
    }


  #index {
    list-style-type: none;
    margin: 0;
    padding: 0;
  }
    ul#index .class_name {
      /* font-size: 110%; */
      font-weight: bold;
    }
    #index ul {
      margin: 0;
    }

  .item {
    margin: 0 0 15px 0;
  }

    .item .class {
      margin: 0 0 25px 30px;
    }

      .item .class ul.class_list {
        margin: 0 0 20px 0;
      }

    .item .name {
      background: #fafafa;
      margin: 0;
      font-weight: bold;
      padding: 5px 10px;
      border-radius: 3px;
      display: inline-block;
      min-width: 40%;
    }
      .item .name:hover {
        background: #f6f6f6;
      }

    .item .empty_desc {
      margin: 0 0 5px 0;
      padding: 0;
    }

    .item .inheritance {
      margin: 3px 0 0 30px;
    }

    .item .inherited {
      color: #666;
    }

    .item .desc {
      padding: 0 8px;
      margin: 0;
    }

      .item .desc p {
        margin: 0 0 10px 0;
      }

    .source_cont {
      margin: 0;
      padding: 0;
    }

    .source_link a {
      background: #ffc300;
      font-weight: 400;
      font-size: .75em;
      text-transform: uppercase;
      color: #fff;
      text-shadow: 1px 1px 0 #f4b700;

      padding: 3px 8px;
      border-radius: 2px;
      transition: background .3s ease-in-out;
    }
      .source_link a:hover {
        background: #FF7200;
        text-shadow: none;
        transition: background .3s ease-in-out;
      }

    .source {
      display: none;
      max-height: 600px;
      overflow-y: scroll;
      margin-bottom: 15px;
    }

      .source .codehilite {
        margin: 0;
      }

  .desc h1, .desc h2, .desc h3 {
    font-size: 100% !important;
  }
  .clear {
    clear: both;
  }

  @media all and (max-width: 950px) {
    #sidebar {
      width: 35%;
    }
    #content {
      width: 65%;
    }
  }
  @media all and (max-width: 650px) {
    #top {
      display: none;
    }
    #sidebar {
      float: none;
      width: auto;
    }
    #content {
      float: none;
      width: auto;
      padding: 30px;
    }

    #index ul {
      padding: 0;
      margin-bottom: 15px;
    }
    #index ul li {
      display: inline-block;
      margin-right: 30px;
    }
    #footer {
      text-align: left;
    }
    #footer p {
      display: block;
      margin: inherit;
    }
  }

  /*****************************/

  </style>

  <style type="text/css">
  .codehilite .hll { background-color: #ffffcc }
.codehilite  { background: #f8f8f8; }
.codehilite .c { color: #408080; font-style: italic } /* Comment */
.codehilite .err { border: 1px solid #FF0000 } /* Error */
.codehilite .k { color: #008000; font-weight: bold } /* Keyword */
.codehilite .o { color: #666666 } /* Operator */
.codehilite .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.codehilite .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.codehilite .cp { color: #BC7A00 } /* Comment.Preproc */
.codehilite .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.codehilite .c1 { color: #408080; font-style: italic } /* Comment.Single */
.codehilite .cs { color: #408080; font-style: italic } /* Comment.Special */
.codehilite .gd { color: #A00000 } /* Generic.Deleted */
.codehilite .ge { font-style: italic } /* Generic.Emph */
.codehilite .gr { color: #FF0000 } /* Generic.Error */
.codehilite .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.codehilite .gi { color: #00A000 } /* Generic.Inserted */
.codehilite .go { color: #888888 } /* Generic.Output */
.codehilite .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.codehilite .gs { font-weight: bold } /* Generic.Strong */
.codehilite .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.codehilite .gt { color: #0044DD } /* Generic.Traceback */
.codehilite .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.codehilite .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.codehilite .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.codehilite .kp { color: #008000 } /* Keyword.Pseudo */
.codehilite .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.codehilite .kt { color: #B00040 } /* Keyword.Type */
.codehilite .m { color: #666666 } /* Literal.Number */
.codehilite .s { color: #BA2121 } /* Literal.String */
.codehilite .na { color: #7D9029 } /* Name.Attribute */
.codehilite .nb { color: #008000 } /* Name.Builtin */
.codehilite .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.codehilite .no { color: #880000 } /* Name.Constant */
.codehilite .nd { color: #AA22FF } /* Name.Decorator */
.codehilite .ni { color: #999999; font-weight: bold } /* Name.Entity */
.codehilite .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.codehilite .nf { color: #0000FF } /* Name.Function */
.codehilite .nl { color: #A0A000 } /* Name.Label */
.codehilite .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.codehilite .nt { color: #008000; font-weight: bold } /* Name.Tag */
.codehilite .nv { color: #19177C } /* Name.Variable */
.codehilite .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.codehilite .w { color: #bbbbbb } /* Text.Whitespace */
.codehilite .mb { color: #666666 } /* Literal.Number.Bin */
.codehilite .mf { color: #666666 } /* Literal.Number.Float */
.codehilite .mh { color: #666666 } /* Literal.Number.Hex */
.codehilite .mi { color: #666666 } /* Literal.Number.Integer */
.codehilite .mo { color: #666666 } /* Literal.Number.Oct */
.codehilite .sb { color: #BA2121 } /* Literal.String.Backtick */
.codehilite .sc { color: #BA2121 } /* Literal.String.Char */
.codehilite .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.codehilite .s2 { color: #BA2121 } /* Literal.String.Double */
.codehilite .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.codehilite .sh { color: #BA2121 } /* Literal.String.Heredoc */
.codehilite .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.codehilite .sx { color: #008000 } /* Literal.String.Other */
.codehilite .sr { color: #BB6688 } /* Literal.String.Regex */
.codehilite .s1 { color: #BA2121 } /* Literal.String.Single */
.codehilite .ss { color: #19177C } /* Literal.String.Symbol */
.codehilite .bp { color: #008000 } /* Name.Builtin.Pseudo */
.codehilite .vc { color: #19177C } /* Name.Variable.Class */
.codehilite .vg { color: #19177C } /* Name.Variable.Global */
.codehilite .vi { color: #19177C } /* Name.Variable.Instance */
.codehilite .il { color: #666666 } /* Literal.Number.Integer.Long */
  </style>

  <style type="text/css">
  
/* ==========================================================================
   EXAMPLE Media Queries for Responsive Design.
   These examples override the primary ('mobile first') styles.
   Modify as content requires.
   ========================================================================== */

@media only screen and (min-width: 35em) {
    /* Style adjustments for viewports that meet the condition */
}

@media print,
       (-o-min-device-pixel-ratio: 5/4),
       (-webkit-min-device-pixel-ratio: 1.25),
       (min-resolution: 120dpi) {
    /* Style adjustments for high resolution devices */
}

/* ==========================================================================
   Print styles.
   Inlined to avoid required HTTP connection: h5bp.com/r
   ========================================================================== */

@media print {
    * {
        background: transparent !important;
        color: #000 !important; /* Black prints faster: h5bp.com/s */
        box-shadow: none !important;
        text-shadow: none !important;
    }

    a,
    a:visited {
        text-decoration: underline;
    }

    a[href]:after {
        content: " (" attr(href) ")";
    }

    abbr[title]:after {
        content: " (" attr(title) ")";
    }

    /*
     * Don't show links for images, or javascript/internal links
     */

    .ir a:after,
    a[href^="javascript:"]:after,
    a[href^="#"]:after {
        content: "";
    }

    pre,
    blockquote {
        border: 1px solid #999;
        page-break-inside: avoid;
    }

    thead {
        display: table-header-group; /* h5bp.com/t */
    }

    tr,
    img {
        page-break-inside: avoid;
    }

    img {
        max-width: 100% !important;
    }

    @page {
        margin: 0.5cm;
    }

    p,
    h2,
    h3 {
        orphans: 3;
        widows: 3;
    }

    h2,
    h3 {
        page-break-after: avoid;
    }
}

  </style>

  <script type="text/javascript">
  function toggle(id, $link) {
    $node = document.getElementById(id);
    if (!$node)
    return;
    if (!$node.style.display || $node.style.display == 'none') {
    $node.style.display = 'block';
    $link.innerHTML = 'Hide source &nequiv;';
    } else {
    $node.style.display = 'none';
    $link.innerHTML = 'Show source &equiv;';
    }
  }
  </script>
  <script type="text/javascript" async
    src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
  </script>
  <script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}
  });
  </script>
</head>
<body>
<a href="https://github.com/scikit-optimize/scikit-optimize"><img style="position: absolute; top: 0; right: 0; border: 0;" src="https://camo.githubusercontent.com/652c5b9acfaddf3a9c326fa6bde407b87f7be0f4/68747470733a2f2f73332e616d617a6f6e6177732e636f6d2f6769746875622f726962626f6e732f666f726b6d655f72696768745f6f72616e67655f6666373630302e706e67" alt="Fork me on GitHub" data-canonical-src="https://s3.amazonaws.com/github/ribbons/forkme_right_orange_ff7600.png"></a>
<a href="#" id="top">Top</a>

<div id="container">
    
  
  <div id="sidebar">
    <ul id="index">
    <li class="set"><h3><a href="https://scikit-optimize.github.io/">Index</a></h3></li>


    <li class="set"><h3><a href="#header-functions">Functions</a></h3>
      
  <ul>
    <li class="mono"><a href="#skopt.dummy_minimize">dummy_minimize</a></li>
    <li class="mono"><a href="#skopt.dump">dump</a></li>
    <li class="mono"><a href="#skopt.forest_minimize">forest_minimize</a></li>
    <li class="mono"><a href="#skopt.gbrt_minimize">gbrt_minimize</a></li>
    <li class="mono"><a href="#skopt.gp_minimize">gp_minimize</a></li>
    <li class="mono"><a href="#skopt.load">load</a></li>
  </ul>

    </li>


    <li class="set"><h3><a href="#header-submodules">Sub-modules</a></h3>
      <ul>
        <li class="mono"><a href="acquisition.m.html">skopt.acquisition</a></li>
        <li class="mono"><a href="benchmarks.m.html">skopt.benchmarks</a></li>
        <li class="mono"><a href="callbacks.m.html">skopt.callbacks</a></li>
        <li class="mono"><a href="learning/index.html">skopt.learning</a></li>
        <li class="mono"><a href="optimizer/index.html">skopt.optimizer</a></li>
        <li class="mono"><a href="plots.m.html">skopt.plots</a></li>
        <li class="mono"><a href="space/index.html">skopt.space</a></li>
      </ul>
    </li>


    <li class="set"><h3><a href="#">Notebooks</a></h3>
      <ul>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/bayesian-optimization.html">Bayesian optimization</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/hyperparameter-optimization.html">Hyperparameter optimization</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/store-and-load-results.html">Store and load results</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/strategy-comparison.html">Strategy comparison</a></li>
        
        <li><a href="https://scikit-optimize.github.io/notebooks/visualizing-results.html">Visualizing results</a></li>
      </ul>
    </li>
    </ul>
  </div>

    <article id="content">
          
  

  


  <header id="section-intro">
  <h1 class="title"><span class="name">skopt</span> module</h1>
  <p>Scikit-Optimize, or <code>skopt</code>, is a simple and efficient library
for sequential model-based optimization, accessible to everybody
and reusable in various contexts.</p>
<p>The library is built on top of NumPy, SciPy and Scikit-Learn.</p>
<p><a href="https://travis-ci.org/scikit-optimize/scikit-optimize"><img alt="Build Status" src="https://travis-ci.org/scikit-optimize/scikit-optimize.svg?branch=master" /></a></p>
<h2>Install</h2>
<div class="codehilite"><pre><span></span>pip install scikit-optimize
</pre></div>


<h2>Getting started</h2>
<p>Find the minimum of the noisy function <code>f(x)</code> over the range <code>-2 &lt; x &lt; 2</code>
with <code>skopt</code>:</p>
<div class="codehilite"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">skopt</span> <span class="kn">import</span> <span class="n">gp_minimize</span>

<span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="mi">5</span> <span class="o">*</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">**</span> <span class="mi">2</span><span class="p">))</span> <span class="o">*</span>
            <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">()</span> <span class="o">*</span> <span class="mf">0.1</span><span class="p">)</span>

<span class="n">res</span> <span class="o">=</span> <span class="n">gp_minimize</span><span class="p">(</span><span class="n">f</span><span class="p">,</span> <span class="p">[(</span><span class="o">-</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">)])</span>
</pre></div>


<p>For more read our <a href="https://scikit-optimize.github.io/notebooks/bayesian-optimization.html">introduction to bayesian optimization</a>
and the other <a href="https://github.com/scikit-optimize/scikit-optimize/tree/master/examples">examples</a>.</p>
<h2>Development</h2>
<p>The library is still experimental and under heavy development.</p>
<p>The development version can be installed through:</p>
<div class="codehilite"><pre><span></span>git clone https://github.com/scikit-optimize/scikit-optimize.git
cd scikit-optimize
pip install -r requirements.txt
python setup.py develop
</pre></div>


<p>Run the tests by executing <code>nosetests</code> in the top level directory.</p>
  
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt', this);">Show source &equiv;</a></p>
  <div id="source-skopt" class="source">
    <div class="codehilite"><pre><span></span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Scikit-Optimize, or `skopt`, is a simple and efficient library</span>
<span class="sd">for sequential model-based optimization, accessible to everybody</span>
<span class="sd">and reusable in various contexts.</span>

<span class="sd">The library is built on top of NumPy, SciPy and Scikit-Learn.</span>

<span class="sd">[![Build Status](https://travis-ci.org/scikit-optimize/scikit-optimize.svg?branch=master)](https://travis-ci.org/scikit-optimize/scikit-optimize)</span>

<span class="sd">## Install</span>

<span class="sd">```</span>
<span class="sd">pip install scikit-optimize</span>
<span class="sd">```</span>

<span class="sd">## Getting started</span>

<span class="sd">Find the minimum of the noisy function `f(x)` over the range `-2 &lt; x &lt; 2`</span>
<span class="sd">with `skopt`:</span>

<span class="sd">```python</span>
<span class="sd">import numpy as np</span>
<span class="sd">from skopt import gp_minimize</span>

<span class="sd">def f(x):</span>
<span class="sd">    return (np.sin(5 * x[0]) * (1 - np.tanh(x[0] ** 2)) *</span>
<span class="sd">            np.random.randn() * 0.1)</span>

<span class="sd">res = gp_minimize(f, [(-2.0, 2.0)])</span>
<span class="sd">```</span>

<span class="sd">For more read our [introduction to bayesian optimization](https://scikit-optimize.github.io/notebooks/bayesian-optimization.html)</span>
<span class="sd">and the other [examples](https://github.com/scikit-optimize/scikit-optimize/tree/master/examples).</span>


<span class="sd">## Development</span>

<span class="sd">The library is still experimental and under heavy development.</span>

<span class="sd">The development version can be installed through:</span>

<span class="sd">    git clone https://github.com/scikit-optimize/scikit-optimize.git</span>
<span class="sd">    cd scikit-optimize</span>
<span class="sd">    pip install -r requirements.txt</span>
<span class="sd">    python setup.py develop</span>

<span class="sd">Run the tests by executing `nosetests` in the top level directory.</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">acquisition</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">benchmarks</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">callbacks</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">learning</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">plots</span>
<span class="kn">from</span> <span class="nn">.</span> <span class="kn">import</span> <span class="n">space</span>
<span class="kn">from</span> <span class="nn">.optimizer</span> <span class="kn">import</span> <span class="n">dummy_minimize</span>
<span class="kn">from</span> <span class="nn">.optimizer</span> <span class="kn">import</span> <span class="n">forest_minimize</span>
<span class="kn">from</span> <span class="nn">.optimizer</span> <span class="kn">import</span> <span class="n">gbrt_minimize</span>
<span class="kn">from</span> <span class="nn">.optimizer</span> <span class="kn">import</span> <span class="n">gp_minimize</span>
<span class="kn">from</span> <span class="nn">.utils</span> <span class="kn">import</span> <span class="n">load</span><span class="p">,</span> <span class="n">dump</span>


<span class="n">__version__</span> <span class="o">=</span> <span class="s2">&quot;0.2&quot;</span>


<span class="n">__all__</span> <span class="o">=</span> <span class="p">(</span><span class="s2">&quot;acquisition&quot;</span><span class="p">,</span>
           <span class="s2">&quot;benchmarks&quot;</span><span class="p">,</span>
           <span class="s2">&quot;callbacks&quot;</span><span class="p">,</span>
           <span class="s2">&quot;learning&quot;</span><span class="p">,</span>
           <span class="s2">&quot;optimizer&quot;</span><span class="p">,</span>
           <span class="s2">&quot;plots&quot;</span><span class="p">,</span>
           <span class="s2">&quot;space&quot;</span><span class="p">,</span>
           <span class="s2">&quot;gp_minimize&quot;</span><span class="p">,</span>
           <span class="s2">&quot;dummy_minimize&quot;</span><span class="p">,</span>
           <span class="s2">&quot;forest_minimize&quot;</span><span class="p">,</span>
           <span class="s2">&quot;gbrt_minimize&quot;</span><span class="p">,</span>
           <span class="s2">&quot;dump&quot;</span><span class="p">,</span>
           <span class="s2">&quot;load&quot;</span><span class="p">,</span>
           <span class="p">)</span>
</pre></div>

  </div>

  </header>



  <section id="section-items">

    <h2 class="section-title" id="header-functions">Functions</h2>
      
  <div class="item">
    <div class="name def" id="skopt.dummy_minimize">
    <p>def <span class="ident">dummy_minimize</span>(</p><p>func, dimensions, n_calls=100, x0=None, y0=None, random_state=None, verbose=False, callback=None)</p>
    </div>
    

    
  
    <div class="desc"><p>Random search by uniform sampling within the given bounds.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>func</code> [callable]:
    Function to minimize. Should take a array of parameters and
    return the function values.</p>
</li>
<li>
<p><code>dimensions</code> [list, shape=(n_dims,)]:
    List of search space dimensions.
    Each search dimension can be defined either as</p>
<ul>
<li>a <code>(upper_bound, lower_bound)</code> tuple (for <code>Real</code> or <code>Integer</code>
  dimensions),</li>
<li>a <code>(upper_bound, lower_bound, "prior")</code> tuple (for <code>Real</code>
  dimensions),</li>
<li>as a list of categories (for <code>Categorical</code> dimensions), or</li>
<li>an instance of a <code>Dimension</code> object (<code>Real</code>, <code>Integer</code> or
  <code>Categorical</code>).</li>
</ul>
</li>
<li>
<p><code>n_calls</code> [int, default=100]:
    Number of calls to <code>func</code> to find the minimum.</p>
</li>
<li>
<p><code>x0</code> [list, list of lists or <code>None</code>]:
    Initial input points.</p>
<ul>
<li>If it is a list of lists, use it as a list of input points.</li>
<li>If it is a list, use it as a single initial input point.</li>
<li>If it is <code>None</code>, no initial input points are used.</li>
</ul>
</li>
<li>
<p><code>y0</code> [list, scalar or <code>None</code>]
    Evaluation of initial input points.</p>
<ul>
<li>If it is a list, then it corresponds to evaluations of the function
  at each element of <code>x0</code> : the i-th element of <code>y0</code> corresponds
  to the function evaluated at the i-th element of <code>x0</code>.</li>
<li>If it is a scalar, then it corresponds to the evaluation of the
  function at <code>x0</code>.</li>
<li>If it is None and <code>x0</code> is provided, then the function is evaluated
  at each element of <code>x0</code>.</li>
</ul>
</li>
<li>
<p><code>random_state</code> [int, RandomState instance, or None (default)]:
    Set random state to something other than None for reproducible
    results.</p>
</li>
<li>
<p><code>verbose</code> [boolean, default=False]:
    Control the verbosity. It is advised to set the verbosity to True
    for long optimization runs.</p>
</li>
<li>
<p><code>callback</code> [callable, list of callables, optional]
    If callable then <code>callback(res)</code> is called after each call to <code>func</code>.
    If list of callables, then each callable in the list is called.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li>
<p><code>res</code> [<code>OptimizeResult</code>, scipy object]:
    The optimization result returned as a OptimizeResult object.
    Important attributes are:</p>
<ul>
<li><code>x</code> [list]: location of the minimum.</li>
<li><code>fun</code> [float]: function value at the minimum.</li>
<li><code>x_iters</code> [list of lists]: location of function evaluation for each
   iteration.</li>
<li><code>func_vals</code> [array]: function value for each iteration.</li>
<li><code>space</code> [Space]: the optimisation space.</li>
<li><code>specs</code> [dict]: the call specifications.</li>
<li><code>rng</code> [RandomState instance]: State of the random state
   at the end of minimization.</li>
</ul>
<p>For more details related to the OptimizeResult object, refer
http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.dummy_minimize', this);">Show source &equiv;</a></p>
  <div id="source-skopt.dummy_minimize" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">dummy_minimize</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">n_calls</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                   <span class="n">x0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">y0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                   <span class="n">callback</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Random search by uniform sampling within the given bounds.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `func` [callable]:</span>
<span class="sd">        Function to minimize. Should take a array of parameters and</span>
<span class="sd">        return the function values.</span>

<span class="sd">    * `dimensions` [list, shape=(n_dims,)]:</span>
<span class="sd">        List of search space dimensions.</span>
<span class="sd">        Each search dimension can be defined either as</span>

<span class="sd">        - a `(upper_bound, lower_bound)` tuple (for `Real` or `Integer`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - a `(upper_bound, lower_bound, &quot;prior&quot;)` tuple (for `Real`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - as a list of categories (for `Categorical` dimensions), or</span>
<span class="sd">        - an instance of a `Dimension` object (`Real`, `Integer` or</span>
<span class="sd">          `Categorical`).</span>

<span class="sd">    * `n_calls` [int, default=100]:</span>
<span class="sd">        Number of calls to `func` to find the minimum.</span>

<span class="sd">    * `x0` [list, list of lists or `None`]:</span>
<span class="sd">        Initial input points.</span>

<span class="sd">        - If it is a list of lists, use it as a list of input points.</span>
<span class="sd">        - If it is a list, use it as a single initial input point.</span>
<span class="sd">        - If it is `None`, no initial input points are used.</span>

<span class="sd">    * `y0` [list, scalar or `None`]</span>
<span class="sd">        Evaluation of initial input points.</span>

<span class="sd">        - If it is a list, then it corresponds to evaluations of the function</span>
<span class="sd">          at each element of `x0` : the i-th element of `y0` corresponds</span>
<span class="sd">          to the function evaluated at the i-th element of `x0`.</span>
<span class="sd">        - If it is a scalar, then it corresponds to the evaluation of the</span>
<span class="sd">          function at `x0`.</span>
<span class="sd">        - If it is None and `x0` is provided, then the function is evaluated</span>
<span class="sd">          at each element of `x0`.</span>

<span class="sd">    * `random_state` [int, RandomState instance, or None (default)]:</span>
<span class="sd">        Set random state to something other than None for reproducible</span>
<span class="sd">        results.</span>

<span class="sd">    * `verbose` [boolean, default=False]:</span>
<span class="sd">        Control the verbosity. It is advised to set the verbosity to True</span>
<span class="sd">        for long optimization runs.</span>

<span class="sd">    * `callback` [callable, list of callables, optional]</span>
<span class="sd">        If callable then `callback(res)` is called after each call to `func`.</span>
<span class="sd">        If list of callables, then each callable in the list is called.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `res` [`OptimizeResult`, scipy object]:</span>
<span class="sd">        The optimization result returned as a OptimizeResult object.</span>
<span class="sd">        Important attributes are:</span>

<span class="sd">        - `x` [list]: location of the minimum.</span>
<span class="sd">        - `fun` [float]: function value at the minimum.</span>
<span class="sd">        - `x_iters` [list of lists]: location of function evaluation for each</span>
<span class="sd">           iteration.</span>
<span class="sd">        - `func_vals` [array]: function value for each iteration.</span>
<span class="sd">        - `space` [Space]: the optimisation space.</span>
<span class="sd">        - `specs` [dict]: the call specifications.</span>
<span class="sd">        - `rng` [RandomState instance]: State of the random state</span>
<span class="sd">           at the end of minimization.</span>

<span class="sd">        For more details related to the OptimizeResult object, refer</span>
<span class="sd">        http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Save call args</span>
    <span class="n">specs</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;args&quot;</span><span class="p">:</span> <span class="n">copy</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">inspect</span><span class="o">.</span><span class="n">currentframe</span><span class="p">()</span><span class="o">.</span><span class="n">f_locals</span><span class="p">),</span>
             <span class="s2">&quot;function&quot;</span><span class="p">:</span> <span class="n">inspect</span><span class="o">.</span><span class="n">currentframe</span><span class="p">()</span><span class="o">.</span><span class="n">f_code</span><span class="o">.</span><span class="n">co_name</span><span class="p">}</span>

    <span class="c1"># Check params</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>
    <span class="n">space</span> <span class="o">=</span> <span class="n">Space</span><span class="p">(</span><span class="n">dimensions</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">x0</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">x0</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">elif</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x0</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">x0</span> <span class="o">=</span> <span class="p">[</span><span class="n">x0</span><span class="p">]</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`x0` should be a list, got </span><span class="si">%s</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">x0</span><span class="p">))</span>

    <span class="n">n_init_func_calls</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">y0</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">Iterable</span><span class="p">):</span>
            <span class="n">y0</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">y0</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">y0</span><span class="p">,</span> <span class="n">numbers</span><span class="o">.</span><span class="n">Number</span><span class="p">):</span>
            <span class="n">y0</span> <span class="o">=</span> <span class="p">[</span><span class="n">y0</span><span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`y0` should be an iterable or a scalar, got </span><span class="si">%s</span><span class="s2">&quot;</span>
                             <span class="o">%</span> <span class="nb">type</span><span class="p">(</span><span class="n">y0</span><span class="p">))</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span> <span class="o">!=</span> <span class="nb">len</span><span class="p">(</span><span class="n">y0</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`x0` and `y0` should have the same length&quot;</span><span class="p">)</span>

        <span class="k">if</span> <span class="ow">not</span> <span class="nb">all</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">,</span> <span class="n">y0</span><span class="p">)):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`y0` elements should be scalars&quot;</span><span class="p">)</span>

    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">y0</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">y0</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">n_calls</span> <span class="o">-=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>
        <span class="n">n_init_func_calls</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span>

    <span class="k">elif</span> <span class="nb">len</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="n">y0</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`x0`cannot be `None` when `y0` is provided&quot;</span><span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>  <span class="c1"># len(x0) == 0 and y0 is None</span>
        <span class="n">y0</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="n">callbacks</span> <span class="o">=</span> <span class="n">check_callback</span><span class="p">(</span><span class="n">callback</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">verbose</span><span class="p">:</span>
        <span class="n">callbacks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">VerboseCallback</span><span class="p">(</span>
            <span class="n">n_init</span><span class="o">=</span><span class="n">n_init_func_calls</span><span class="p">,</span> <span class="n">n_total</span><span class="o">=</span><span class="n">n_calls</span><span class="p">))</span>

    <span class="n">X</span> <span class="o">=</span> <span class="n">x0</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">y0</span>

    <span class="c1"># Random search</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">X</span> <span class="o">+</span> <span class="n">space</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="n">n_calls</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>
    <span class="n">first</span> <span class="o">=</span> <span class="bp">True</span>

    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">y0</span><span class="p">),</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)):</span>
        <span class="n">y_i</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">])</span>

        <span class="k">if</span> <span class="n">first</span><span class="p">:</span>
            <span class="n">first</span> <span class="o">=</span> <span class="bp">False</span>
            <span class="k">if</span> <span class="ow">not</span> <span class="n">np</span><span class="o">.</span><span class="n">isscalar</span><span class="p">(</span><span class="n">y_i</span><span class="p">):</span>
                <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;`func` should return a scalar&quot;</span><span class="p">)</span>

        <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">y_i</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">callbacks</span><span class="p">:</span>
            <span class="n">curr_res</span> <span class="o">=</span> <span class="n">create_result</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span> <span class="n">i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">],</span> <span class="n">y</span><span class="p">,</span> <span class="n">space</span><span class="p">,</span> <span class="n">rng</span><span class="p">,</span> <span class="n">specs</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">c</span> <span class="ow">in</span> <span class="n">callbacks</span><span class="p">:</span>
                <span class="n">c</span><span class="p">(</span><span class="n">curr_res</span><span class="p">)</span>

    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">create_result</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">space</span><span class="p">,</span> <span class="n">rng</span><span class="p">,</span> <span class="n">specs</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="skopt.dump">
    <p>def <span class="ident">dump</span>(</p><p>res, filename, store_objective=True, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>Store an skopt optimization result into a file.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>res</code> [<code>OptimizeResult</code>, scipy object]:
    Optimization result object to be stored.</p>
</li>
<li>
<p><code>filename</code> [string or <code>pathlib.Path</code>]:
    The path of the file in which it is to be stored. The compression
    method corresponding to one of the supported filename extensions ('.z',
    '.gz', '.bz2', '.xz' or '.lzma') will be used automatically.</p>
</li>
<li>
<p><code>store_objective</code> [boolean, default=True]:
    Whether the objective function should be stored. Set <code>store_objective</code>
    to <code>False</code> if your objective function (<code>.specs['args']['func']</code>) is
    unserializable (i.e. if an exception is raised when trying to serialize
    the optimization result).</p>
<p>Notice that if <code>store_objective</code> is set to <code>False</code>, a deep copy of the
optimization result is created, potentially leading to performance
problems if <code>res</code> is very large. If the objective function is not
critical, one can delete it before calling <code>skopt.dump()</code> and thus avoid
deep copying of <code>res</code>.</p>
</li>
<li>
<p><code>**kwargs</code> [other keyword arguments]:
    All other keyword arguments will be passed to <code>joblib.dump</code>.</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.dump', this);">Show source &equiv;</a></p>
  <div id="source-skopt.dump" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">dump</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="n">store_objective</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Store an skopt optimization result into a file.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `res` [`OptimizeResult`, scipy object]:</span>
<span class="sd">        Optimization result object to be stored.</span>

<span class="sd">    * `filename` [string or `pathlib.Path`]:</span>
<span class="sd">        The path of the file in which it is to be stored. The compression</span>
<span class="sd">        method corresponding to one of the supported filename extensions (&#39;.z&#39;,</span>
<span class="sd">        &#39;.gz&#39;, &#39;.bz2&#39;, &#39;.xz&#39; or &#39;.lzma&#39;) will be used automatically.</span>

<span class="sd">    * `store_objective` [boolean, default=True]:</span>
<span class="sd">        Whether the objective function should be stored. Set `store_objective`</span>
<span class="sd">        to `False` if your objective function (`.specs[&#39;args&#39;][&#39;func&#39;]`) is</span>
<span class="sd">        unserializable (i.e. if an exception is raised when trying to serialize</span>
<span class="sd">        the optimization result).</span>

<span class="sd">        Notice that if `store_objective` is set to `False`, a deep copy of the</span>
<span class="sd">        optimization result is created, potentially leading to performance</span>
<span class="sd">        problems if `res` is very large. If the objective function is not</span>
<span class="sd">        critical, one can delete it before calling `skopt.dump()` and thus avoid</span>
<span class="sd">        deep copying of `res`.</span>

<span class="sd">    * `**kwargs` [other keyword arguments]:</span>
<span class="sd">        All other keyword arguments will be passed to `joblib.dump`.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">store_objective</span><span class="p">:</span>
        <span class="n">dump_</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">elif</span> <span class="s1">&#39;func&#39;</span> <span class="ow">in</span> <span class="n">res</span><span class="o">.</span><span class="n">specs</span><span class="p">[</span><span class="s1">&#39;args&#39;</span><span class="p">]:</span>
        <span class="c1"># If the user does not want to store the objective and it is indeed</span>
        <span class="c1"># present in the provided object, then create a deep copy of it and</span>
        <span class="c1"># remove the objective function before dumping it with joblib.dump.</span>
        <span class="n">res_without_func</span> <span class="o">=</span> <span class="n">deepcopy</span><span class="p">(</span><span class="n">res</span><span class="p">)</span>
        <span class="k">del</span> <span class="n">res_without_func</span><span class="o">.</span><span class="n">specs</span><span class="p">[</span><span class="s1">&#39;args&#39;</span><span class="p">][</span><span class="s1">&#39;func&#39;</span><span class="p">]</span>
        <span class="n">dump_</span><span class="p">(</span><span class="n">res_without_func</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># If the user does not want to store the objective and it is already</span>
        <span class="c1"># missing in the provided object, dump it without copying.</span>
        <span class="n">dump_</span><span class="p">(</span><span class="n">res</span><span class="p">,</span> <span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="skopt.forest_minimize">
    <p>def <span class="ident">forest_minimize</span>(</p><p>func, dimensions, base_estimator=&#39;ET&#39;, n_calls=100, n_random_starts=10, acq_func=&#39;EI&#39;, acq_optimizer=&#39;auto&#39;, x0=None, y0=None, random_state=None, verbose=False, callback=None, n_points=1000, xi=0.01, kappa=1.96, n_jobs=1)</p>
    </div>
    

    
  
    <div class="desc"><p>Sequential optimisation using decision trees.</p>
<p>A tree based regression model is used to model the expensive to evaluate
function <code>func</code>. The model is improved by sequentially evaluating
the expensive function at the next best point. Thereby finding the
minimum of <code>func</code> with as few evaluations as possible.</p>
<p>The total number of evaluations, <code>n_calls</code>, are performed like the
following. If <code>x0</code> is provided but not <code>y0</code>, then the elements of <code>x0</code>
are first evaluated, followed by <code>n_random_starts</code> evaluations.
Finally, <code>n_calls - len(x0) - n_random_starts</code> evaluations are
made guided by the surrogate model. If <code>x0</code> and <code>y0</code> are both
provided then <code>n_random_starts</code> evaluations are first made then
<code>n_calls - n_random_starts</code> subsequent evaluations are made
guided by the surrogate model.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>func</code> [callable]:
    Function to minimize. Should take a array of parameters and
    return the function values.</p>
</li>
<li>
<p><code>dimensions</code> [list, shape=(n_dims,)]:
    List of search space dimensions.
    Each search dimension can be defined either as</p>
<ul>
<li>a <code>(upper_bound, lower_bound)</code> tuple (for <code>Real</code> or <code>Integer</code>
  dimensions),</li>
<li>a <code>(upper_bound, lower_bound, "prior")</code> tuple (for <code>Real</code>
  dimensions),</li>
<li>as a list of categories (for <code>Categorical</code> dimensions), or</li>
<li>an instance of a <code>Dimension</code> object (<code>Real</code>, <code>Integer</code> or
  <code>Categorical</code>).</li>
</ul>
</li>
<li>
<p><code>base_estimator</code> [string or <code>Regressor</code>, default=<code>"ET"</code>]:
    The regressor to use as surrogate model. Can be either</p>
<ul>
<li><code>"RF"</code> for random forest regressor</li>
<li><code>"ET"</code> for extra trees regressor</li>
<li>instance of regressor with support for <code>return_std</code> in its predict
  method</li>
</ul>
<p>The predefined models are initilized with good defaults. If you
want to adjust the model parameters pass your own instance of
a regressor which returns the mean and standard deviation when
making predictions.</p>
</li>
<li>
<p><code>n_calls</code> [int, default=100]:
    Number of calls to <code>func</code>.</p>
</li>
<li>
<p><code>n_random_starts</code> [int, default=10]:
    Number of evaluations of <code>func</code> with random initialization points
    before approximating the <code>func</code> with <code>base_estimator</code>.</p>
</li>
<li>
<p><code>acq_func</code> [string, default=<code>"LCB"</code>]:
    Function to minimize over the forest posterior. Can be either</p>
<ul>
<li><code>"LCB"</code> for lower confidence bound.</li>
<li><code>"EI"</code> for negative expected improvement.</li>
<li><code>"PI"</code> for negative probability of improvement.</li>
</ul>
</li>
<li>
<p><code>x0</code> [list, list of lists or <code>None</code>]:
    Initial input points.</p>
<ul>
<li>If it is a list of lists, use it as a list of input points.</li>
<li>If it is a list, use it as a single initial input point.</li>
<li>If it is <code>None</code>, no initial input points are used.</li>
</ul>
</li>
<li>
<p><code>y0</code> [list, scalar or <code>None</code>]:
    Evaluation of initial input points.</p>
<ul>
<li>If it is a list, then it corresponds to evaluations of the function
  at each element of <code>x0</code> : the i-th element of <code>y0</code> corresponds
  to the function evaluated at the i-th element of <code>x0</code>.</li>
<li>If it is a scalar, then it corresponds to the evaluation of the
  function at <code>x0</code>.</li>
<li>If it is None and <code>x0</code> is provided, then the function is evaluated
  at each element of <code>x0</code>.</li>
</ul>
</li>
<li>
<p><code>random_state</code> [int, RandomState instance, or None (default)]:
    Set random state to something other than None for reproducible
    results.</p>
</li>
<li>
<p><code>verbose</code> [boolean, default=False]:
    Control the verbosity. It is advised to set the verbosity to True
    for long optimization runs.</p>
</li>
<li>
<p><code>callback</code> [callable, optional]
    If provided, then <code>callback(res)</code> is called after call to func.</p>
</li>
<li>
<p><code>n_points</code> [int, default=1000]:
    Number of points to sample when minimizing the acquisition function.</p>
</li>
<li>
<p><code>xi</code> [float, default=0.01]:
    Controls how much improvement one wants over the previous best
    values. Used when the acquisition is either <code>"EI"</code> or <code>"PI"</code>.</p>
</li>
<li>
<p><code>kappa</code> [float, default=1.96]:
    Controls how much of the variance in the predicted values should be
    taken into account. If set to be very high, then we are favouring
    exploration over exploitation and vice versa.
    Used when the acquisition is <code>"LCB"</code>.</p>
</li>
<li>
<p><code>n_jobs</code> [int, default=1]:
    The number of jobs to run in parallel for <code>fit</code> and <code>predict</code>.
    If -1, then the number of jobs is set to the number of cores.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li>
<p><code>res</code> [<code>OptimizeResult</code>, scipy object]:
    The optimization result returned as a OptimizeResult object.
    Important attributes are:</p>
<ul>
<li><code>x</code> [list]: location of the minimum.</li>
<li><code>fun</code> [float]: function value at the minimum.</li>
<li><code>models</code>: surrogate models used for each iteration.</li>
<li><code>x_iters</code> [list of lists]: location of function evaluation for each
   iteration.</li>
<li><code>func_vals</code> [array]: function value for each iteration.</li>
<li><code>space</code> [Space]: the optimization space.</li>
<li><code>specs</code> [dict]`: the call specifications.</li>
</ul>
<p>For more details related to the OptimizeResult object, refer
http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.forest_minimize', this);">Show source &equiv;</a></p>
  <div id="source-skopt.forest_minimize" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">forest_minimize</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">base_estimator</span><span class="o">=</span><span class="s2">&quot;ET&quot;</span><span class="p">,</span>
                    <span class="n">n_calls</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_random_starts</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                    <span class="n">acq_func</span><span class="o">=</span><span class="s2">&quot;EI&quot;</span><span class="p">,</span> <span class="n">acq_optimizer</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                    <span class="n">x0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">y0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                    <span class="n">callback</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_points</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="mf">1.96</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sequential optimisation using decision trees.</span>

<span class="sd">    A tree based regression model is used to model the expensive to evaluate</span>
<span class="sd">    function `func`. The model is improved by sequentially evaluating</span>
<span class="sd">    the expensive function at the next best point. Thereby finding the</span>
<span class="sd">    minimum of `func` with as few evaluations as possible.</span>

<span class="sd">    The total number of evaluations, `n_calls`, are performed like the</span>
<span class="sd">    following. If `x0` is provided but not `y0`, then the elements of `x0`</span>
<span class="sd">    are first evaluated, followed by `n_random_starts` evaluations.</span>
<span class="sd">    Finally, `n_calls - len(x0) - n_random_starts` evaluations are</span>
<span class="sd">    made guided by the surrogate model. If `x0` and `y0` are both</span>
<span class="sd">    provided then `n_random_starts` evaluations are first made then</span>
<span class="sd">    `n_calls - n_random_starts` subsequent evaluations are made</span>
<span class="sd">    guided by the surrogate model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `func` [callable]:</span>
<span class="sd">        Function to minimize. Should take a array of parameters and</span>
<span class="sd">        return the function values.</span>

<span class="sd">    * `dimensions` [list, shape=(n_dims,)]:</span>
<span class="sd">        List of search space dimensions.</span>
<span class="sd">        Each search dimension can be defined either as</span>

<span class="sd">        - a `(upper_bound, lower_bound)` tuple (for `Real` or `Integer`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - a `(upper_bound, lower_bound, &quot;prior&quot;)` tuple (for `Real`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - as a list of categories (for `Categorical` dimensions), or</span>
<span class="sd">        - an instance of a `Dimension` object (`Real`, `Integer` or</span>
<span class="sd">          `Categorical`).</span>

<span class="sd">    * `base_estimator` [string or `Regressor`, default=`&quot;ET&quot;`]:</span>
<span class="sd">        The regressor to use as surrogate model. Can be either</span>

<span class="sd">        - `&quot;RF&quot;` for random forest regressor</span>
<span class="sd">        - `&quot;ET&quot;` for extra trees regressor</span>
<span class="sd">        - instance of regressor with support for `return_std` in its predict</span>
<span class="sd">          method</span>

<span class="sd">        The predefined models are initilized with good defaults. If you</span>
<span class="sd">        want to adjust the model parameters pass your own instance of</span>
<span class="sd">        a regressor which returns the mean and standard deviation when</span>
<span class="sd">        making predictions.</span>

<span class="sd">    * `n_calls` [int, default=100]:</span>
<span class="sd">        Number of calls to `func`.</span>

<span class="sd">    * `n_random_starts` [int, default=10]:</span>
<span class="sd">        Number of evaluations of `func` with random initialization points</span>
<span class="sd">        before approximating the `func` with `base_estimator`.</span>

<span class="sd">    * `acq_func` [string, default=`&quot;LCB&quot;`]:</span>
<span class="sd">        Function to minimize over the forest posterior. Can be either</span>

<span class="sd">        - `&quot;LCB&quot;` for lower confidence bound.</span>
<span class="sd">        - `&quot;EI&quot;` for negative expected improvement.</span>
<span class="sd">        - `&quot;PI&quot;` for negative probability of improvement.</span>

<span class="sd">    * `x0` [list, list of lists or `None`]:</span>
<span class="sd">        Initial input points.</span>

<span class="sd">        - If it is a list of lists, use it as a list of input points.</span>
<span class="sd">        - If it is a list, use it as a single initial input point.</span>
<span class="sd">        - If it is `None`, no initial input points are used.</span>

<span class="sd">    * `y0` [list, scalar or `None`]:</span>
<span class="sd">        Evaluation of initial input points.</span>

<span class="sd">        - If it is a list, then it corresponds to evaluations of the function</span>
<span class="sd">          at each element of `x0` : the i-th element of `y0` corresponds</span>
<span class="sd">          to the function evaluated at the i-th element of `x0`.</span>
<span class="sd">        - If it is a scalar, then it corresponds to the evaluation of the</span>
<span class="sd">          function at `x0`.</span>
<span class="sd">        - If it is None and `x0` is provided, then the function is evaluated</span>
<span class="sd">          at each element of `x0`.</span>

<span class="sd">    * `random_state` [int, RandomState instance, or None (default)]:</span>
<span class="sd">        Set random state to something other than None for reproducible</span>
<span class="sd">        results.</span>

<span class="sd">    * `verbose` [boolean, default=False]:</span>
<span class="sd">        Control the verbosity. It is advised to set the verbosity to True</span>
<span class="sd">        for long optimization runs.</span>

<span class="sd">    * `callback` [callable, optional]</span>
<span class="sd">        If provided, then `callback(res)` is called after call to func.</span>

<span class="sd">    * `n_points` [int, default=1000]:</span>
<span class="sd">        Number of points to sample when minimizing the acquisition function.</span>

<span class="sd">    * `xi` [float, default=0.01]:</span>
<span class="sd">        Controls how much improvement one wants over the previous best</span>
<span class="sd">        values. Used when the acquisition is either `&quot;EI&quot;` or `&quot;PI&quot;`.</span>

<span class="sd">    * `kappa` [float, default=1.96]:</span>
<span class="sd">        Controls how much of the variance in the predicted values should be</span>
<span class="sd">        taken into account. If set to be very high, then we are favouring</span>
<span class="sd">        exploration over exploitation and vice versa.</span>
<span class="sd">        Used when the acquisition is `&quot;LCB&quot;`.</span>

<span class="sd">    * `n_jobs` [int, default=1]:</span>
<span class="sd">        The number of jobs to run in parallel for `fit` and `predict`.</span>
<span class="sd">        If -1, then the number of jobs is set to the number of cores.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `res` [`OptimizeResult`, scipy object]:</span>
<span class="sd">        The optimization result returned as a OptimizeResult object.</span>
<span class="sd">        Important attributes are:</span>

<span class="sd">        - `x` [list]: location of the minimum.</span>
<span class="sd">        - `fun` [float]: function value at the minimum.</span>
<span class="sd">        - `models`: surrogate models used for each iteration.</span>
<span class="sd">        - `x_iters` [list of lists]: location of function evaluation for each</span>
<span class="sd">           iteration.</span>
<span class="sd">        - `func_vals` [array]: function value for each iteration.</span>
<span class="sd">        - `space` [Space]: the optimization space.</span>
<span class="sd">        - `specs` [dict]`: the call specifications.</span>

<span class="sd">        For more details related to the OptimizeResult object, refer</span>
<span class="sd">        http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>

    <span class="c1"># Default estimator</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">base_estimator</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">base_estimator</span> <span class="ow">not</span> <span class="ow">in</span> <span class="p">(</span><span class="s2">&quot;RF&quot;</span><span class="p">,</span> <span class="s2">&quot;ET&quot;</span><span class="p">):</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
                <span class="s2">&quot;Valid strings for the base_estimator parameter&quot;</span>
                <span class="s2">&quot; are: &#39;RF&#39; or &#39;ET&#39;, not &#39;</span><span class="si">%s</span><span class="s2">&#39;&quot;</span> <span class="o">%</span> <span class="n">base_estimator</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">base_estimator</span> <span class="o">==</span> <span class="s2">&quot;RF&quot;</span><span class="p">:</span>
            <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">RandomForestRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                                   <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                                   <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
                                                   <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

        <span class="k">elif</span> <span class="n">base_estimator</span> <span class="o">==</span> <span class="s2">&quot;ET&quot;</span><span class="p">:</span>
            <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">ExtraTreesRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
                                                 <span class="n">min_samples_leaf</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
                                                 <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
                                                 <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">base_minimize</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">base_estimator</span><span class="p">,</span>
                         <span class="n">n_calls</span><span class="o">=</span><span class="n">n_calls</span><span class="p">,</span> <span class="n">n_points</span><span class="o">=</span><span class="n">n_points</span><span class="p">,</span>
                         <span class="n">n_random_starts</span><span class="o">=</span><span class="n">n_random_starts</span><span class="p">,</span>
                         <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="o">=</span><span class="n">y0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span>
                         <span class="n">acq_func</span><span class="o">=</span><span class="n">acq_func</span><span class="p">,</span>
                         <span class="n">xi</span><span class="o">=</span><span class="n">xi</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="n">kappa</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
                         <span class="n">callback</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span> <span class="n">acq_optimizer</span><span class="o">=</span><span class="s2">&quot;sampling&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="skopt.gbrt_minimize">
    <p>def <span class="ident">gbrt_minimize</span>(</p><p>func, dimensions, base_estimator=None, n_calls=100, n_random_starts=10, acq_func=&#39;EI&#39;, acq_optimizer=&#39;auto&#39;, x0=None, y0=None, random_state=None, verbose=False, callback=None, n_points=1000, xi=0.01, kappa=1.96, n_jobs=1)</p>
    </div>
    

    
  
    <div class="desc"><p>Sequential optimization using gradient boosted trees.</p>
<p>Gradient boosted regression trees are used to model the (very)
expensive to evaluate function <code>func</code>. The model is improved
by sequentially evaluating the expensive function at the next
best point. Thereby finding the minimum of <code>func</code> with as
few evaluations as possible.</p>
<p>The total number of evaluations, <code>n_calls</code>, are performed like the
following. If <code>x0</code> is provided but not <code>y0</code>, then the elements of <code>x0</code>
are first evaluated, followed by <code>n_random_starts</code> evaluations.
Finally, <code>n_calls - len(x0) - n_random_starts</code> evaluations are
made guided by the surrogate model. If <code>x0</code> and <code>y0</code> are both
provided then <code>n_random_starts</code> evaluations are first made then
<code>n_calls - n_random_starts</code> subsequent evaluations are made
guided by the surrogate model.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>func</code> [callable]:
    Function to minimize. Should take a array of parameters and
    return the function values.</p>
</li>
<li>
<p><code>dimensions</code> [list, shape=(n_dims,)]:
    List of search space dimensions.
    Each search dimension can be defined either as</p>
<ul>
<li>a <code>(upper_bound, lower_bound)</code> tuple (for <code>Real</code> or <code>Integer</code>
  dimensions),</li>
<li>a <code>(upper_bound, lower_bound, "prior")</code> tuple (for <code>Real</code>
  dimensions),</li>
<li>as a list of categories (for <code>Categorical</code> dimensions), or</li>
<li>an instance of a <code>Dimension</code> object (<code>Real</code>, <code>Integer</code> or
  <code>Categorical</code>).</li>
</ul>
</li>
<li>
<p><code>base_estimator</code> [<code>GradientBoostingQuantileRegressor</code>]:
    The regressor to use as surrogate model</p>
</li>
<li>
<p><code>n_calls</code> [int, default=100]:
    Number of calls to <code>func</code>.</p>
</li>
<li>
<p><code>n_random_starts</code> [int, default=10]:
    Number of evaluations of <code>func</code> with random initialization points
    before approximating the <code>func</code> with <code>base_estimator</code>.</p>
</li>
<li>
<p><code>acq_func</code> [string, default=<code>"LCB"</code>]:
    Function to minimize over the forest posterior. Can be either</p>
<ul>
<li><code>"LCB"</code> for lower confidence bound.</li>
<li><code>"EI"</code> for negative expected improvement.</li>
<li><code>"PI"</code> for negative probability of improvement.</li>
</ul>
</li>
<li>
<p><code>x0</code> [list, list of lists or <code>None</code>]:
    Initial input points.</p>
<ul>
<li>If it is a list of lists, use it as a list of input points.</li>
<li>If it is a list, use it as a single initial input point.</li>
<li>If it is <code>None</code>, no initial input points are used.</li>
</ul>
</li>
<li>
<p><code>y0</code> [list, scalar or <code>None</code>]:
    Evaluation of initial input points.</p>
<ul>
<li>If it is a list, then it corresponds to evaluations of the function
  at each element of <code>x0</code> : the i-th element of <code>y0</code> corresponds
  to the function evaluated at the i-th element of <code>x0</code>.</li>
<li>If it is a scalar, then it corresponds to the evaluation of the
  function at <code>x0</code>.</li>
<li>If it is None and <code>x0</code> is provided, then the function is evaluated
  at each element of <code>x0</code>.</li>
</ul>
</li>
<li>
<p><code>random_state</code> [int, RandomState instance, or None (default)]:
    Set random state to something other than None for reproducible
    results.</p>
</li>
<li>
<p><code>verbose</code> [boolean, default=False]:
    Control the verbosity. It is advised to set the verbosity to True
    for long optimization runs.</p>
</li>
<li>
<p><code>callback</code> [callable, optional]
    If provided, then <code>callback(res)</code> is called after call to func.</p>
</li>
<li>
<p><code>n_points</code> [int, default=1000]:
    Number of points to sample when minimizing the acquisition function.</p>
</li>
<li>
<p><code>xi</code> [float, default=0.01]:
    Controls how much improvement one wants over the previous best
    values. Used when the acquisition is either <code>"EI"</code> or <code>"PI"</code>.</p>
</li>
<li>
<p><code>kappa</code> [float, default=1.96]:
    Controls how much of the variance in the predicted values should be
    taken into account. If set to be very high, then we are favouring
    exploration over exploitation and vice versa.
    Used when the acquisition is <code>"LCB"</code>.</p>
</li>
<li>
<p><code>n_jobs</code> [int, default=1]:
    The number of jobs to run in parallel for <code>fit</code> and <code>predict</code>.
    If -1, then the number of jobs is set to the number of cores.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li>
<p><code>res</code> [<code>OptimizeResult</code>, scipy object]:
    The optimization result returned as a OptimizeResult object.
    Important attributes are:</p>
<ul>
<li><code>x</code> [list]: location of the minimum.</li>
<li><code>fun</code> [float]: function value at the minimum.</li>
<li><code>models</code>: surrogate models used for each iteration.</li>
<li><code>x_iters</code> [list of lists]: location of function evaluation for each
   iteration.</li>
<li><code>func_vals</code> [array]: function value for each iteration.</li>
<li><code>space</code> [Space]: the optimization space.</li>
<li><code>specs</code> [dict]`: the call specifications.</li>
<li><code>rng</code> [RandomState instance]: State of the random state
   at the end of minimization.</li>
</ul>
<p>For more details related to the OptimizeResult object, refer
http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.gbrt_minimize', this);">Show source &equiv;</a></p>
  <div id="source-skopt.gbrt_minimize" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">gbrt_minimize</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">base_estimator</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                  <span class="n">n_calls</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_random_starts</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                  <span class="n">acq_func</span><span class="o">=</span><span class="s2">&quot;EI&quot;</span><span class="p">,</span> <span class="n">acq_optimizer</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span>
                  <span class="n">x0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">y0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span>
                  <span class="n">callback</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">n_points</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="mf">1.96</span><span class="p">,</span> <span class="n">n_jobs</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Sequential optimization using gradient boosted trees.</span>

<span class="sd">    Gradient boosted regression trees are used to model the (very)</span>
<span class="sd">    expensive to evaluate function `func`. The model is improved</span>
<span class="sd">    by sequentially evaluating the expensive function at the next</span>
<span class="sd">    best point. Thereby finding the minimum of `func` with as</span>
<span class="sd">    few evaluations as possible.</span>

<span class="sd">    The total number of evaluations, `n_calls`, are performed like the</span>
<span class="sd">    following. If `x0` is provided but not `y0`, then the elements of `x0`</span>
<span class="sd">    are first evaluated, followed by `n_random_starts` evaluations.</span>
<span class="sd">    Finally, `n_calls - len(x0) - n_random_starts` evaluations are</span>
<span class="sd">    made guided by the surrogate model. If `x0` and `y0` are both</span>
<span class="sd">    provided then `n_random_starts` evaluations are first made then</span>
<span class="sd">    `n_calls - n_random_starts` subsequent evaluations are made</span>
<span class="sd">    guided by the surrogate model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `func` [callable]:</span>
<span class="sd">        Function to minimize. Should take a array of parameters and</span>
<span class="sd">        return the function values.</span>

<span class="sd">    * `dimensions` [list, shape=(n_dims,)]:</span>
<span class="sd">        List of search space dimensions.</span>
<span class="sd">        Each search dimension can be defined either as</span>

<span class="sd">        - a `(upper_bound, lower_bound)` tuple (for `Real` or `Integer`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - a `(upper_bound, lower_bound, &quot;prior&quot;)` tuple (for `Real`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - as a list of categories (for `Categorical` dimensions), or</span>
<span class="sd">        - an instance of a `Dimension` object (`Real`, `Integer` or</span>
<span class="sd">          `Categorical`).</span>

<span class="sd">    * `base_estimator` [`GradientBoostingQuantileRegressor`]:</span>
<span class="sd">        The regressor to use as surrogate model</span>

<span class="sd">    * `n_calls` [int, default=100]:</span>
<span class="sd">        Number of calls to `func`.</span>

<span class="sd">    * `n_random_starts` [int, default=10]:</span>
<span class="sd">        Number of evaluations of `func` with random initialization points</span>
<span class="sd">        before approximating the `func` with `base_estimator`.</span>

<span class="sd">    * `acq_func` [string, default=`&quot;LCB&quot;`]:</span>
<span class="sd">        Function to minimize over the forest posterior. Can be either</span>

<span class="sd">        - `&quot;LCB&quot;` for lower confidence bound.</span>
<span class="sd">        - `&quot;EI&quot;` for negative expected improvement.</span>
<span class="sd">        - `&quot;PI&quot;` for negative probability of improvement.</span>

<span class="sd">    * `x0` [list, list of lists or `None`]:</span>
<span class="sd">        Initial input points.</span>

<span class="sd">        - If it is a list of lists, use it as a list of input points.</span>
<span class="sd">        - If it is a list, use it as a single initial input point.</span>
<span class="sd">        - If it is `None`, no initial input points are used.</span>

<span class="sd">    * `y0` [list, scalar or `None`]:</span>
<span class="sd">        Evaluation of initial input points.</span>

<span class="sd">        - If it is a list, then it corresponds to evaluations of the function</span>
<span class="sd">          at each element of `x0` : the i-th element of `y0` corresponds</span>
<span class="sd">          to the function evaluated at the i-th element of `x0`.</span>
<span class="sd">        - If it is a scalar, then it corresponds to the evaluation of the</span>
<span class="sd">          function at `x0`.</span>
<span class="sd">        - If it is None and `x0` is provided, then the function is evaluated</span>
<span class="sd">          at each element of `x0`.</span>

<span class="sd">    * `random_state` [int, RandomState instance, or None (default)]:</span>
<span class="sd">        Set random state to something other than None for reproducible</span>
<span class="sd">        results.</span>

<span class="sd">    * `verbose` [boolean, default=False]:</span>
<span class="sd">        Control the verbosity. It is advised to set the verbosity to True</span>
<span class="sd">        for long optimization runs.</span>

<span class="sd">    * `callback` [callable, optional]</span>
<span class="sd">        If provided, then `callback(res)` is called after call to func.</span>

<span class="sd">    * `n_points` [int, default=1000]:</span>
<span class="sd">        Number of points to sample when minimizing the acquisition function.</span>

<span class="sd">    * `xi` [float, default=0.01]:</span>
<span class="sd">        Controls how much improvement one wants over the previous best</span>
<span class="sd">        values. Used when the acquisition is either `&quot;EI&quot;` or `&quot;PI&quot;`.</span>

<span class="sd">    * `kappa` [float, default=1.96]:</span>
<span class="sd">        Controls how much of the variance in the predicted values should be</span>
<span class="sd">        taken into account. If set to be very high, then we are favouring</span>
<span class="sd">        exploration over exploitation and vice versa.</span>
<span class="sd">        Used when the acquisition is `&quot;LCB&quot;`.</span>

<span class="sd">    * `n_jobs` [int, default=1]:</span>
<span class="sd">        The number of jobs to run in parallel for `fit` and `predict`.</span>
<span class="sd">        If -1, then the number of jobs is set to the number of cores.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `res` [`OptimizeResult`, scipy object]:</span>
<span class="sd">        The optimization result returned as a OptimizeResult object.</span>
<span class="sd">        Important attributes are:</span>

<span class="sd">        - `x` [list]: location of the minimum.</span>
<span class="sd">        - `fun` [float]: function value at the minimum.</span>
<span class="sd">        - `models`: surrogate models used for each iteration.</span>
<span class="sd">        - `x_iters` [list of lists]: location of function evaluation for each</span>
<span class="sd">           iteration.</span>
<span class="sd">        - `func_vals` [array]: function value for each iteration.</span>
<span class="sd">        - `space` [Space]: the optimization space.</span>
<span class="sd">        - `specs` [dict]`: the call specifications.</span>
<span class="sd">        - `rng` [RandomState instance]: State of the random state</span>
<span class="sd">           at the end of minimization.</span>

<span class="sd">        For more details related to the OptimizeResult object, refer</span>
<span class="sd">        http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check params</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>

    <span class="c1"># Default estimator</span>
    <span class="k">if</span> <span class="n">base_estimator</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">gbrt</span> <span class="o">=</span> <span class="n">GradientBoostingRegressor</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;quantile&#39;</span><span class="p">)</span>
        <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">GradientBoostingQuantileRegressor</span><span class="p">(</span><span class="n">base_estimator</span><span class="o">=</span><span class="n">gbrt</span><span class="p">,</span>
                                                           <span class="n">n_jobs</span><span class="o">=</span><span class="n">n_jobs</span><span class="p">,</span>
                                                           <span class="n">random_state</span><span class="o">=</span><span class="n">rng</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">base_minimize</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">base_estimator</span><span class="p">,</span>
                         <span class="n">n_calls</span><span class="o">=</span><span class="n">n_calls</span><span class="p">,</span> <span class="n">n_points</span><span class="o">=</span><span class="n">n_points</span><span class="p">,</span>
                         <span class="n">n_random_starts</span><span class="o">=</span><span class="n">n_random_starts</span><span class="p">,</span>
                         <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="o">=</span><span class="n">y0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="n">xi</span><span class="p">,</span>
                         <span class="n">kappa</span><span class="o">=</span><span class="n">kappa</span><span class="p">,</span> <span class="n">acq_func</span><span class="o">=</span><span class="n">acq_func</span><span class="p">,</span>
                         <span class="n">callback</span><span class="o">=</span><span class="n">callback</span><span class="p">,</span> <span class="n">acq_optimizer</span><span class="o">=</span><span class="s2">&quot;sampling&quot;</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="skopt.gp_minimize">
    <p>def <span class="ident">gp_minimize</span>(</p><p>func, dimensions, base_estimator=None, n_calls=100, n_random_starts=10, acq_func=&#39;EI&#39;, acq_optimizer=&#39;auto&#39;, x0=None, y0=None, random_state=None, verbose=False, callback=None, n_points=10000, n_restarts_optimizer=5, xi=0.01, kappa=1.96, noise=&#39;gaussian&#39;)</p>
    </div>
    

    
  
    <div class="desc"><p>Bayesian optimization using Gaussian Processes.</p>
<p>If every function evaluation is expensive, for instance
when the parameters are the hyperparameters of a neural network
and the function evaluation is the mean cross-validation score across
ten folds, optimizing the hyperparameters by standard optimization
routines would take for ever!</p>
<p>The idea is to approximate the function using a Gaussian process.
In other words the function values are assumed to follow a multivariate
gaussian. The covariance of the function values are given by a
GP kernel between the parameters. Then a smart choice to choose the
next parameter to evaluate can be made by the acquisition function
over the Gaussian prior which is much quicker to evaluate.</p>
<p>The total number of evaluations, <code>n_calls</code>, are performed like the
following. If <code>x0</code> is provided but not <code>y0</code>, then the elements of <code>x0</code>
are first evaluated, followed by <code>n_random_starts</code> evaluations.
Finally, <code>n_calls - len(x0) - n_random_starts</code> evaluations are
made guided by the surrogate model. If <code>x0</code> and <code>y0</code> are both
provided then <code>n_random_starts</code> evaluations are first made then
<code>n_calls - n_random_starts</code> subsequent evaluations are made
guided by the surrogate model.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>func</code> [callable]:
    Function to minimize. Should take a array of parameters and
    return the function values.</p>
</li>
<li>
<p><code>dimensions</code> [list, shape=(n_dims,)]:
    List of search space dimensions.
    Each search dimension can be defined either as</p>
<ul>
<li>a <code>(upper_bound, lower_bound)</code> tuple (for <code>Real</code> or <code>Integer</code>
  dimensions),</li>
<li>a <code>(upper_bound, lower_bound, "prior")</code> tuple (for <code>Real</code>
  dimensions),</li>
<li>as a list of categories (for <code>Categorical</code> dimensions), or</li>
<li>an instance of a <code>Dimension</code> object (<code>Real</code>, <code>Integer</code> or
  <code>Categorical</code>).</li>
</ul>
</li>
<li>
<p><code>base_estimator</code> [a Gaussian process estimator]:
    The Gaussian process estimator to use for optimization.
    By default, a Matern kernel is used with the following
    hyperparameters tuned.</p>
<ul>
<li>All the length scales of the Matern kernel.</li>
<li>The covariance amplitude that each element is multiplied with.</li>
<li>Noise that is added to the matern kernel. The noise is assumed
  to be iid gaussian.</li>
</ul>
</li>
<li>
<p><code>n_calls</code> [int, default=100]:
    Number of calls to <code>func</code>.</p>
</li>
<li>
<p><code>n_random_starts</code> [int, default=10]:
    Number of evaluations of <code>func</code> with random initialization points
    before approximating the <code>func</code> with <code>base_estimator</code>.</p>
</li>
<li>
<p><code>acq_func</code> [string, default=<code>"EI"</code>]:
    Function to minimize over the gaussian prior. Can be either</p>
<ul>
<li><code>"LCB"</code> for lower confidence bound.</li>
<li><code>"EI"</code> for negative expected improvement.</li>
<li><code>"PI"</code> for negative probability of improvement.</li>
</ul>
</li>
<li>
<p><code>acq_optimizer</code> [string, <code>"auto"</code>, <code>"sampling"</code> or <code>"lbfgs"</code>, default=<code>"auto"</code>]:
    Method to minimize the acquistion function. The fit model
    is updated with the optimal value obtained by optimizing <code>acq_func</code>
    with <code>acq_optimizer</code>.</p>
<ul>
<li>If set to <code>"sampling"</code>, then <code>acq_func</code> is optimized by computing
  <code>acq_func</code> at <code>n_points</code> sampled randomly.</li>
<li>If set to <code>"lbfgs"</code>, then <code>acq_func</code> is optimized by<ul>
<li>Sampling <code>n_restarts_optimizer</code> points randomly.</li>
<li><code>"lbfgs"</code> is run for 20 iterations with these points as initial
    points to find local minima.</li>
<li>The optimal of these local minima is used to update the prior.</li>
</ul>
</li>
<li>If set to <code>"auto"</code>, then it is set to <code>"lbfgs"`` if
  all the search dimensions are Real (continuous). It defaults to</code>"sampling"` for all other cases.</li>
</ul>
</li>
<li>
<p><code>x0</code> [list, list of lists or <code>None</code>]:
    Initial input points.</p>
<ul>
<li>If it is a list of lists, use it as a list of input points.</li>
<li>If it is a list, use it as a single initial input point.</li>
<li>If it is <code>None</code>, no initial input points are used.</li>
</ul>
</li>
<li>
<p><code>y0</code> [list, scalar or <code>None</code>]
    Evaluation of initial input points.</p>
<ul>
<li>If it is a list, then it corresponds to evaluations of the function
  at each element of <code>x0</code> : the i-th element of <code>y0</code> corresponds
  to the function evaluated at the i-th element of <code>x0</code>.</li>
<li>If it is a scalar, then it corresponds to the evaluation of the
  function at <code>x0</code>.</li>
<li>If it is None and <code>x0</code> is provided, then the function is evaluated
  at each element of <code>x0</code>.</li>
</ul>
</li>
<li>
<p><code>random_state</code> [int, RandomState instance, or None (default)]:
    Set random state to something other than None for reproducible
    results.</p>
</li>
<li>
<p><code>verbose</code> [boolean, default=False]:
    Control the verbosity. It is advised to set the verbosity to True
    for long optimization runs.</p>
</li>
<li>
<p><code>callback</code> [callable, list of callables, optional]
    If callable then <code>callback(res)</code> is called after each call to <code>func</code>.
    If list of callables, then each callable in the list is called.</p>
</li>
<li>
<p><code>n_points</code> [int, default=500]:
    Number of points to sample to determine the next "best" point.
    Useless if acq_optimizer is set to <code>"lbfgs"</code>.</p>
</li>
<li>
<p><code>n_restarts_optimizer</code> [int, default=5]:
    The number of restarts of the optimizer when <code>acq_optimizer</code> is <code>"lbfgs"</code>.</p>
</li>
<li>
<p><code>kappa</code> [float, default=1.96]:
    Controls how much of the variance in the predicted values should be
    taken into account. If set to be very high, then we are favouring
    exploration over exploitation and vice versa.
    Used when the acquisition is <code>"LCB"</code>.</p>
</li>
<li>
<p><code>xi</code> [float, default=0.01]:
    Controls how much improvement one wants over the previous best
    values. Used when the acquisition is either <code>"EI"</code> or <code>"PI"</code>.</p>
</li>
<li>
<p><code>noise</code> [float, default="gaussian"]:</p>
<ul>
<li>Use noise="gaussian" if the objective returns noisy observations.
  The noise of each observation is assumed to be iid with
  mean zero and a fixed variance.</li>
<li>If the variance is known before-hand, this can be set directly
  to the variance of the noise.</li>
<li>Set this to a value close to zero (1e-10) if the function is noise-free.
  Setting to zero might cause stability issues.</li>
</ul>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li>
<p><code>res</code> [<code>OptimizeResult</code>, scipy object]:
    The optimization result returned as a OptimizeResult object.
    Important attributes are:</p>
<ul>
<li><code>x</code> [list]: location of the minimum.</li>
<li><code>fun</code> [float]: function value at the minimum.</li>
<li><code>models</code>: surrogate models used for each iteration.</li>
<li><code>x_iters</code> [list of lists]: location of function evaluation for each
   iteration.</li>
<li><code>func_vals</code> [array]: function value for each iteration.</li>
<li><code>space</code> [Space]: the optimization space.</li>
<li><code>specs</code> [dict]`: the call specifications.</li>
<li><code>rng</code> [RandomState instance]: State of the random state
   at the end of minimization.</li>
</ul>
<p>For more details related to the OptimizeResult object, refer
http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</p>
</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.gp_minimize', this);">Show source &equiv;</a></p>
  <div id="source-skopt.gp_minimize" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">gp_minimize</span><span class="p">(</span><span class="n">func</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">base_estimator</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">n_calls</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_random_starts</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                <span class="n">acq_func</span><span class="o">=</span><span class="s2">&quot;EI&quot;</span><span class="p">,</span> <span class="n">acq_optimizer</span><span class="o">=</span><span class="s2">&quot;auto&quot;</span><span class="p">,</span> <span class="n">x0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">y0</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">random_state</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">callback</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span>
                <span class="n">n_points</span><span class="o">=</span><span class="mi">10000</span><span class="p">,</span> <span class="n">n_restarts_optimizer</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">xi</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="mf">1.96</span><span class="p">,</span>
                <span class="n">noise</span><span class="o">=</span><span class="s2">&quot;gaussian&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Bayesian optimization using Gaussian Processes.</span>

<span class="sd">    If every function evaluation is expensive, for instance</span>
<span class="sd">    when the parameters are the hyperparameters of a neural network</span>
<span class="sd">    and the function evaluation is the mean cross-validation score across</span>
<span class="sd">    ten folds, optimizing the hyperparameters by standard optimization</span>
<span class="sd">    routines would take for ever!</span>

<span class="sd">    The idea is to approximate the function using a Gaussian process.</span>
<span class="sd">    In other words the function values are assumed to follow a multivariate</span>
<span class="sd">    gaussian. The covariance of the function values are given by a</span>
<span class="sd">    GP kernel between the parameters. Then a smart choice to choose the</span>
<span class="sd">    next parameter to evaluate can be made by the acquisition function</span>
<span class="sd">    over the Gaussian prior which is much quicker to evaluate.</span>

<span class="sd">    The total number of evaluations, `n_calls`, are performed like the</span>
<span class="sd">    following. If `x0` is provided but not `y0`, then the elements of `x0`</span>
<span class="sd">    are first evaluated, followed by `n_random_starts` evaluations.</span>
<span class="sd">    Finally, `n_calls - len(x0) - n_random_starts` evaluations are</span>
<span class="sd">    made guided by the surrogate model. If `x0` and `y0` are both</span>
<span class="sd">    provided then `n_random_starts` evaluations are first made then</span>
<span class="sd">    `n_calls - n_random_starts` subsequent evaluations are made</span>
<span class="sd">    guided by the surrogate model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `func` [callable]:</span>
<span class="sd">        Function to minimize. Should take a array of parameters and</span>
<span class="sd">        return the function values.</span>

<span class="sd">    * `dimensions` [list, shape=(n_dims,)]:</span>
<span class="sd">        List of search space dimensions.</span>
<span class="sd">        Each search dimension can be defined either as</span>

<span class="sd">        - a `(upper_bound, lower_bound)` tuple (for `Real` or `Integer`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - a `(upper_bound, lower_bound, &quot;prior&quot;)` tuple (for `Real`</span>
<span class="sd">          dimensions),</span>
<span class="sd">        - as a list of categories (for `Categorical` dimensions), or</span>
<span class="sd">        - an instance of a `Dimension` object (`Real`, `Integer` or</span>
<span class="sd">          `Categorical`).</span>

<span class="sd">    * `base_estimator` [a Gaussian process estimator]:</span>
<span class="sd">        The Gaussian process estimator to use for optimization.</span>
<span class="sd">        By default, a Matern kernel is used with the following</span>
<span class="sd">        hyperparameters tuned.</span>
<span class="sd">        - All the length scales of the Matern kernel.</span>
<span class="sd">        - The covariance amplitude that each element is multiplied with.</span>
<span class="sd">        - Noise that is added to the matern kernel. The noise is assumed</span>
<span class="sd">          to be iid gaussian.</span>

<span class="sd">    * `n_calls` [int, default=100]:</span>
<span class="sd">        Number of calls to `func`.</span>

<span class="sd">    * `n_random_starts` [int, default=10]:</span>
<span class="sd">        Number of evaluations of `func` with random initialization points</span>
<span class="sd">        before approximating the `func` with `base_estimator`.</span>

<span class="sd">    * `acq_func` [string, default=`&quot;EI&quot;`]:</span>
<span class="sd">        Function to minimize over the gaussian prior. Can be either</span>

<span class="sd">        - `&quot;LCB&quot;` for lower confidence bound.</span>
<span class="sd">        - `&quot;EI&quot;` for negative expected improvement.</span>
<span class="sd">        - `&quot;PI&quot;` for negative probability of improvement.</span>

<span class="sd">    * `acq_optimizer` [string, `&quot;auto&quot;`, `&quot;sampling&quot;` or `&quot;lbfgs&quot;`, default=`&quot;auto&quot;`]:</span>
<span class="sd">        Method to minimize the acquistion function. The fit model</span>
<span class="sd">        is updated with the optimal value obtained by optimizing `acq_func`</span>
<span class="sd">        with `acq_optimizer`.</span>

<span class="sd">        - If set to `&quot;sampling&quot;`, then `acq_func` is optimized by computing</span>
<span class="sd">          `acq_func` at `n_points` sampled randomly.</span>
<span class="sd">        - If set to `&quot;lbfgs&quot;`, then `acq_func` is optimized by</span>
<span class="sd">              - Sampling `n_restarts_optimizer` points randomly.</span>
<span class="sd">              - `&quot;lbfgs&quot;` is run for 20 iterations with these points as initial</span>
<span class="sd">                points to find local minima.</span>
<span class="sd">              - The optimal of these local minima is used to update the prior.</span>
<span class="sd">        - If set to `&quot;auto&quot;`, then it is set to `&quot;lbfgs&quot;`` if</span>
<span class="sd">          all the search dimensions are Real (continuous). It defaults to</span>
<span class="sd">          `&quot;sampling&quot;` for all other cases.</span>

<span class="sd">    * `x0` [list, list of lists or `None`]:</span>
<span class="sd">        Initial input points.</span>

<span class="sd">        - If it is a list of lists, use it as a list of input points.</span>
<span class="sd">        - If it is a list, use it as a single initial input point.</span>
<span class="sd">        - If it is `None`, no initial input points are used.</span>

<span class="sd">    * `y0` [list, scalar or `None`]</span>
<span class="sd">        Evaluation of initial input points.</span>

<span class="sd">        - If it is a list, then it corresponds to evaluations of the function</span>
<span class="sd">          at each element of `x0` : the i-th element of `y0` corresponds</span>
<span class="sd">          to the function evaluated at the i-th element of `x0`.</span>
<span class="sd">        - If it is a scalar, then it corresponds to the evaluation of the</span>
<span class="sd">          function at `x0`.</span>
<span class="sd">        - If it is None and `x0` is provided, then the function is evaluated</span>
<span class="sd">          at each element of `x0`.</span>

<span class="sd">    * `random_state` [int, RandomState instance, or None (default)]:</span>
<span class="sd">        Set random state to something other than None for reproducible</span>
<span class="sd">        results.</span>

<span class="sd">    * `verbose` [boolean, default=False]:</span>
<span class="sd">        Control the verbosity. It is advised to set the verbosity to True</span>
<span class="sd">        for long optimization runs.</span>

<span class="sd">    * `callback` [callable, list of callables, optional]</span>
<span class="sd">        If callable then `callback(res)` is called after each call to `func`.</span>
<span class="sd">        If list of callables, then each callable in the list is called.</span>

<span class="sd">    * `n_points` [int, default=500]:</span>
<span class="sd">        Number of points to sample to determine the next &quot;best&quot; point.</span>
<span class="sd">        Useless if acq_optimizer is set to `&quot;lbfgs&quot;`.</span>

<span class="sd">    * `n_restarts_optimizer` [int, default=5]:</span>
<span class="sd">        The number of restarts of the optimizer when `acq_optimizer` is `&quot;lbfgs&quot;`.</span>

<span class="sd">    * `kappa` [float, default=1.96]:</span>
<span class="sd">        Controls how much of the variance in the predicted values should be</span>
<span class="sd">        taken into account. If set to be very high, then we are favouring</span>
<span class="sd">        exploration over exploitation and vice versa.</span>
<span class="sd">        Used when the acquisition is `&quot;LCB&quot;`.</span>

<span class="sd">    * `xi` [float, default=0.01]:</span>
<span class="sd">        Controls how much improvement one wants over the previous best</span>
<span class="sd">        values. Used when the acquisition is either `&quot;EI&quot;` or `&quot;PI&quot;`.</span>

<span class="sd">    * `noise` [float, default=&quot;gaussian&quot;]:</span>
<span class="sd">        - Use noise=&quot;gaussian&quot; if the objective returns noisy observations.</span>
<span class="sd">          The noise of each observation is assumed to be iid with</span>
<span class="sd">          mean zero and a fixed variance.</span>
<span class="sd">        - If the variance is known before-hand, this can be set directly</span>
<span class="sd">          to the variance of the noise.</span>
<span class="sd">        - Set this to a value close to zero (1e-10) if the function is noise-free.</span>
<span class="sd">          Setting to zero might cause stability issues.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `res` [`OptimizeResult`, scipy object]:</span>
<span class="sd">        The optimization result returned as a OptimizeResult object.</span>
<span class="sd">        Important attributes are:</span>

<span class="sd">        - `x` [list]: location of the minimum.</span>
<span class="sd">        - `fun` [float]: function value at the minimum.</span>
<span class="sd">        - `models`: surrogate models used for each iteration.</span>
<span class="sd">        - `x_iters` [list of lists]: location of function evaluation for each</span>
<span class="sd">           iteration.</span>
<span class="sd">        - `func_vals` [array]: function value for each iteration.</span>
<span class="sd">        - `space` [Space]: the optimization space.</span>
<span class="sd">        - `specs` [dict]`: the call specifications.</span>
<span class="sd">        - `rng` [RandomState instance]: State of the random state</span>
<span class="sd">           at the end of minimization.</span>

<span class="sd">        For more details related to the OptimizeResult object, refer</span>
<span class="sd">        http://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.OptimizeResult.html</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Check params</span>
    <span class="n">rng</span> <span class="o">=</span> <span class="n">check_random_state</span><span class="p">(</span><span class="n">random_state</span><span class="p">)</span>

    <span class="c1"># To make sure that GP operates in the [0, 1] space</span>
    <span class="n">dimensions</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">check_dimension</span><span class="p">(</span><span class="n">dim</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="s2">&quot;normalize&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">dim</span> <span class="ow">in</span> <span class="n">dimensions</span><span class="p">]</span>
    <span class="n">space</span> <span class="o">=</span> <span class="n">Space</span><span class="p">(</span><span class="n">dimensions</span><span class="p">)</span>

    <span class="c1"># Default GP</span>
    <span class="k">if</span> <span class="n">base_estimator</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
        <span class="n">cov_amplitude</span> <span class="o">=</span> <span class="n">ConstantKernel</span><span class="p">(</span><span class="mf">1.0</span><span class="p">,</span> <span class="p">(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mf">1000.0</span><span class="p">))</span>
        <span class="n">matern</span> <span class="o">=</span> <span class="n">Matern</span><span class="p">(</span><span class="n">length_scale</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">space</span><span class="o">.</span><span class="n">transformed_n_dims</span><span class="p">),</span>
                        <span class="n">length_scale_bounds</span><span class="o">=</span><span class="p">[(</span><span class="mf">0.01</span><span class="p">,</span> <span class="mi">100</span><span class="p">)]</span> <span class="o">*</span> <span class="n">space</span><span class="o">.</span><span class="n">transformed_n_dims</span><span class="p">,</span>
                        <span class="n">nu</span><span class="o">=</span><span class="mf">2.5</span><span class="p">)</span>
        <span class="n">base_estimator</span> <span class="o">=</span> <span class="n">GaussianProcessRegressor</span><span class="p">(</span>
            <span class="n">kernel</span><span class="o">=</span><span class="n">cov_amplitude</span> <span class="o">*</span> <span class="n">matern</span><span class="p">,</span>
            <span class="n">normalize_y</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="n">noise</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">base_minimize</span><span class="p">(</span>
        <span class="n">func</span><span class="p">,</span> <span class="n">dimensions</span><span class="p">,</span> <span class="n">base_estimator</span><span class="o">=</span><span class="n">base_estimator</span><span class="p">,</span>
        <span class="n">acq_func</span><span class="o">=</span><span class="n">acq_func</span><span class="p">,</span>
        <span class="n">xi</span><span class="o">=</span><span class="n">xi</span><span class="p">,</span> <span class="n">kappa</span><span class="o">=</span><span class="n">kappa</span><span class="p">,</span> <span class="n">acq_optimizer</span><span class="o">=</span><span class="n">acq_optimizer</span><span class="p">,</span> <span class="n">n_calls</span><span class="o">=</span><span class="n">n_calls</span><span class="p">,</span>
        <span class="n">n_points</span><span class="o">=</span><span class="n">n_points</span><span class="p">,</span> <span class="n">n_random_starts</span><span class="o">=</span><span class="n">n_random_starts</span><span class="p">,</span>
        <span class="n">n_restarts_optimizer</span><span class="o">=</span><span class="n">n_restarts_optimizer</span><span class="p">,</span>
        <span class="n">x0</span><span class="o">=</span><span class="n">x0</span><span class="p">,</span> <span class="n">y0</span><span class="o">=</span><span class="n">y0</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">random_state</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">verbose</span><span class="p">,</span>
        <span class="n">callback</span><span class="o">=</span><span class="n">callback</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  
      
  <div class="item">
    <div class="name def" id="skopt.load">
    <p>def <span class="ident">load</span>(</p><p>filename, **kwargs)</p>
    </div>
    

    
  
    <div class="desc"><p>Reconstruct a skopt optimization result from a file
persisted with skopt.dump.</p>
<p>Notice that the loaded optimization result can be missing
the objective function (<code>.specs['args']['func']</code>) if <a href="#skopt.dump"><code>dump</code></a>
was called with <code>store_objective=False</code>.</p>
<h2>Parameters</h2>
<ul>
<li>
<p><code>filename</code> [string or <code>pathlib.Path</code>]:
    The path of the file from which to load the optimization result.</p>
</li>
<li>
<p><code>**kwargs</code> [other keyword arguments]:
    All other keyword arguments will be passed to <code>joblib.load</code>.</p>
</li>
</ul>
<h2>Returns</h2>
<ul>
<li><code>res</code> [<code>OptimizeResult</code>, scipy object]:
    Reconstructed OptimizeResult instance.</li>
</ul></div>
  <div class="source_cont">
  <p class="source_link"><a href="javascript:void(0);" onclick="toggle('source-skopt.load', this);">Show source &equiv;</a></p>
  <div id="source-skopt.load" class="source">
    <div class="codehilite"><pre><span></span><span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reconstruct a skopt optimization result from a file</span>
<span class="sd">    persisted with skopt.dump.</span>

<span class="sd">    Notice that the loaded optimization result can be missing</span>
<span class="sd">    the objective function (`.specs[&#39;args&#39;][&#39;func&#39;]`) if `skopt.dump`</span>
<span class="sd">    was called with `store_objective=False`.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    * `filename` [string or `pathlib.Path`]:</span>
<span class="sd">        The path of the file from which to load the optimization result.</span>

<span class="sd">    * `**kwargs` [other keyword arguments]:</span>
<span class="sd">        All other keyword arguments will be passed to `joblib.load`.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    * `res` [`OptimizeResult`, scipy object]:</span>
<span class="sd">        Reconstructed OptimizeResult instance.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">load_</span><span class="p">(</span><span class="n">filename</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</pre></div>

  </div>
</div>

  </div>
  

  </section>

    </article>
  <div class="clear"> </div>
  <footer id="footer">
    <p>
      Documentation generated by
      <a href="https://github.com/BurntSushi/pdoc">pdoc 0.3.2</a>
    </p>

    <p>Design by <a href="http://nadh.in">Kailash Nadh</a></p>
  </footer>
</div>
</body>
</html>
